{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRGradientDescent import LogisticRegressionGradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 2000\n",
    "fullSetx_NF = np.loadtxt('data_digits_8_vs_9_noisy/x_train.csv', delimiter=',', skiprows=1)\n",
    "N = fullSetx_NF.shape[0]\n",
    "A = N-M\n",
    "vsetx_MF = fullSetx_NF.copy()[:M]\n",
    "tsetx_AF = fullSetx_NF.copy()[M:]\n",
    "fullSety_N = np.loadtxt('data_digits_8_vs_9_noisy/y_train.csv', delimiter=',', skiprows=1)\n",
    "vsety_M = fullSety_N.copy()[:M]\n",
    "tsety_A = fullSety_N.copy()[M:]\n",
    "s_lr = LogisticRegressionGradientDescent(\n",
    "        alpha=10, step_size = 0.01, init_w_recipe='zeros')\n",
    "s_lr.fit(tsetx_AF, tsety_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 10000 iters of gradient descent with step_size 0.01\n",
      "iter    0/10000  loss         1.000000  avg_L1_norm_grad         0.024676  w[0]    0.000 bias    0.000\n",
      "iter    1/10000  loss         0.985324  avg_L1_norm_grad         0.024269  w[0]   -0.000 bias    0.000\n",
      "iter    2/10000  loss         0.971326  avg_L1_norm_grad         0.023975  w[0]   -0.000 bias    0.000\n",
      "iter    3/10000  loss         0.957867  avg_L1_norm_grad         0.023721  w[0]   -0.000 bias    0.000\n",
      "iter    4/10000  loss         0.944861  avg_L1_norm_grad         0.023479  w[0]   -0.000 bias    0.001\n",
      "iter    5/10000  loss         0.932256  avg_L1_norm_grad         0.023247  w[0]   -0.000 bias    0.001\n",
      "iter    6/10000  loss         0.920016  avg_L1_norm_grad         0.023010  w[0]   -0.000 bias    0.001\n",
      "iter    7/10000  loss         0.908117  avg_L1_norm_grad         0.022765  w[0]   -0.000 bias    0.001\n",
      "iter    8/10000  loss         0.896541  avg_L1_norm_grad         0.022515  w[0]   -0.000 bias    0.002\n",
      "iter    9/10000  loss         0.885273  avg_L1_norm_grad         0.022262  w[0]   -0.000 bias    0.002\n",
      "iter   10/10000  loss         0.874302  avg_L1_norm_grad         0.022009  w[0]   -0.000 bias    0.002\n",
      "iter   11/10000  loss         0.863617  avg_L1_norm_grad         0.021755  w[0]   -0.000 bias    0.003\n",
      "iter   12/10000  loss         0.853208  avg_L1_norm_grad         0.021501  w[0]   -0.000 bias    0.003\n",
      "iter   13/10000  loss         0.843067  avg_L1_norm_grad         0.021249  w[0]    0.000 bias    0.003\n",
      "iter   14/10000  loss         0.833185  avg_L1_norm_grad         0.020999  w[0]    0.000 bias    0.004\n",
      "iter   15/10000  loss         0.823554  avg_L1_norm_grad         0.020752  w[0]    0.000 bias    0.004\n",
      "iter   16/10000  loss         0.814166  avg_L1_norm_grad         0.020507  w[0]    0.000 bias    0.004\n",
      "iter   17/10000  loss         0.805013  avg_L1_norm_grad         0.020265  w[0]    0.000 bias    0.005\n",
      "iter   18/10000  loss         0.796089  avg_L1_norm_grad         0.020027  w[0]    0.000 bias    0.005\n",
      "iter   19/10000  loss         0.787386  avg_L1_norm_grad         0.019793  w[0]    0.000 bias    0.005\n",
      "iter   20/10000  loss         0.778897  avg_L1_norm_grad         0.019563  w[0]    0.000 bias    0.006\n",
      "iter   21/10000  loss         0.770616  avg_L1_norm_grad         0.019336  w[0]    0.000 bias    0.006\n",
      "iter   40/10000  loss         0.645315  avg_L1_norm_grad         0.015729  w[0]    0.000 bias    0.012\n",
      "iter   41/10000  loss         0.640081  avg_L1_norm_grad         0.015572  w[0]    0.000 bias    0.012\n",
      "iter   60/10000  loss         0.558274  avg_L1_norm_grad         0.013069  w[0]    0.000 bias    0.017\n",
      "iter   61/10000  loss         0.554733  avg_L1_norm_grad         0.012959  w[0]    0.000 bias    0.018\n",
      "iter   80/10000  loss         0.497787  avg_L1_norm_grad         0.011176  w[0]    0.000 bias    0.022\n",
      "iter   81/10000  loss         0.495248  avg_L1_norm_grad         0.011096  w[0]    0.000 bias    0.022\n",
      "iter  100/10000  loss         0.453472  avg_L1_norm_grad         0.009771  w[0]    0.001 bias    0.027\n",
      "iter  101/10000  loss         0.451566  avg_L1_norm_grad         0.009710  w[0]    0.001 bias    0.027\n",
      "iter  120/10000  loss         0.419617  avg_L1_norm_grad         0.008692  w[0]    0.001 bias    0.031\n",
      "iter  121/10000  loss         0.418132  avg_L1_norm_grad         0.008645  w[0]    0.001 bias    0.031\n",
      "iter  140/10000  loss         0.392876  avg_L1_norm_grad         0.007840  w[0]    0.001 bias    0.035\n",
      "iter  141/10000  loss         0.391685  avg_L1_norm_grad         0.007802  w[0]    0.001 bias    0.035\n",
      "iter  160/10000  loss         0.371182  avg_L1_norm_grad         0.007151  w[0]    0.001 bias    0.039\n",
      "iter  161/10000  loss         0.370203  avg_L1_norm_grad         0.007120  w[0]    0.001 bias    0.039\n",
      "iter  180/10000  loss         0.353193  avg_L1_norm_grad         0.006584  w[0]    0.001 bias    0.042\n",
      "iter  181/10000  loss         0.352374  avg_L1_norm_grad         0.006558  w[0]    0.001 bias    0.042\n",
      "iter  200/10000  loss         0.338006  avg_L1_norm_grad         0.006109  w[0]    0.001 bias    0.046\n",
      "iter  201/10000  loss         0.337308  avg_L1_norm_grad         0.006087  w[0]    0.001 bias    0.046\n",
      "iter  220/10000  loss         0.324988  avg_L1_norm_grad         0.005705  w[0]    0.001 bias    0.049\n",
      "iter  221/10000  loss         0.324386  avg_L1_norm_grad         0.005686  w[0]    0.001 bias    0.049\n",
      "iter  240/10000  loss         0.313686  avg_L1_norm_grad         0.005354  w[0]    0.001 bias    0.052\n",
      "iter  241/10000  loss         0.313159  avg_L1_norm_grad         0.005338  w[0]    0.001 bias    0.052\n",
      "iter  260/10000  loss         0.303764  avg_L1_norm_grad         0.005048  w[0]    0.001 bias    0.055\n",
      "iter  261/10000  loss         0.303299  avg_L1_norm_grad         0.005034  w[0]    0.001 bias    0.055\n",
      "iter  280/10000  loss         0.294971  avg_L1_norm_grad         0.004779  w[0]    0.001 bias    0.058\n",
      "iter  281/10000  loss         0.294557  avg_L1_norm_grad         0.004766  w[0]    0.001 bias    0.058\n",
      "iter  300/10000  loss         0.287113  avg_L1_norm_grad         0.004541  w[0]    0.001 bias    0.061\n",
      "iter  301/10000  loss         0.286741  avg_L1_norm_grad         0.004529  w[0]    0.001 bias    0.061\n",
      "iter  320/10000  loss         0.280039  avg_L1_norm_grad         0.004327  w[0]    0.001 bias    0.064\n",
      "iter  321/10000  loss         0.279703  avg_L1_norm_grad         0.004317  w[0]    0.001 bias    0.064\n",
      "iter  340/10000  loss         0.273629  avg_L1_norm_grad         0.004135  w[0]    0.001 bias    0.067\n",
      "iter  341/10000  loss         0.273324  avg_L1_norm_grad         0.004125  w[0]    0.001 bias    0.067\n",
      "iter  360/10000  loss         0.267787  avg_L1_norm_grad         0.003961  w[0]    0.001 bias    0.069\n",
      "iter  361/10000  loss         0.267509  avg_L1_norm_grad         0.003953  w[0]    0.001 bias    0.069\n",
      "iter  380/10000  loss         0.262436  avg_L1_norm_grad         0.003803  w[0]    0.001 bias    0.072\n",
      "iter  381/10000  loss         0.262181  avg_L1_norm_grad         0.003796  w[0]    0.001 bias    0.072\n",
      "iter  400/10000  loss         0.257512  avg_L1_norm_grad         0.003660  w[0]    0.001 bias    0.074\n",
      "iter  401/10000  loss         0.257276  avg_L1_norm_grad         0.003653  w[0]    0.001 bias    0.075\n",
      "iter  420/10000  loss         0.252960  avg_L1_norm_grad         0.003528  w[0]    0.001 bias    0.077\n",
      "iter  421/10000  loss         0.252741  avg_L1_norm_grad         0.003522  w[0]    0.001 bias    0.077\n",
      "iter  440/10000  loss         0.248737  avg_L1_norm_grad         0.003408  w[0]    0.001 bias    0.079\n",
      "iter  441/10000  loss         0.248534  avg_L1_norm_grad         0.003403  w[0]    0.001 bias    0.079\n",
      "iter  460/10000  loss         0.244805  avg_L1_norm_grad         0.003297  w[0]    0.001 bias    0.082\n",
      "iter  461/10000  loss         0.244616  avg_L1_norm_grad         0.003292  w[0]    0.001 bias    0.082\n",
      "iter  480/10000  loss         0.241133  avg_L1_norm_grad         0.003194  w[0]    0.001 bias    0.084\n",
      "iter  481/10000  loss         0.240956  avg_L1_norm_grad         0.003189  w[0]    0.001 bias    0.084\n",
      "iter  500/10000  loss         0.237693  avg_L1_norm_grad         0.003098  w[0]    0.001 bias    0.086\n",
      "iter  501/10000  loss         0.237527  avg_L1_norm_grad         0.003094  w[0]    0.001 bias    0.087\n",
      "iter  520/10000  loss         0.234462  avg_L1_norm_grad         0.003009  w[0]    0.001 bias    0.089\n",
      "iter  521/10000  loss         0.234306  avg_L1_norm_grad         0.003005  w[0]    0.001 bias    0.089\n",
      "iter  540/10000  loss         0.231420  avg_L1_norm_grad         0.002926  w[0]    0.001 bias    0.091\n",
      "iter  541/10000  loss         0.231272  avg_L1_norm_grad         0.002922  w[0]    0.001 bias    0.091\n",
      "iter  560/10000  loss         0.228548  avg_L1_norm_grad         0.002848  w[0]    0.001 bias    0.093\n",
      "iter  561/10000  loss         0.228409  avg_L1_norm_grad         0.002844  w[0]    0.001 bias    0.093\n",
      "iter  580/10000  loss         0.225832  avg_L1_norm_grad         0.002775  w[0]    0.001 bias    0.095\n",
      "iter  581/10000  loss         0.225700  avg_L1_norm_grad         0.002771  w[0]    0.001 bias    0.095\n",
      "iter  600/10000  loss         0.223258  avg_L1_norm_grad         0.002706  w[0]    0.001 bias    0.097\n",
      "iter  601/10000  loss         0.223133  avg_L1_norm_grad         0.002703  w[0]    0.001 bias    0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/10000  loss         0.220814  avg_L1_norm_grad         0.002642  w[0]    0.001 bias    0.099\n",
      "iter  621/10000  loss         0.220695  avg_L1_norm_grad         0.002638  w[0]    0.001 bias    0.100\n",
      "iter  640/10000  loss         0.218489  avg_L1_norm_grad         0.002581  w[0]    0.001 bias    0.101\n",
      "iter  641/10000  loss         0.218375  avg_L1_norm_grad         0.002578  w[0]    0.001 bias    0.102\n",
      "iter  660/10000  loss         0.216274  avg_L1_norm_grad         0.002523  w[0]    0.001 bias    0.104\n",
      "iter  661/10000  loss         0.216166  avg_L1_norm_grad         0.002521  w[0]    0.001 bias    0.104\n",
      "iter  680/10000  loss         0.214160  avg_L1_norm_grad         0.002469  w[0]    0.001 bias    0.106\n",
      "iter  681/10000  loss         0.214057  avg_L1_norm_grad         0.002466  w[0]    0.001 bias    0.106\n",
      "iter  700/10000  loss         0.212141  avg_L1_norm_grad         0.002417  w[0]    0.001 bias    0.108\n",
      "iter  701/10000  loss         0.212042  avg_L1_norm_grad         0.002415  w[0]    0.001 bias    0.108\n",
      "iter  720/10000  loss         0.210208  avg_L1_norm_grad         0.002368  w[0]    0.001 bias    0.109\n",
      "iter  721/10000  loss         0.210114  avg_L1_norm_grad         0.002366  w[0]    0.001 bias    0.110\n",
      "iter  740/10000  loss         0.208357  avg_L1_norm_grad         0.002321  w[0]    0.001 bias    0.111\n",
      "iter  741/10000  loss         0.208266  avg_L1_norm_grad         0.002319  w[0]    0.001 bias    0.111\n",
      "iter  760/10000  loss         0.206581  avg_L1_norm_grad         0.002277  w[0]    0.001 bias    0.113\n",
      "iter  761/10000  loss         0.206494  avg_L1_norm_grad         0.002274  w[0]    0.001 bias    0.113\n",
      "iter  780/10000  loss         0.204875  avg_L1_norm_grad         0.002234  w[0]    0.001 bias    0.115\n",
      "iter  781/10000  loss         0.204792  avg_L1_norm_grad         0.002232  w[0]    0.001 bias    0.115\n",
      "iter  800/10000  loss         0.203236  avg_L1_norm_grad         0.002193  w[0]    0.001 bias    0.117\n",
      "iter  801/10000  loss         0.203156  avg_L1_norm_grad         0.002191  w[0]    0.001 bias    0.117\n",
      "iter  820/10000  loss         0.201658  avg_L1_norm_grad         0.002154  w[0]    0.000 bias    0.119\n",
      "iter  821/10000  loss         0.201581  avg_L1_norm_grad         0.002152  w[0]    0.000 bias    0.119\n",
      "iter  840/10000  loss         0.200138  avg_L1_norm_grad         0.002117  w[0]    0.000 bias    0.121\n",
      "iter  841/10000  loss         0.200064  avg_L1_norm_grad         0.002115  w[0]    0.000 bias    0.121\n",
      "iter  860/10000  loss         0.198673  avg_L1_norm_grad         0.002081  w[0]    0.000 bias    0.122\n",
      "iter  861/10000  loss         0.198601  avg_L1_norm_grad         0.002079  w[0]    0.000 bias    0.123\n",
      "iter  880/10000  loss         0.197259  avg_L1_norm_grad         0.002047  w[0]    0.000 bias    0.124\n",
      "iter  881/10000  loss         0.197189  avg_L1_norm_grad         0.002045  w[0]    0.000 bias    0.124\n",
      "iter  900/10000  loss         0.195893  avg_L1_norm_grad         0.002014  w[0]    0.000 bias    0.126\n",
      "iter  901/10000  loss         0.195826  avg_L1_norm_grad         0.002012  w[0]    0.000 bias    0.126\n",
      "iter  920/10000  loss         0.194572  avg_L1_norm_grad         0.001982  w[0]    0.000 bias    0.128\n",
      "iter  921/10000  loss         0.194508  avg_L1_norm_grad         0.001980  w[0]    0.000 bias    0.128\n",
      "iter  940/10000  loss         0.193295  avg_L1_norm_grad         0.001951  w[0]    0.000 bias    0.129\n",
      "iter  941/10000  loss         0.193233  avg_L1_norm_grad         0.001950  w[0]    0.000 bias    0.129\n",
      "iter  960/10000  loss         0.192059  avg_L1_norm_grad         0.001922  w[0]    0.000 bias    0.131\n",
      "iter  961/10000  loss         0.191998  avg_L1_norm_grad         0.001920  w[0]    0.000 bias    0.131\n",
      "iter  980/10000  loss         0.190862  avg_L1_norm_grad         0.001894  w[0]   -0.000 bias    0.133\n",
      "iter  981/10000  loss         0.190803  avg_L1_norm_grad         0.001892  w[0]   -0.000 bias    0.133\n",
      "iter 1000/10000  loss         0.189701  avg_L1_norm_grad         0.001866  w[0]   -0.000 bias    0.134\n",
      "iter 1001/10000  loss         0.189644  avg_L1_norm_grad         0.001865  w[0]   -0.000 bias    0.135\n",
      "iter 1020/10000  loss         0.188575  avg_L1_norm_grad         0.001840  w[0]   -0.000 bias    0.136\n",
      "iter 1021/10000  loss         0.188519  avg_L1_norm_grad         0.001839  w[0]   -0.000 bias    0.136\n",
      "iter 1040/10000  loss         0.187482  avg_L1_norm_grad         0.001814  w[0]   -0.000 bias    0.138\n",
      "iter 1041/10000  loss         0.187428  avg_L1_norm_grad         0.001813  w[0]   -0.000 bias    0.138\n",
      "iter 1060/10000  loss         0.186420  avg_L1_norm_grad         0.001790  w[0]   -0.000 bias    0.139\n",
      "iter 1061/10000  loss         0.186368  avg_L1_norm_grad         0.001789  w[0]   -0.000 bias    0.139\n",
      "iter 1080/10000  loss         0.185389  avg_L1_norm_grad         0.001766  w[0]   -0.000 bias    0.141\n",
      "iter 1081/10000  loss         0.185338  avg_L1_norm_grad         0.001765  w[0]   -0.000 bias    0.141\n",
      "iter 1100/10000  loss         0.184387  avg_L1_norm_grad         0.001743  w[0]   -0.000 bias    0.143\n",
      "iter 1101/10000  loss         0.184337  avg_L1_norm_grad         0.001742  w[0]   -0.000 bias    0.143\n",
      "iter 1120/10000  loss         0.183411  avg_L1_norm_grad         0.001721  w[0]   -0.000 bias    0.144\n",
      "iter 1121/10000  loss         0.183363  avg_L1_norm_grad         0.001720  w[0]   -0.000 bias    0.144\n",
      "iter 1140/10000  loss         0.182462  avg_L1_norm_grad         0.001699  w[0]   -0.001 bias    0.146\n",
      "iter 1141/10000  loss         0.182415  avg_L1_norm_grad         0.001698  w[0]   -0.001 bias    0.146\n",
      "iter 1160/10000  loss         0.181538  avg_L1_norm_grad         0.001678  w[0]   -0.001 bias    0.147\n",
      "iter 1161/10000  loss         0.181492  avg_L1_norm_grad         0.001677  w[0]   -0.001 bias    0.147\n",
      "iter 1180/10000  loss         0.180638  avg_L1_norm_grad         0.001658  w[0]   -0.001 bias    0.149\n",
      "iter 1181/10000  loss         0.180593  avg_L1_norm_grad         0.001657  w[0]   -0.001 bias    0.149\n",
      "iter 1200/10000  loss         0.179760  avg_L1_norm_grad         0.001638  w[0]   -0.001 bias    0.150\n",
      "iter 1201/10000  loss         0.179717  avg_L1_norm_grad         0.001637  w[0]   -0.001 bias    0.150\n",
      "iter 1220/10000  loss         0.178905  avg_L1_norm_grad         0.001619  w[0]   -0.001 bias    0.152\n",
      "iter 1221/10000  loss         0.178863  avg_L1_norm_grad         0.001618  w[0]   -0.001 bias    0.152\n",
      "iter 1240/10000  loss         0.178071  avg_L1_norm_grad         0.001600  w[0]   -0.001 bias    0.153\n",
      "iter 1241/10000  loss         0.178029  avg_L1_norm_grad         0.001600  w[0]   -0.001 bias    0.153\n",
      "iter 1260/10000  loss         0.177256  avg_L1_norm_grad         0.001582  w[0]   -0.001 bias    0.155\n",
      "iter 1261/10000  loss         0.177216  avg_L1_norm_grad         0.001581  w[0]   -0.001 bias    0.155\n",
      "iter 1280/10000  loss         0.176462  avg_L1_norm_grad         0.001565  w[0]   -0.001 bias    0.156\n",
      "iter 1281/10000  loss         0.176422  avg_L1_norm_grad         0.001564  w[0]   -0.001 bias    0.156\n",
      "iter 1300/10000  loss         0.175685  avg_L1_norm_grad         0.001547  w[0]   -0.001 bias    0.158\n",
      "iter 1301/10000  loss         0.175647  avg_L1_norm_grad         0.001547  w[0]   -0.001 bias    0.158\n",
      "iter 1320/10000  loss         0.174927  avg_L1_norm_grad         0.001531  w[0]   -0.001 bias    0.159\n",
      "iter 1321/10000  loss         0.174890  avg_L1_norm_grad         0.001530  w[0]   -0.001 bias    0.159\n",
      "iter 1340/10000  loss         0.174186  avg_L1_norm_grad         0.001514  w[0]   -0.001 bias    0.161\n",
      "iter 1341/10000  loss         0.174150  avg_L1_norm_grad         0.001513  w[0]   -0.001 bias    0.161\n",
      "iter 1360/10000  loss         0.173462  avg_L1_norm_grad         0.001498  w[0]   -0.001 bias    0.162\n",
      "iter 1361/10000  loss         0.173426  avg_L1_norm_grad         0.001498  w[0]   -0.001 bias    0.162\n",
      "iter 1380/10000  loss         0.172754  avg_L1_norm_grad         0.001483  w[0]   -0.001 bias    0.163\n",
      "iter 1381/10000  loss         0.172719  avg_L1_norm_grad         0.001482  w[0]   -0.001 bias    0.163\n",
      "iter 1400/10000  loss         0.172061  avg_L1_norm_grad         0.001468  w[0]   -0.002 bias    0.165\n",
      "iter 1401/10000  loss         0.172026  avg_L1_norm_grad         0.001467  w[0]   -0.002 bias    0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/10000  loss         0.171383  avg_L1_norm_grad         0.001453  w[0]   -0.002 bias    0.166\n",
      "iter 1421/10000  loss         0.171349  avg_L1_norm_grad         0.001452  w[0]   -0.002 bias    0.166\n",
      "iter 1440/10000  loss         0.170719  avg_L1_norm_grad         0.001438  w[0]   -0.002 bias    0.168\n",
      "iter 1441/10000  loss         0.170686  avg_L1_norm_grad         0.001438  w[0]   -0.002 bias    0.168\n",
      "iter 1460/10000  loss         0.170069  avg_L1_norm_grad         0.001424  w[0]   -0.002 bias    0.169\n",
      "iter 1461/10000  loss         0.170037  avg_L1_norm_grad         0.001423  w[0]   -0.002 bias    0.169\n",
      "iter 1480/10000  loss         0.169433  avg_L1_norm_grad         0.001410  w[0]   -0.002 bias    0.170\n",
      "iter 1481/10000  loss         0.169401  avg_L1_norm_grad         0.001410  w[0]   -0.002 bias    0.170\n",
      "iter 1500/10000  loss         0.168809  avg_L1_norm_grad         0.001397  w[0]   -0.002 bias    0.172\n",
      "iter 1501/10000  loss         0.168778  avg_L1_norm_grad         0.001396  w[0]   -0.002 bias    0.172\n",
      "iter 1520/10000  loss         0.168198  avg_L1_norm_grad         0.001384  w[0]   -0.002 bias    0.173\n",
      "iter 1521/10000  loss         0.168168  avg_L1_norm_grad         0.001383  w[0]   -0.002 bias    0.173\n",
      "iter 1540/10000  loss         0.167599  avg_L1_norm_grad         0.001371  w[0]   -0.002 bias    0.174\n",
      "iter 1541/10000  loss         0.167570  avg_L1_norm_grad         0.001371  w[0]   -0.002 bias    0.174\n",
      "iter 1560/10000  loss         0.167012  avg_L1_norm_grad         0.001359  w[0]   -0.002 bias    0.176\n",
      "iter 1561/10000  loss         0.166983  avg_L1_norm_grad         0.001358  w[0]   -0.002 bias    0.176\n",
      "iter 1580/10000  loss         0.166436  avg_L1_norm_grad         0.001346  w[0]   -0.002 bias    0.177\n",
      "iter 1581/10000  loss         0.166408  avg_L1_norm_grad         0.001346  w[0]   -0.002 bias    0.177\n",
      "iter 1600/10000  loss         0.165871  avg_L1_norm_grad         0.001335  w[0]   -0.002 bias    0.178\n",
      "iter 1601/10000  loss         0.165843  avg_L1_norm_grad         0.001334  w[0]   -0.002 bias    0.178\n",
      "iter 1620/10000  loss         0.165317  avg_L1_norm_grad         0.001323  w[0]   -0.003 bias    0.180\n",
      "iter 1621/10000  loss         0.165290  avg_L1_norm_grad         0.001322  w[0]   -0.003 bias    0.180\n",
      "iter 1640/10000  loss         0.164773  avg_L1_norm_grad         0.001312  w[0]   -0.003 bias    0.181\n",
      "iter 1641/10000  loss         0.164746  avg_L1_norm_grad         0.001311  w[0]   -0.003 bias    0.181\n",
      "iter 1660/10000  loss         0.164239  avg_L1_norm_grad         0.001300  w[0]   -0.003 bias    0.182\n",
      "iter 1661/10000  loss         0.164213  avg_L1_norm_grad         0.001300  w[0]   -0.003 bias    0.182\n",
      "iter 1680/10000  loss         0.163715  avg_L1_norm_grad         0.001289  w[0]   -0.003 bias    0.183\n",
      "iter 1681/10000  loss         0.163689  avg_L1_norm_grad         0.001289  w[0]   -0.003 bias    0.184\n",
      "iter 1700/10000  loss         0.163200  avg_L1_norm_grad         0.001279  w[0]   -0.003 bias    0.185\n",
      "iter 1701/10000  loss         0.163175  avg_L1_norm_grad         0.001278  w[0]   -0.003 bias    0.185\n",
      "iter 1720/10000  loss         0.162695  avg_L1_norm_grad         0.001268  w[0]   -0.003 bias    0.186\n",
      "iter 1721/10000  loss         0.162669  avg_L1_norm_grad         0.001268  w[0]   -0.003 bias    0.186\n",
      "iter 1740/10000  loss         0.162198  avg_L1_norm_grad         0.001258  w[0]   -0.003 bias    0.187\n",
      "iter 1741/10000  loss         0.162173  avg_L1_norm_grad         0.001257  w[0]   -0.003 bias    0.187\n",
      "iter 1760/10000  loss         0.161710  avg_L1_norm_grad         0.001248  w[0]   -0.003 bias    0.188\n",
      "iter 1761/10000  loss         0.161685  avg_L1_norm_grad         0.001247  w[0]   -0.003 bias    0.189\n",
      "iter 1780/10000  loss         0.161230  avg_L1_norm_grad         0.001238  w[0]   -0.003 bias    0.190\n",
      "iter 1781/10000  loss         0.161206  avg_L1_norm_grad         0.001237  w[0]   -0.003 bias    0.190\n",
      "iter 1800/10000  loss         0.160758  avg_L1_norm_grad         0.001228  w[0]   -0.003 bias    0.191\n",
      "iter 1801/10000  loss         0.160735  avg_L1_norm_grad         0.001228  w[0]   -0.003 bias    0.191\n",
      "iter 1820/10000  loss         0.160294  avg_L1_norm_grad         0.001219  w[0]   -0.003 bias    0.192\n",
      "iter 1821/10000  loss         0.160271  avg_L1_norm_grad         0.001218  w[0]   -0.003 bias    0.192\n",
      "iter 1840/10000  loss         0.159838  avg_L1_norm_grad         0.001209  w[0]   -0.003 bias    0.193\n",
      "iter 1841/10000  loss         0.159816  avg_L1_norm_grad         0.001209  w[0]   -0.003 bias    0.193\n",
      "iter 1860/10000  loss         0.159390  avg_L1_norm_grad         0.001200  w[0]   -0.004 bias    0.195\n",
      "iter 1861/10000  loss         0.159368  avg_L1_norm_grad         0.001200  w[0]   -0.004 bias    0.195\n",
      "iter 1880/10000  loss         0.158949  avg_L1_norm_grad         0.001191  w[0]   -0.004 bias    0.196\n",
      "iter 1881/10000  loss         0.158927  avg_L1_norm_grad         0.001191  w[0]   -0.004 bias    0.196\n",
      "iter 1900/10000  loss         0.158514  avg_L1_norm_grad         0.001182  w[0]   -0.004 bias    0.197\n",
      "iter 1901/10000  loss         0.158493  avg_L1_norm_grad         0.001182  w[0]   -0.004 bias    0.197\n",
      "iter 1920/10000  loss         0.158087  avg_L1_norm_grad         0.001174  w[0]   -0.004 bias    0.198\n",
      "iter 1921/10000  loss         0.158066  avg_L1_norm_grad         0.001173  w[0]   -0.004 bias    0.198\n",
      "iter 1940/10000  loss         0.157667  avg_L1_norm_grad         0.001165  w[0]   -0.004 bias    0.199\n",
      "iter 1941/10000  loss         0.157646  avg_L1_norm_grad         0.001165  w[0]   -0.004 bias    0.199\n",
      "iter 1960/10000  loss         0.157253  avg_L1_norm_grad         0.001157  w[0]   -0.004 bias    0.201\n",
      "iter 1961/10000  loss         0.157232  avg_L1_norm_grad         0.001156  w[0]   -0.004 bias    0.201\n",
      "iter 1980/10000  loss         0.156845  avg_L1_norm_grad         0.001149  w[0]   -0.004 bias    0.202\n",
      "iter 1981/10000  loss         0.156825  avg_L1_norm_grad         0.001148  w[0]   -0.004 bias    0.202\n",
      "iter 2000/10000  loss         0.156444  avg_L1_norm_grad         0.001141  w[0]   -0.004 bias    0.203\n",
      "iter 2001/10000  loss         0.156424  avg_L1_norm_grad         0.001140  w[0]   -0.004 bias    0.203\n",
      "iter 2020/10000  loss         0.156049  avg_L1_norm_grad         0.001133  w[0]   -0.004 bias    0.204\n",
      "iter 2021/10000  loss         0.156029  avg_L1_norm_grad         0.001133  w[0]   -0.004 bias    0.204\n",
      "iter 2040/10000  loss         0.155660  avg_L1_norm_grad         0.001125  w[0]   -0.004 bias    0.205\n",
      "iter 2041/10000  loss         0.155640  avg_L1_norm_grad         0.001125  w[0]   -0.004 bias    0.205\n",
      "iter 2060/10000  loss         0.155276  avg_L1_norm_grad         0.001118  w[0]   -0.004 bias    0.206\n",
      "iter 2061/10000  loss         0.155257  avg_L1_norm_grad         0.001117  w[0]   -0.004 bias    0.206\n",
      "iter 2080/10000  loss         0.154899  avg_L1_norm_grad         0.001110  w[0]   -0.005 bias    0.207\n",
      "iter 2081/10000  loss         0.154880  avg_L1_norm_grad         0.001110  w[0]   -0.005 bias    0.208\n",
      "iter 2100/10000  loss         0.154526  avg_L1_norm_grad         0.001103  w[0]   -0.005 bias    0.209\n",
      "iter 2101/10000  loss         0.154508  avg_L1_norm_grad         0.001102  w[0]   -0.005 bias    0.209\n",
      "iter 2120/10000  loss         0.154159  avg_L1_norm_grad         0.001096  w[0]   -0.005 bias    0.210\n",
      "iter 2121/10000  loss         0.154141  avg_L1_norm_grad         0.001095  w[0]   -0.005 bias    0.210\n",
      "iter 2140/10000  loss         0.153798  avg_L1_norm_grad         0.001088  w[0]   -0.005 bias    0.211\n",
      "iter 2141/10000  loss         0.153780  avg_L1_norm_grad         0.001088  w[0]   -0.005 bias    0.211\n",
      "iter 2160/10000  loss         0.153441  avg_L1_norm_grad         0.001081  w[0]   -0.005 bias    0.212\n",
      "iter 2161/10000  loss         0.153424  avg_L1_norm_grad         0.001081  w[0]   -0.005 bias    0.212\n",
      "iter 2180/10000  loss         0.153090  avg_L1_norm_grad         0.001074  w[0]   -0.005 bias    0.213\n",
      "iter 2181/10000  loss         0.153072  avg_L1_norm_grad         0.001074  w[0]   -0.005 bias    0.213\n",
      "iter 2200/10000  loss         0.152743  avg_L1_norm_grad         0.001068  w[0]   -0.005 bias    0.214\n",
      "iter 2201/10000  loss         0.152726  avg_L1_norm_grad         0.001067  w[0]   -0.005 bias    0.214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/10000  loss         0.152402  avg_L1_norm_grad         0.001061  w[0]   -0.005 bias    0.215\n",
      "iter 2221/10000  loss         0.152385  avg_L1_norm_grad         0.001061  w[0]   -0.005 bias    0.215\n",
      "iter 2240/10000  loss         0.152065  avg_L1_norm_grad         0.001054  w[0]   -0.005 bias    0.216\n",
      "iter 2241/10000  loss         0.152048  avg_L1_norm_grad         0.001054  w[0]   -0.005 bias    0.216\n",
      "iter 2260/10000  loss         0.151732  avg_L1_norm_grad         0.001048  w[0]   -0.005 bias    0.217\n",
      "iter 2261/10000  loss         0.151716  avg_L1_norm_grad         0.001047  w[0]   -0.005 bias    0.217\n",
      "iter 2280/10000  loss         0.151404  avg_L1_norm_grad         0.001041  w[0]   -0.005 bias    0.219\n",
      "iter 2281/10000  loss         0.151388  avg_L1_norm_grad         0.001041  w[0]   -0.005 bias    0.219\n",
      "iter 2300/10000  loss         0.151080  avg_L1_norm_grad         0.001035  w[0]   -0.006 bias    0.220\n",
      "iter 2301/10000  loss         0.151064  avg_L1_norm_grad         0.001035  w[0]   -0.006 bias    0.220\n",
      "iter 2320/10000  loss         0.150761  avg_L1_norm_grad         0.001029  w[0]   -0.006 bias    0.221\n",
      "iter 2321/10000  loss         0.150745  avg_L1_norm_grad         0.001028  w[0]   -0.006 bias    0.221\n",
      "iter 2340/10000  loss         0.150446  avg_L1_norm_grad         0.001023  w[0]   -0.006 bias    0.222\n",
      "iter 2341/10000  loss         0.150430  avg_L1_norm_grad         0.001022  w[0]   -0.006 bias    0.222\n",
      "iter 2360/10000  loss         0.150135  avg_L1_norm_grad         0.001017  w[0]   -0.006 bias    0.223\n",
      "iter 2361/10000  loss         0.150119  avg_L1_norm_grad         0.001016  w[0]   -0.006 bias    0.223\n",
      "iter 2380/10000  loss         0.149828  avg_L1_norm_grad         0.001011  w[0]   -0.006 bias    0.224\n",
      "iter 2381/10000  loss         0.149813  avg_L1_norm_grad         0.001010  w[0]   -0.006 bias    0.224\n",
      "iter 2400/10000  loss         0.149525  avg_L1_norm_grad         0.001005  w[0]   -0.006 bias    0.225\n",
      "iter 2401/10000  loss         0.149510  avg_L1_norm_grad         0.001004  w[0]   -0.006 bias    0.225\n",
      "iter 2420/10000  loss         0.149226  avg_L1_norm_grad         0.000999  w[0]   -0.006 bias    0.226\n",
      "iter 2421/10000  loss         0.149211  avg_L1_norm_grad         0.000999  w[0]   -0.006 bias    0.226\n",
      "iter 2440/10000  loss         0.148930  avg_L1_norm_grad         0.000993  w[0]   -0.006 bias    0.227\n",
      "iter 2441/10000  loss         0.148916  avg_L1_norm_grad         0.000993  w[0]   -0.006 bias    0.227\n",
      "iter 2460/10000  loss         0.148639  avg_L1_norm_grad         0.000988  w[0]   -0.006 bias    0.228\n",
      "iter 2461/10000  loss         0.148624  avg_L1_norm_grad         0.000987  w[0]   -0.006 bias    0.228\n",
      "iter 2480/10000  loss         0.148350  avg_L1_norm_grad         0.000982  w[0]   -0.006 bias    0.229\n",
      "iter 2481/10000  loss         0.148336  avg_L1_norm_grad         0.000982  w[0]   -0.006 bias    0.229\n",
      "iter 2500/10000  loss         0.148066  avg_L1_norm_grad         0.000977  w[0]   -0.007 bias    0.230\n",
      "iter 2501/10000  loss         0.148052  avg_L1_norm_grad         0.000976  w[0]   -0.007 bias    0.230\n",
      "iter 2520/10000  loss         0.147785  avg_L1_norm_grad         0.000971  w[0]   -0.007 bias    0.231\n",
      "iter 2521/10000  loss         0.147771  avg_L1_norm_grad         0.000971  w[0]   -0.007 bias    0.231\n",
      "iter 2540/10000  loss         0.147507  avg_L1_norm_grad         0.000966  w[0]   -0.007 bias    0.232\n",
      "iter 2541/10000  loss         0.147493  avg_L1_norm_grad         0.000965  w[0]   -0.007 bias    0.232\n",
      "iter 2560/10000  loss         0.147233  avg_L1_norm_grad         0.000960  w[0]   -0.007 bias    0.233\n",
      "iter 2561/10000  loss         0.147219  avg_L1_norm_grad         0.000960  w[0]   -0.007 bias    0.233\n",
      "iter 2580/10000  loss         0.146962  avg_L1_norm_grad         0.000955  w[0]   -0.007 bias    0.234\n",
      "iter 2581/10000  loss         0.146948  avg_L1_norm_grad         0.000955  w[0]   -0.007 bias    0.234\n",
      "iter 2600/10000  loss         0.146694  avg_L1_norm_grad         0.000950  w[0]   -0.007 bias    0.235\n",
      "iter 2601/10000  loss         0.146681  avg_L1_norm_grad         0.000950  w[0]   -0.007 bias    0.235\n",
      "iter 2620/10000  loss         0.146429  avg_L1_norm_grad         0.000945  w[0]   -0.007 bias    0.236\n",
      "iter 2621/10000  loss         0.146416  avg_L1_norm_grad         0.000945  w[0]   -0.007 bias    0.236\n",
      "iter 2640/10000  loss         0.146168  avg_L1_norm_grad         0.000940  w[0]   -0.007 bias    0.237\n",
      "iter 2641/10000  loss         0.146155  avg_L1_norm_grad         0.000940  w[0]   -0.007 bias    0.237\n",
      "iter 2660/10000  loss         0.145909  avg_L1_norm_grad         0.000935  w[0]   -0.007 bias    0.238\n",
      "iter 2661/10000  loss         0.145896  avg_L1_norm_grad         0.000935  w[0]   -0.007 bias    0.238\n",
      "iter 2680/10000  loss         0.145654  avg_L1_norm_grad         0.000930  w[0]   -0.007 bias    0.239\n",
      "iter 2681/10000  loss         0.145641  avg_L1_norm_grad         0.000930  w[0]   -0.007 bias    0.239\n",
      "iter 2700/10000  loss         0.145401  avg_L1_norm_grad         0.000925  w[0]   -0.007 bias    0.240\n",
      "iter 2701/10000  loss         0.145389  avg_L1_norm_grad         0.000925  w[0]   -0.007 bias    0.240\n",
      "iter 2720/10000  loss         0.145151  avg_L1_norm_grad         0.000920  w[0]   -0.008 bias    0.241\n",
      "iter 2721/10000  loss         0.145139  avg_L1_norm_grad         0.000920  w[0]   -0.008 bias    0.241\n",
      "iter 2740/10000  loss         0.144904  avg_L1_norm_grad         0.000916  w[0]   -0.008 bias    0.242\n",
      "iter 2741/10000  loss         0.144892  avg_L1_norm_grad         0.000915  w[0]   -0.008 bias    0.242\n",
      "iter 2760/10000  loss         0.144660  avg_L1_norm_grad         0.000911  w[0]   -0.008 bias    0.243\n",
      "iter 2761/10000  loss         0.144648  avg_L1_norm_grad         0.000911  w[0]   -0.008 bias    0.243\n",
      "iter 2780/10000  loss         0.144419  avg_L1_norm_grad         0.000906  w[0]   -0.008 bias    0.244\n",
      "iter 2781/10000  loss         0.144407  avg_L1_norm_grad         0.000906  w[0]   -0.008 bias    0.244\n",
      "iter 2800/10000  loss         0.144180  avg_L1_norm_grad         0.000902  w[0]   -0.008 bias    0.245\n",
      "iter 2801/10000  loss         0.144168  avg_L1_norm_grad         0.000902  w[0]   -0.008 bias    0.245\n",
      "iter 2820/10000  loss         0.143944  avg_L1_norm_grad         0.000897  w[0]   -0.008 bias    0.246\n",
      "iter 2821/10000  loss         0.143932  avg_L1_norm_grad         0.000897  w[0]   -0.008 bias    0.246\n",
      "iter 2840/10000  loss         0.143710  avg_L1_norm_grad         0.000893  w[0]   -0.008 bias    0.247\n",
      "iter 2841/10000  loss         0.143699  avg_L1_norm_grad         0.000893  w[0]   -0.008 bias    0.247\n",
      "iter 2860/10000  loss         0.143479  avg_L1_norm_grad         0.000889  w[0]   -0.008 bias    0.248\n",
      "iter 2861/10000  loss         0.143468  avg_L1_norm_grad         0.000888  w[0]   -0.008 bias    0.248\n",
      "iter 2880/10000  loss         0.143251  avg_L1_norm_grad         0.000884  w[0]   -0.008 bias    0.249\n",
      "iter 2881/10000  loss         0.143239  avg_L1_norm_grad         0.000884  w[0]   -0.008 bias    0.249\n",
      "iter 2900/10000  loss         0.143025  avg_L1_norm_grad         0.000880  w[0]   -0.008 bias    0.250\n",
      "iter 2901/10000  loss         0.143013  avg_L1_norm_grad         0.000880  w[0]   -0.008 bias    0.250\n",
      "iter 2920/10000  loss         0.142801  avg_L1_norm_grad         0.000876  w[0]   -0.008 bias    0.251\n",
      "iter 2921/10000  loss         0.142790  avg_L1_norm_grad         0.000876  w[0]   -0.008 bias    0.251\n",
      "iter 2940/10000  loss         0.142580  avg_L1_norm_grad         0.000872  w[0]   -0.009 bias    0.252\n",
      "iter 2941/10000  loss         0.142569  avg_L1_norm_grad         0.000871  w[0]   -0.009 bias    0.252\n",
      "iter 2960/10000  loss         0.142361  avg_L1_norm_grad         0.000867  w[0]   -0.009 bias    0.253\n",
      "iter 2961/10000  loss         0.142350  avg_L1_norm_grad         0.000867  w[0]   -0.009 bias    0.253\n",
      "iter 2980/10000  loss         0.142144  avg_L1_norm_grad         0.000863  w[0]   -0.009 bias    0.254\n",
      "iter 2981/10000  loss         0.142133  avg_L1_norm_grad         0.000863  w[0]   -0.009 bias    0.254\n",
      "iter 3000/10000  loss         0.141929  avg_L1_norm_grad         0.000859  w[0]   -0.009 bias    0.254\n",
      "iter 3001/10000  loss         0.141919  avg_L1_norm_grad         0.000859  w[0]   -0.009 bias    0.254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/10000  loss         0.141717  avg_L1_norm_grad         0.000855  w[0]   -0.009 bias    0.255\n",
      "iter 3021/10000  loss         0.141706  avg_L1_norm_grad         0.000855  w[0]   -0.009 bias    0.255\n",
      "iter 3040/10000  loss         0.141507  avg_L1_norm_grad         0.000852  w[0]   -0.009 bias    0.256\n",
      "iter 3041/10000  loss         0.141496  avg_L1_norm_grad         0.000851  w[0]   -0.009 bias    0.256\n",
      "iter 3060/10000  loss         0.141299  avg_L1_norm_grad         0.000848  w[0]   -0.009 bias    0.257\n",
      "iter 3061/10000  loss         0.141288  avg_L1_norm_grad         0.000847  w[0]   -0.009 bias    0.257\n",
      "iter 3080/10000  loss         0.141093  avg_L1_norm_grad         0.000844  w[0]   -0.009 bias    0.258\n",
      "iter 3081/10000  loss         0.141083  avg_L1_norm_grad         0.000844  w[0]   -0.009 bias    0.258\n",
      "iter 3100/10000  loss         0.140889  avg_L1_norm_grad         0.000840  w[0]   -0.009 bias    0.259\n",
      "iter 3101/10000  loss         0.140879  avg_L1_norm_grad         0.000840  w[0]   -0.009 bias    0.259\n",
      "iter 3120/10000  loss         0.140687  avg_L1_norm_grad         0.000836  w[0]   -0.009 bias    0.260\n",
      "iter 3121/10000  loss         0.140677  avg_L1_norm_grad         0.000836  w[0]   -0.009 bias    0.260\n",
      "iter 3140/10000  loss         0.140487  avg_L1_norm_grad         0.000833  w[0]   -0.009 bias    0.261\n",
      "iter 3141/10000  loss         0.140477  avg_L1_norm_grad         0.000832  w[0]   -0.009 bias    0.261\n",
      "iter 3160/10000  loss         0.140289  avg_L1_norm_grad         0.000829  w[0]   -0.010 bias    0.262\n",
      "iter 3161/10000  loss         0.140279  avg_L1_norm_grad         0.000829  w[0]   -0.010 bias    0.262\n",
      "iter 3180/10000  loss         0.140093  avg_L1_norm_grad         0.000825  w[0]   -0.010 bias    0.263\n",
      "iter 3181/10000  loss         0.140084  avg_L1_norm_grad         0.000825  w[0]   -0.010 bias    0.263\n",
      "iter 3200/10000  loss         0.139899  avg_L1_norm_grad         0.000822  w[0]   -0.010 bias    0.264\n",
      "iter 3201/10000  loss         0.139890  avg_L1_norm_grad         0.000822  w[0]   -0.010 bias    0.264\n",
      "iter 3220/10000  loss         0.139707  avg_L1_norm_grad         0.000818  w[0]   -0.010 bias    0.264\n",
      "iter 3221/10000  loss         0.139698  avg_L1_norm_grad         0.000818  w[0]   -0.010 bias    0.264\n",
      "iter 3240/10000  loss         0.139517  avg_L1_norm_grad         0.000815  w[0]   -0.010 bias    0.265\n",
      "iter 3241/10000  loss         0.139507  avg_L1_norm_grad         0.000815  w[0]   -0.010 bias    0.265\n",
      "iter 3260/10000  loss         0.139328  avg_L1_norm_grad         0.000811  w[0]   -0.010 bias    0.266\n",
      "iter 3261/10000  loss         0.139319  avg_L1_norm_grad         0.000811  w[0]   -0.010 bias    0.266\n",
      "iter 3280/10000  loss         0.139142  avg_L1_norm_grad         0.000808  w[0]   -0.010 bias    0.267\n",
      "iter 3281/10000  loss         0.139132  avg_L1_norm_grad         0.000808  w[0]   -0.010 bias    0.267\n",
      "iter 3300/10000  loss         0.138957  avg_L1_norm_grad         0.000804  w[0]   -0.010 bias    0.268\n",
      "iter 3301/10000  loss         0.138947  avg_L1_norm_grad         0.000804  w[0]   -0.010 bias    0.268\n",
      "iter 3320/10000  loss         0.138773  avg_L1_norm_grad         0.000801  w[0]   -0.010 bias    0.269\n",
      "iter 3321/10000  loss         0.138764  avg_L1_norm_grad         0.000801  w[0]   -0.010 bias    0.269\n",
      "iter 3340/10000  loss         0.138592  avg_L1_norm_grad         0.000798  w[0]   -0.010 bias    0.270\n",
      "iter 3341/10000  loss         0.138583  avg_L1_norm_grad         0.000797  w[0]   -0.010 bias    0.270\n",
      "iter 3360/10000  loss         0.138412  avg_L1_norm_grad         0.000794  w[0]   -0.010 bias    0.271\n",
      "iter 3361/10000  loss         0.138403  avg_L1_norm_grad         0.000794  w[0]   -0.011 bias    0.271\n",
      "iter 3380/10000  loss         0.138234  avg_L1_norm_grad         0.000791  w[0]   -0.011 bias    0.271\n",
      "iter 3381/10000  loss         0.138225  avg_L1_norm_grad         0.000791  w[0]   -0.011 bias    0.271\n",
      "iter 3400/10000  loss         0.138057  avg_L1_norm_grad         0.000788  w[0]   -0.011 bias    0.272\n",
      "iter 3401/10000  loss         0.138049  avg_L1_norm_grad         0.000788  w[0]   -0.011 bias    0.272\n",
      "iter 3420/10000  loss         0.137882  avg_L1_norm_grad         0.000785  w[0]   -0.011 bias    0.273\n",
      "iter 3421/10000  loss         0.137874  avg_L1_norm_grad         0.000784  w[0]   -0.011 bias    0.273\n",
      "iter 3440/10000  loss         0.137709  avg_L1_norm_grad         0.000781  w[0]   -0.011 bias    0.274\n",
      "iter 3441/10000  loss         0.137700  avg_L1_norm_grad         0.000781  w[0]   -0.011 bias    0.274\n",
      "iter 3460/10000  loss         0.137537  avg_L1_norm_grad         0.000778  w[0]   -0.011 bias    0.275\n",
      "iter 3461/10000  loss         0.137529  avg_L1_norm_grad         0.000778  w[0]   -0.011 bias    0.275\n",
      "iter 3480/10000  loss         0.137367  avg_L1_norm_grad         0.000775  w[0]   -0.011 bias    0.276\n",
      "iter 3481/10000  loss         0.137359  avg_L1_norm_grad         0.000775  w[0]   -0.011 bias    0.276\n",
      "iter 3500/10000  loss         0.137198  avg_L1_norm_grad         0.000772  w[0]   -0.011 bias    0.277\n",
      "iter 3501/10000  loss         0.137190  avg_L1_norm_grad         0.000772  w[0]   -0.011 bias    0.277\n",
      "iter 3520/10000  loss         0.137031  avg_L1_norm_grad         0.000769  w[0]   -0.011 bias    0.277\n",
      "iter 3521/10000  loss         0.137023  avg_L1_norm_grad         0.000769  w[0]   -0.011 bias    0.277\n",
      "iter 3540/10000  loss         0.136865  avg_L1_norm_grad         0.000766  w[0]   -0.011 bias    0.278\n",
      "iter 3541/10000  loss         0.136857  avg_L1_norm_grad         0.000766  w[0]   -0.011 bias    0.278\n",
      "iter 3560/10000  loss         0.136701  avg_L1_norm_grad         0.000763  w[0]   -0.011 bias    0.279\n",
      "iter 3561/10000  loss         0.136693  avg_L1_norm_grad         0.000763  w[0]   -0.011 bias    0.279\n",
      "iter 3580/10000  loss         0.136538  avg_L1_norm_grad         0.000760  w[0]   -0.012 bias    0.280\n",
      "iter 3581/10000  loss         0.136530  avg_L1_norm_grad         0.000759  w[0]   -0.012 bias    0.280\n",
      "iter 3600/10000  loss         0.136377  avg_L1_norm_grad         0.000757  w[0]   -0.012 bias    0.281\n",
      "iter 3601/10000  loss         0.136369  avg_L1_norm_grad         0.000757  w[0]   -0.012 bias    0.281\n",
      "iter 3620/10000  loss         0.136216  avg_L1_norm_grad         0.000754  w[0]   -0.012 bias    0.282\n",
      "iter 3621/10000  loss         0.136209  avg_L1_norm_grad         0.000754  w[0]   -0.012 bias    0.282\n",
      "iter 3640/10000  loss         0.136058  avg_L1_norm_grad         0.000751  w[0]   -0.012 bias    0.282\n",
      "iter 3641/10000  loss         0.136050  avg_L1_norm_grad         0.000751  w[0]   -0.012 bias    0.282\n",
      "iter 3660/10000  loss         0.135900  avg_L1_norm_grad         0.000748  w[0]   -0.012 bias    0.283\n",
      "iter 3661/10000  loss         0.135893  avg_L1_norm_grad         0.000748  w[0]   -0.012 bias    0.283\n",
      "iter 3680/10000  loss         0.135744  avg_L1_norm_grad         0.000745  w[0]   -0.012 bias    0.284\n",
      "iter 3681/10000  loss         0.135737  avg_L1_norm_grad         0.000745  w[0]   -0.012 bias    0.284\n",
      "iter 3700/10000  loss         0.135590  avg_L1_norm_grad         0.000742  w[0]   -0.012 bias    0.285\n",
      "iter 3701/10000  loss         0.135582  avg_L1_norm_grad         0.000742  w[0]   -0.012 bias    0.285\n",
      "iter 3720/10000  loss         0.135436  avg_L1_norm_grad         0.000739  w[0]   -0.012 bias    0.286\n",
      "iter 3721/10000  loss         0.135429  avg_L1_norm_grad         0.000739  w[0]   -0.012 bias    0.286\n",
      "iter 3740/10000  loss         0.135284  avg_L1_norm_grad         0.000736  w[0]   -0.012 bias    0.287\n",
      "iter 3741/10000  loss         0.135277  avg_L1_norm_grad         0.000736  w[0]   -0.012 bias    0.287\n",
      "iter 3760/10000  loss         0.135133  avg_L1_norm_grad         0.000734  w[0]   -0.012 bias    0.287\n",
      "iter 3761/10000  loss         0.135126  avg_L1_norm_grad         0.000734  w[0]   -0.012 bias    0.287\n",
      "iter 3780/10000  loss         0.134984  avg_L1_norm_grad         0.000731  w[0]   -0.012 bias    0.288\n",
      "iter 3781/10000  loss         0.134976  avg_L1_norm_grad         0.000731  w[0]   -0.012 bias    0.288\n",
      "iter 3800/10000  loss         0.134835  avg_L1_norm_grad         0.000728  w[0]   -0.012 bias    0.289\n",
      "iter 3801/10000  loss         0.134828  avg_L1_norm_grad         0.000728  w[0]   -0.012 bias    0.289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/10000  loss         0.134688  avg_L1_norm_grad         0.000725  w[0]   -0.013 bias    0.290\n",
      "iter 3821/10000  loss         0.134681  avg_L1_norm_grad         0.000725  w[0]   -0.013 bias    0.290\n",
      "iter 3840/10000  loss         0.134542  avg_L1_norm_grad         0.000723  w[0]   -0.013 bias    0.291\n",
      "iter 3841/10000  loss         0.134535  avg_L1_norm_grad         0.000723  w[0]   -0.013 bias    0.291\n",
      "iter 3860/10000  loss         0.134397  avg_L1_norm_grad         0.000720  w[0]   -0.013 bias    0.291\n",
      "iter 3861/10000  loss         0.134390  avg_L1_norm_grad         0.000720  w[0]   -0.013 bias    0.291\n",
      "iter 3880/10000  loss         0.134254  avg_L1_norm_grad         0.000717  w[0]   -0.013 bias    0.292\n",
      "iter 3881/10000  loss         0.134247  avg_L1_norm_grad         0.000717  w[0]   -0.013 bias    0.292\n",
      "iter 3900/10000  loss         0.134111  avg_L1_norm_grad         0.000715  w[0]   -0.013 bias    0.293\n",
      "iter 3901/10000  loss         0.134104  avg_L1_norm_grad         0.000715  w[0]   -0.013 bias    0.293\n",
      "iter 3920/10000  loss         0.133970  avg_L1_norm_grad         0.000712  w[0]   -0.013 bias    0.294\n",
      "iter 3921/10000  loss         0.133963  avg_L1_norm_grad         0.000712  w[0]   -0.013 bias    0.294\n",
      "iter 3940/10000  loss         0.133830  avg_L1_norm_grad         0.000710  w[0]   -0.013 bias    0.295\n",
      "iter 3941/10000  loss         0.133823  avg_L1_norm_grad         0.000709  w[0]   -0.013 bias    0.295\n",
      "iter 3960/10000  loss         0.133691  avg_L1_norm_grad         0.000707  w[0]   -0.013 bias    0.295\n",
      "iter 3961/10000  loss         0.133684  avg_L1_norm_grad         0.000707  w[0]   -0.013 bias    0.295\n",
      "iter 3980/10000  loss         0.133553  avg_L1_norm_grad         0.000704  w[0]   -0.013 bias    0.296\n",
      "iter 3981/10000  loss         0.133546  avg_L1_norm_grad         0.000704  w[0]   -0.013 bias    0.296\n",
      "iter 4000/10000  loss         0.133416  avg_L1_norm_grad         0.000702  w[0]   -0.013 bias    0.297\n",
      "iter 4001/10000  loss         0.133409  avg_L1_norm_grad         0.000702  w[0]   -0.013 bias    0.297\n",
      "iter 4020/10000  loss         0.133280  avg_L1_norm_grad         0.000699  w[0]   -0.013 bias    0.298\n",
      "iter 4021/10000  loss         0.133273  avg_L1_norm_grad         0.000699  w[0]   -0.013 bias    0.298\n",
      "iter 4040/10000  loss         0.133145  avg_L1_norm_grad         0.000697  w[0]   -0.014 bias    0.299\n",
      "iter 4041/10000  loss         0.133139  avg_L1_norm_grad         0.000697  w[0]   -0.014 bias    0.299\n",
      "iter 4060/10000  loss         0.133011  avg_L1_norm_grad         0.000694  w[0]   -0.014 bias    0.299\n",
      "iter 4061/10000  loss         0.133005  avg_L1_norm_grad         0.000694  w[0]   -0.014 bias    0.299\n",
      "iter 4080/10000  loss         0.132879  avg_L1_norm_grad         0.000692  w[0]   -0.014 bias    0.300\n",
      "iter 4081/10000  loss         0.132872  avg_L1_norm_grad         0.000692  w[0]   -0.014 bias    0.300\n",
      "iter 4100/10000  loss         0.132747  avg_L1_norm_grad         0.000690  w[0]   -0.014 bias    0.301\n",
      "iter 4101/10000  loss         0.132740  avg_L1_norm_grad         0.000689  w[0]   -0.014 bias    0.301\n",
      "iter 4120/10000  loss         0.132616  avg_L1_norm_grad         0.000687  w[0]   -0.014 bias    0.302\n",
      "iter 4121/10000  loss         0.132610  avg_L1_norm_grad         0.000687  w[0]   -0.014 bias    0.302\n",
      "iter 4140/10000  loss         0.132487  avg_L1_norm_grad         0.000685  w[0]   -0.014 bias    0.302\n",
      "iter 4141/10000  loss         0.132480  avg_L1_norm_grad         0.000685  w[0]   -0.014 bias    0.302\n",
      "iter 4160/10000  loss         0.132358  avg_L1_norm_grad         0.000682  w[0]   -0.014 bias    0.303\n",
      "iter 4161/10000  loss         0.132351  avg_L1_norm_grad         0.000682  w[0]   -0.014 bias    0.303\n",
      "iter 4180/10000  loss         0.132230  avg_L1_norm_grad         0.000680  w[0]   -0.014 bias    0.304\n",
      "iter 4181/10000  loss         0.132224  avg_L1_norm_grad         0.000680  w[0]   -0.014 bias    0.304\n",
      "iter 4200/10000  loss         0.132103  avg_L1_norm_grad         0.000678  w[0]   -0.014 bias    0.305\n",
      "iter 4201/10000  loss         0.132097  avg_L1_norm_grad         0.000677  w[0]   -0.014 bias    0.305\n",
      "iter 4220/10000  loss         0.131977  avg_L1_norm_grad         0.000675  w[0]   -0.014 bias    0.305\n",
      "iter 4221/10000  loss         0.131971  avg_L1_norm_grad         0.000675  w[0]   -0.014 bias    0.305\n",
      "iter 4240/10000  loss         0.131853  avg_L1_norm_grad         0.000673  w[0]   -0.014 bias    0.306\n",
      "iter 4241/10000  loss         0.131846  avg_L1_norm_grad         0.000673  w[0]   -0.014 bias    0.306\n",
      "iter 4260/10000  loss         0.131728  avg_L1_norm_grad         0.000671  w[0]   -0.015 bias    0.307\n",
      "iter 4261/10000  loss         0.131722  avg_L1_norm_grad         0.000670  w[0]   -0.015 bias    0.307\n",
      "iter 4280/10000  loss         0.131605  avg_L1_norm_grad         0.000668  w[0]   -0.015 bias    0.308\n",
      "iter 4281/10000  loss         0.131599  avg_L1_norm_grad         0.000668  w[0]   -0.015 bias    0.308\n",
      "iter 4300/10000  loss         0.131483  avg_L1_norm_grad         0.000666  w[0]   -0.015 bias    0.308\n",
      "iter 4301/10000  loss         0.131477  avg_L1_norm_grad         0.000666  w[0]   -0.015 bias    0.309\n",
      "iter 4320/10000  loss         0.131362  avg_L1_norm_grad         0.000664  w[0]   -0.015 bias    0.309\n",
      "iter 4321/10000  loss         0.131356  avg_L1_norm_grad         0.000664  w[0]   -0.015 bias    0.309\n",
      "iter 4340/10000  loss         0.131241  avg_L1_norm_grad         0.000662  w[0]   -0.015 bias    0.310\n",
      "iter 4341/10000  loss         0.131235  avg_L1_norm_grad         0.000661  w[0]   -0.015 bias    0.310\n",
      "iter 4360/10000  loss         0.131122  avg_L1_norm_grad         0.000659  w[0]   -0.015 bias    0.311\n",
      "iter 4361/10000  loss         0.131116  avg_L1_norm_grad         0.000659  w[0]   -0.015 bias    0.311\n",
      "iter 4380/10000  loss         0.131003  avg_L1_norm_grad         0.000657  w[0]   -0.015 bias    0.311\n",
      "iter 4381/10000  loss         0.130997  avg_L1_norm_grad         0.000657  w[0]   -0.015 bias    0.312\n",
      "iter 4400/10000  loss         0.130885  avg_L1_norm_grad         0.000655  w[0]   -0.015 bias    0.312\n",
      "iter 4401/10000  loss         0.130879  avg_L1_norm_grad         0.000655  w[0]   -0.015 bias    0.312\n",
      "iter 4420/10000  loss         0.130768  avg_L1_norm_grad         0.000653  w[0]   -0.015 bias    0.313\n",
      "iter 4421/10000  loss         0.130762  avg_L1_norm_grad         0.000653  w[0]   -0.015 bias    0.313\n",
      "iter 4440/10000  loss         0.130652  avg_L1_norm_grad         0.000651  w[0]   -0.015 bias    0.314\n",
      "iter 4441/10000  loss         0.130646  avg_L1_norm_grad         0.000651  w[0]   -0.015 bias    0.314\n",
      "iter 4460/10000  loss         0.130537  avg_L1_norm_grad         0.000648  w[0]   -0.015 bias    0.314\n",
      "iter 4461/10000  loss         0.130531  avg_L1_norm_grad         0.000648  w[0]   -0.015 bias    0.314\n",
      "iter 4480/10000  loss         0.130422  avg_L1_norm_grad         0.000646  w[0]   -0.015 bias    0.315\n",
      "iter 4481/10000  loss         0.130416  avg_L1_norm_grad         0.000646  w[0]   -0.015 bias    0.315\n",
      "iter 4500/10000  loss         0.130308  avg_L1_norm_grad         0.000644  w[0]   -0.016 bias    0.316\n",
      "iter 4501/10000  loss         0.130303  avg_L1_norm_grad         0.000644  w[0]   -0.016 bias    0.316\n",
      "iter 4520/10000  loss         0.130195  avg_L1_norm_grad         0.000642  w[0]   -0.016 bias    0.317\n",
      "iter 4521/10000  loss         0.130190  avg_L1_norm_grad         0.000642  w[0]   -0.016 bias    0.317\n",
      "iter 4540/10000  loss         0.130083  avg_L1_norm_grad         0.000640  w[0]   -0.016 bias    0.317\n",
      "iter 4541/10000  loss         0.130078  avg_L1_norm_grad         0.000640  w[0]   -0.016 bias    0.317\n",
      "iter 4560/10000  loss         0.129972  avg_L1_norm_grad         0.000638  w[0]   -0.016 bias    0.318\n",
      "iter 4561/10000  loss         0.129966  avg_L1_norm_grad         0.000638  w[0]   -0.016 bias    0.318\n",
      "iter 4580/10000  loss         0.129861  avg_L1_norm_grad         0.000636  w[0]   -0.016 bias    0.319\n",
      "iter 4581/10000  loss         0.129856  avg_L1_norm_grad         0.000636  w[0]   -0.016 bias    0.319\n",
      "iter 4600/10000  loss         0.129751  avg_L1_norm_grad         0.000634  w[0]   -0.016 bias    0.320\n",
      "iter 4601/10000  loss         0.129746  avg_L1_norm_grad         0.000634  w[0]   -0.016 bias    0.320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4620/10000  loss         0.129642  avg_L1_norm_grad         0.000632  w[0]   -0.016 bias    0.320\n",
      "iter 4621/10000  loss         0.129637  avg_L1_norm_grad         0.000632  w[0]   -0.016 bias    0.320\n",
      "iter 4640/10000  loss         0.129534  avg_L1_norm_grad         0.000630  w[0]   -0.016 bias    0.321\n",
      "iter 4641/10000  loss         0.129528  avg_L1_norm_grad         0.000630  w[0]   -0.016 bias    0.321\n",
      "iter 4660/10000  loss         0.129426  avg_L1_norm_grad         0.000628  w[0]   -0.016 bias    0.322\n",
      "iter 4661/10000  loss         0.129421  avg_L1_norm_grad         0.000628  w[0]   -0.016 bias    0.322\n",
      "iter 4680/10000  loss         0.129319  avg_L1_norm_grad         0.000626  w[0]   -0.016 bias    0.322\n",
      "iter 4681/10000  loss         0.129314  avg_L1_norm_grad         0.000626  w[0]   -0.016 bias    0.322\n",
      "iter 4700/10000  loss         0.129213  avg_L1_norm_grad         0.000624  w[0]   -0.016 bias    0.323\n",
      "iter 4701/10000  loss         0.129208  avg_L1_norm_grad         0.000624  w[0]   -0.016 bias    0.323\n",
      "iter 4720/10000  loss         0.129108  avg_L1_norm_grad         0.000622  w[0]   -0.016 bias    0.324\n",
      "iter 4721/10000  loss         0.129102  avg_L1_norm_grad         0.000622  w[0]   -0.016 bias    0.324\n",
      "iter 4740/10000  loss         0.129003  avg_L1_norm_grad         0.000620  w[0]   -0.017 bias    0.325\n",
      "iter 4741/10000  loss         0.128998  avg_L1_norm_grad         0.000620  w[0]   -0.017 bias    0.325\n",
      "iter 4760/10000  loss         0.128899  avg_L1_norm_grad         0.000618  w[0]   -0.017 bias    0.325\n",
      "iter 4761/10000  loss         0.128893  avg_L1_norm_grad         0.000618  w[0]   -0.017 bias    0.325\n",
      "iter 4780/10000  loss         0.128795  avg_L1_norm_grad         0.000616  w[0]   -0.017 bias    0.326\n",
      "iter 4781/10000  loss         0.128790  avg_L1_norm_grad         0.000616  w[0]   -0.017 bias    0.326\n",
      "iter 4800/10000  loss         0.128692  avg_L1_norm_grad         0.000614  w[0]   -0.017 bias    0.327\n",
      "iter 4801/10000  loss         0.128687  avg_L1_norm_grad         0.000614  w[0]   -0.017 bias    0.327\n",
      "iter 4820/10000  loss         0.128590  avg_L1_norm_grad         0.000612  w[0]   -0.017 bias    0.327\n",
      "iter 4821/10000  loss         0.128585  avg_L1_norm_grad         0.000612  w[0]   -0.017 bias    0.327\n",
      "iter 4840/10000  loss         0.128489  avg_L1_norm_grad         0.000611  w[0]   -0.017 bias    0.328\n",
      "iter 4841/10000  loss         0.128484  avg_L1_norm_grad         0.000610  w[0]   -0.017 bias    0.328\n",
      "iter 4860/10000  loss         0.128388  avg_L1_norm_grad         0.000609  w[0]   -0.017 bias    0.329\n",
      "iter 4861/10000  loss         0.128383  avg_L1_norm_grad         0.000609  w[0]   -0.017 bias    0.329\n",
      "iter 4880/10000  loss         0.128288  avg_L1_norm_grad         0.000607  w[0]   -0.017 bias    0.329\n",
      "iter 4881/10000  loss         0.128283  avg_L1_norm_grad         0.000607  w[0]   -0.017 bias    0.330\n",
      "iter 4900/10000  loss         0.128189  avg_L1_norm_grad         0.000605  w[0]   -0.017 bias    0.330\n",
      "iter 4901/10000  loss         0.128184  avg_L1_norm_grad         0.000605  w[0]   -0.017 bias    0.330\n",
      "iter 4920/10000  loss         0.128090  avg_L1_norm_grad         0.000603  w[0]   -0.017 bias    0.331\n",
      "iter 4921/10000  loss         0.128085  avg_L1_norm_grad         0.000603  w[0]   -0.017 bias    0.331\n",
      "iter 4940/10000  loss         0.127992  avg_L1_norm_grad         0.000601  w[0]   -0.017 bias    0.332\n",
      "iter 4941/10000  loss         0.127987  avg_L1_norm_grad         0.000601  w[0]   -0.017 bias    0.332\n",
      "iter 4960/10000  loss         0.127894  avg_L1_norm_grad         0.000599  w[0]   -0.017 bias    0.332\n",
      "iter 4961/10000  loss         0.127889  avg_L1_norm_grad         0.000599  w[0]   -0.017 bias    0.332\n",
      "iter 4980/10000  loss         0.127797  avg_L1_norm_grad         0.000598  w[0]   -0.018 bias    0.333\n",
      "iter 4981/10000  loss         0.127793  avg_L1_norm_grad         0.000598  w[0]   -0.018 bias    0.333\n",
      "iter 5000/10000  loss         0.127701  avg_L1_norm_grad         0.000596  w[0]   -0.018 bias    0.334\n",
      "iter 5001/10000  loss         0.127696  avg_L1_norm_grad         0.000596  w[0]   -0.018 bias    0.334\n",
      "iter 5020/10000  loss         0.127605  avg_L1_norm_grad         0.000594  w[0]   -0.018 bias    0.334\n",
      "iter 5021/10000  loss         0.127601  avg_L1_norm_grad         0.000594  w[0]   -0.018 bias    0.334\n",
      "iter 5040/10000  loss         0.127510  avg_L1_norm_grad         0.000592  w[0]   -0.018 bias    0.335\n",
      "iter 5041/10000  loss         0.127506  avg_L1_norm_grad         0.000592  w[0]   -0.018 bias    0.335\n",
      "iter 5060/10000  loss         0.127416  avg_L1_norm_grad         0.000591  w[0]   -0.018 bias    0.336\n",
      "iter 5061/10000  loss         0.127411  avg_L1_norm_grad         0.000591  w[0]   -0.018 bias    0.336\n",
      "iter 5080/10000  loss         0.127322  avg_L1_norm_grad         0.000589  w[0]   -0.018 bias    0.336\n",
      "iter 5081/10000  loss         0.127317  avg_L1_norm_grad         0.000589  w[0]   -0.018 bias    0.336\n",
      "iter 5100/10000  loss         0.127229  avg_L1_norm_grad         0.000587  w[0]   -0.018 bias    0.337\n",
      "iter 5101/10000  loss         0.127224  avg_L1_norm_grad         0.000587  w[0]   -0.018 bias    0.337\n",
      "iter 5120/10000  loss         0.127136  avg_L1_norm_grad         0.000585  w[0]   -0.018 bias    0.338\n",
      "iter 5121/10000  loss         0.127131  avg_L1_norm_grad         0.000585  w[0]   -0.018 bias    0.338\n",
      "iter 5140/10000  loss         0.127044  avg_L1_norm_grad         0.000584  w[0]   -0.018 bias    0.338\n",
      "iter 5141/10000  loss         0.127039  avg_L1_norm_grad         0.000584  w[0]   -0.018 bias    0.338\n",
      "iter 5160/10000  loss         0.126952  avg_L1_norm_grad         0.000582  w[0]   -0.018 bias    0.339\n",
      "iter 5161/10000  loss         0.126947  avg_L1_norm_grad         0.000582  w[0]   -0.018 bias    0.339\n",
      "iter 5180/10000  loss         0.126861  avg_L1_norm_grad         0.000580  w[0]   -0.018 bias    0.340\n",
      "iter 5181/10000  loss         0.126856  avg_L1_norm_grad         0.000580  w[0]   -0.018 bias    0.340\n",
      "iter 5200/10000  loss         0.126771  avg_L1_norm_grad         0.000579  w[0]   -0.018 bias    0.340\n",
      "iter 5201/10000  loss         0.126766  avg_L1_norm_grad         0.000579  w[0]   -0.018 bias    0.340\n",
      "iter 5220/10000  loss         0.126681  avg_L1_norm_grad         0.000577  w[0]   -0.019 bias    0.341\n",
      "iter 5221/10000  loss         0.126676  avg_L1_norm_grad         0.000577  w[0]   -0.019 bias    0.341\n",
      "iter 5240/10000  loss         0.126591  avg_L1_norm_grad         0.000575  w[0]   -0.019 bias    0.342\n",
      "iter 5241/10000  loss         0.126587  avg_L1_norm_grad         0.000575  w[0]   -0.019 bias    0.342\n",
      "iter 5260/10000  loss         0.126502  avg_L1_norm_grad         0.000574  w[0]   -0.019 bias    0.342\n",
      "iter 5261/10000  loss         0.126498  avg_L1_norm_grad         0.000574  w[0]   -0.019 bias    0.342\n",
      "iter 5280/10000  loss         0.126414  avg_L1_norm_grad         0.000572  w[0]   -0.019 bias    0.343\n",
      "iter 5281/10000  loss         0.126410  avg_L1_norm_grad         0.000572  w[0]   -0.019 bias    0.343\n",
      "iter 5300/10000  loss         0.126326  avg_L1_norm_grad         0.000570  w[0]   -0.019 bias    0.344\n",
      "iter 5301/10000  loss         0.126322  avg_L1_norm_grad         0.000570  w[0]   -0.019 bias    0.344\n",
      "iter 5320/10000  loss         0.126239  avg_L1_norm_grad         0.000569  w[0]   -0.019 bias    0.344\n",
      "iter 5321/10000  loss         0.126235  avg_L1_norm_grad         0.000569  w[0]   -0.019 bias    0.344\n",
      "iter 5340/10000  loss         0.126152  avg_L1_norm_grad         0.000567  w[0]   -0.019 bias    0.345\n",
      "iter 5341/10000  loss         0.126148  avg_L1_norm_grad         0.000567  w[0]   -0.019 bias    0.345\n",
      "iter 5360/10000  loss         0.126066  avg_L1_norm_grad         0.000566  w[0]   -0.019 bias    0.346\n",
      "iter 5361/10000  loss         0.126062  avg_L1_norm_grad         0.000565  w[0]   -0.019 bias    0.346\n",
      "iter 5380/10000  loss         0.125980  avg_L1_norm_grad         0.000564  w[0]   -0.019 bias    0.346\n",
      "iter 5381/10000  loss         0.125976  avg_L1_norm_grad         0.000564  w[0]   -0.019 bias    0.346\n",
      "iter 5400/10000  loss         0.125895  avg_L1_norm_grad         0.000562  w[0]   -0.019 bias    0.347\n",
      "iter 5401/10000  loss         0.125891  avg_L1_norm_grad         0.000562  w[0]   -0.019 bias    0.347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5420/10000  loss         0.125810  avg_L1_norm_grad         0.000561  w[0]   -0.019 bias    0.348\n",
      "iter 5421/10000  loss         0.125806  avg_L1_norm_grad         0.000561  w[0]   -0.019 bias    0.348\n",
      "iter 5440/10000  loss         0.125726  avg_L1_norm_grad         0.000559  w[0]   -0.019 bias    0.348\n",
      "iter 5441/10000  loss         0.125722  avg_L1_norm_grad         0.000559  w[0]   -0.019 bias    0.348\n",
      "iter 5460/10000  loss         0.125642  avg_L1_norm_grad         0.000558  w[0]   -0.019 bias    0.349\n",
      "iter 5461/10000  loss         0.125638  avg_L1_norm_grad         0.000558  w[0]   -0.020 bias    0.349\n",
      "iter 5480/10000  loss         0.125559  avg_L1_norm_grad         0.000556  w[0]   -0.020 bias    0.350\n",
      "iter 5481/10000  loss         0.125555  avg_L1_norm_grad         0.000556  w[0]   -0.020 bias    0.350\n",
      "iter 5500/10000  loss         0.125476  avg_L1_norm_grad         0.000555  w[0]   -0.020 bias    0.350\n",
      "iter 5501/10000  loss         0.125472  avg_L1_norm_grad         0.000555  w[0]   -0.020 bias    0.350\n",
      "iter 5520/10000  loss         0.125394  avg_L1_norm_grad         0.000553  w[0]   -0.020 bias    0.351\n",
      "iter 5521/10000  loss         0.125390  avg_L1_norm_grad         0.000553  w[0]   -0.020 bias    0.351\n",
      "iter 5540/10000  loss         0.125312  avg_L1_norm_grad         0.000552  w[0]   -0.020 bias    0.352\n",
      "iter 5541/10000  loss         0.125308  avg_L1_norm_grad         0.000551  w[0]   -0.020 bias    0.352\n",
      "iter 5560/10000  loss         0.125231  avg_L1_norm_grad         0.000550  w[0]   -0.020 bias    0.352\n",
      "iter 5561/10000  loss         0.125227  avg_L1_norm_grad         0.000550  w[0]   -0.020 bias    0.352\n",
      "iter 5580/10000  loss         0.125150  avg_L1_norm_grad         0.000549  w[0]   -0.020 bias    0.353\n",
      "iter 5581/10000  loss         0.125146  avg_L1_norm_grad         0.000548  w[0]   -0.020 bias    0.353\n",
      "iter 5600/10000  loss         0.125070  avg_L1_norm_grad         0.000547  w[0]   -0.020 bias    0.354\n",
      "iter 5601/10000  loss         0.125066  avg_L1_norm_grad         0.000547  w[0]   -0.020 bias    0.354\n",
      "iter 5620/10000  loss         0.124990  avg_L1_norm_grad         0.000546  w[0]   -0.020 bias    0.354\n",
      "iter 5621/10000  loss         0.124986  avg_L1_norm_grad         0.000546  w[0]   -0.020 bias    0.354\n",
      "iter 5640/10000  loss         0.124910  avg_L1_norm_grad         0.000544  w[0]   -0.020 bias    0.355\n",
      "iter 5641/10000  loss         0.124906  avg_L1_norm_grad         0.000544  w[0]   -0.020 bias    0.355\n",
      "iter 5660/10000  loss         0.124831  avg_L1_norm_grad         0.000543  w[0]   -0.020 bias    0.355\n",
      "iter 5661/10000  loss         0.124827  avg_L1_norm_grad         0.000543  w[0]   -0.020 bias    0.355\n",
      "iter 5680/10000  loss         0.124752  avg_L1_norm_grad         0.000541  w[0]   -0.020 bias    0.356\n",
      "iter 5681/10000  loss         0.124749  avg_L1_norm_grad         0.000541  w[0]   -0.020 bias    0.356\n",
      "iter 5700/10000  loss         0.124674  avg_L1_norm_grad         0.000540  w[0]   -0.020 bias    0.357\n",
      "iter 5701/10000  loss         0.124670  avg_L1_norm_grad         0.000540  w[0]   -0.020 bias    0.357\n",
      "iter 5720/10000  loss         0.124597  avg_L1_norm_grad         0.000538  w[0]   -0.021 bias    0.357\n",
      "iter 5721/10000  loss         0.124593  avg_L1_norm_grad         0.000538  w[0]   -0.021 bias    0.357\n",
      "iter 5740/10000  loss         0.124519  avg_L1_norm_grad         0.000537  w[0]   -0.021 bias    0.358\n",
      "iter 5741/10000  loss         0.124515  avg_L1_norm_grad         0.000537  w[0]   -0.021 bias    0.358\n",
      "iter 5760/10000  loss         0.124442  avg_L1_norm_grad         0.000535  w[0]   -0.021 bias    0.359\n",
      "iter 5761/10000  loss         0.124439  avg_L1_norm_grad         0.000535  w[0]   -0.021 bias    0.359\n",
      "iter 5780/10000  loss         0.124366  avg_L1_norm_grad         0.000534  w[0]   -0.021 bias    0.359\n",
      "iter 5781/10000  loss         0.124362  avg_L1_norm_grad         0.000534  w[0]   -0.021 bias    0.359\n",
      "iter 5800/10000  loss         0.124290  avg_L1_norm_grad         0.000533  w[0]   -0.021 bias    0.360\n",
      "iter 5801/10000  loss         0.124286  avg_L1_norm_grad         0.000533  w[0]   -0.021 bias    0.360\n",
      "iter 5820/10000  loss         0.124214  avg_L1_norm_grad         0.000531  w[0]   -0.021 bias    0.360\n",
      "iter 5821/10000  loss         0.124211  avg_L1_norm_grad         0.000531  w[0]   -0.021 bias    0.361\n",
      "iter 5840/10000  loss         0.124139  avg_L1_norm_grad         0.000530  w[0]   -0.021 bias    0.361\n",
      "iter 5841/10000  loss         0.124135  avg_L1_norm_grad         0.000530  w[0]   -0.021 bias    0.361\n",
      "iter 5860/10000  loss         0.124064  avg_L1_norm_grad         0.000528  w[0]   -0.021 bias    0.362\n",
      "iter 5861/10000  loss         0.124061  avg_L1_norm_grad         0.000528  w[0]   -0.021 bias    0.362\n",
      "iter 5880/10000  loss         0.123990  avg_L1_norm_grad         0.000527  w[0]   -0.021 bias    0.362\n",
      "iter 5881/10000  loss         0.123986  avg_L1_norm_grad         0.000527  w[0]   -0.021 bias    0.362\n",
      "iter 5900/10000  loss         0.123916  avg_L1_norm_grad         0.000526  w[0]   -0.021 bias    0.363\n",
      "iter 5901/10000  loss         0.123912  avg_L1_norm_grad         0.000526  w[0]   -0.021 bias    0.363\n",
      "iter 5920/10000  loss         0.123842  avg_L1_norm_grad         0.000524  w[0]   -0.021 bias    0.364\n",
      "iter 5921/10000  loss         0.123839  avg_L1_norm_grad         0.000524  w[0]   -0.021 bias    0.364\n",
      "iter 5940/10000  loss         0.123769  avg_L1_norm_grad         0.000523  w[0]   -0.021 bias    0.364\n",
      "iter 5941/10000  loss         0.123766  avg_L1_norm_grad         0.000523  w[0]   -0.021 bias    0.364\n",
      "iter 5960/10000  loss         0.123696  avg_L1_norm_grad         0.000522  w[0]   -0.021 bias    0.365\n",
      "iter 5961/10000  loss         0.123693  avg_L1_norm_grad         0.000522  w[0]   -0.021 bias    0.365\n",
      "iter 5980/10000  loss         0.123624  avg_L1_norm_grad         0.000520  w[0]   -0.022 bias    0.365\n",
      "iter 5981/10000  loss         0.123620  avg_L1_norm_grad         0.000520  w[0]   -0.022 bias    0.365\n",
      "iter 6000/10000  loss         0.123552  avg_L1_norm_grad         0.000519  w[0]   -0.022 bias    0.366\n",
      "iter 6001/10000  loss         0.123548  avg_L1_norm_grad         0.000519  w[0]   -0.022 bias    0.366\n",
      "iter 6020/10000  loss         0.123480  avg_L1_norm_grad         0.000518  w[0]   -0.022 bias    0.367\n",
      "iter 6021/10000  loss         0.123477  avg_L1_norm_grad         0.000518  w[0]   -0.022 bias    0.367\n",
      "iter 6040/10000  loss         0.123409  avg_L1_norm_grad         0.000516  w[0]   -0.022 bias    0.367\n",
      "iter 6041/10000  loss         0.123406  avg_L1_norm_grad         0.000516  w[0]   -0.022 bias    0.367\n",
      "iter 6060/10000  loss         0.123338  avg_L1_norm_grad         0.000515  w[0]   -0.022 bias    0.368\n",
      "iter 6061/10000  loss         0.123335  avg_L1_norm_grad         0.000515  w[0]   -0.022 bias    0.368\n",
      "iter 6080/10000  loss         0.123268  avg_L1_norm_grad         0.000514  w[0]   -0.022 bias    0.369\n",
      "iter 6081/10000  loss         0.123264  avg_L1_norm_grad         0.000514  w[0]   -0.022 bias    0.369\n",
      "iter 6100/10000  loss         0.123198  avg_L1_norm_grad         0.000512  w[0]   -0.022 bias    0.369\n",
      "iter 6101/10000  loss         0.123194  avg_L1_norm_grad         0.000512  w[0]   -0.022 bias    0.369\n",
      "iter 6120/10000  loss         0.123128  avg_L1_norm_grad         0.000511  w[0]   -0.022 bias    0.370\n",
      "iter 6121/10000  loss         0.123124  avg_L1_norm_grad         0.000511  w[0]   -0.022 bias    0.370\n",
      "iter 6140/10000  loss         0.123058  avg_L1_norm_grad         0.000510  w[0]   -0.022 bias    0.370\n",
      "iter 6141/10000  loss         0.123055  avg_L1_norm_grad         0.000510  w[0]   -0.022 bias    0.370\n",
      "iter 6160/10000  loss         0.122989  avg_L1_norm_grad         0.000508  w[0]   -0.022 bias    0.371\n",
      "iter 6161/10000  loss         0.122986  avg_L1_norm_grad         0.000508  w[0]   -0.022 bias    0.371\n",
      "iter 6180/10000  loss         0.122921  avg_L1_norm_grad         0.000507  w[0]   -0.022 bias    0.372\n",
      "iter 6181/10000  loss         0.122917  avg_L1_norm_grad         0.000507  w[0]   -0.022 bias    0.372\n",
      "iter 6200/10000  loss         0.122852  avg_L1_norm_grad         0.000506  w[0]   -0.022 bias    0.372\n",
      "iter 6201/10000  loss         0.122849  avg_L1_norm_grad         0.000506  w[0]   -0.022 bias    0.372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6220/10000  loss         0.122784  avg_L1_norm_grad         0.000505  w[0]   -0.022 bias    0.373\n",
      "iter 6221/10000  loss         0.122781  avg_L1_norm_grad         0.000505  w[0]   -0.022 bias    0.373\n",
      "iter 6240/10000  loss         0.122717  avg_L1_norm_grad         0.000503  w[0]   -0.022 bias    0.373\n",
      "iter 6241/10000  loss         0.122713  avg_L1_norm_grad         0.000503  w[0]   -0.022 bias    0.373\n",
      "iter 6260/10000  loss         0.122650  avg_L1_norm_grad         0.000502  w[0]   -0.023 bias    0.374\n",
      "iter 6261/10000  loss         0.122646  avg_L1_norm_grad         0.000502  w[0]   -0.023 bias    0.374\n",
      "iter 6280/10000  loss         0.122583  avg_L1_norm_grad         0.000501  w[0]   -0.023 bias    0.375\n",
      "iter 6281/10000  loss         0.122579  avg_L1_norm_grad         0.000501  w[0]   -0.023 bias    0.375\n",
      "iter 6300/10000  loss         0.122516  avg_L1_norm_grad         0.000500  w[0]   -0.023 bias    0.375\n",
      "iter 6301/10000  loss         0.122513  avg_L1_norm_grad         0.000500  w[0]   -0.023 bias    0.375\n",
      "iter 6320/10000  loss         0.122450  avg_L1_norm_grad         0.000498  w[0]   -0.023 bias    0.376\n",
      "iter 6321/10000  loss         0.122446  avg_L1_norm_grad         0.000498  w[0]   -0.023 bias    0.376\n",
      "iter 6340/10000  loss         0.122384  avg_L1_norm_grad         0.000497  w[0]   -0.023 bias    0.376\n",
      "iter 6341/10000  loss         0.122381  avg_L1_norm_grad         0.000497  w[0]   -0.023 bias    0.376\n",
      "iter 6360/10000  loss         0.122318  avg_L1_norm_grad         0.000496  w[0]   -0.023 bias    0.377\n",
      "iter 6361/10000  loss         0.122315  avg_L1_norm_grad         0.000496  w[0]   -0.023 bias    0.377\n",
      "iter 6380/10000  loss         0.122253  avg_L1_norm_grad         0.000495  w[0]   -0.023 bias    0.378\n",
      "iter 6381/10000  loss         0.122250  avg_L1_norm_grad         0.000495  w[0]   -0.023 bias    0.378\n",
      "iter 6400/10000  loss         0.122188  avg_L1_norm_grad         0.000494  w[0]   -0.023 bias    0.378\n",
      "iter 6401/10000  loss         0.122185  avg_L1_norm_grad         0.000493  w[0]   -0.023 bias    0.378\n",
      "iter 6420/10000  loss         0.122124  avg_L1_norm_grad         0.000492  w[0]   -0.023 bias    0.379\n",
      "iter 6421/10000  loss         0.122120  avg_L1_norm_grad         0.000492  w[0]   -0.023 bias    0.379\n",
      "iter 6440/10000  loss         0.122059  avg_L1_norm_grad         0.000491  w[0]   -0.023 bias    0.379\n",
      "iter 6441/10000  loss         0.122056  avg_L1_norm_grad         0.000491  w[0]   -0.023 bias    0.379\n",
      "iter 6460/10000  loss         0.121995  avg_L1_norm_grad         0.000490  w[0]   -0.023 bias    0.380\n",
      "iter 6461/10000  loss         0.121992  avg_L1_norm_grad         0.000490  w[0]   -0.023 bias    0.380\n",
      "iter 6480/10000  loss         0.121932  avg_L1_norm_grad         0.000489  w[0]   -0.023 bias    0.380\n",
      "iter 6481/10000  loss         0.121929  avg_L1_norm_grad         0.000489  w[0]   -0.023 bias    0.381\n",
      "iter 6500/10000  loss         0.121868  avg_L1_norm_grad         0.000488  w[0]   -0.023 bias    0.381\n",
      "iter 6501/10000  loss         0.121865  avg_L1_norm_grad         0.000488  w[0]   -0.023 bias    0.381\n",
      "iter 6520/10000  loss         0.121805  avg_L1_norm_grad         0.000486  w[0]   -0.023 bias    0.382\n",
      "iter 6521/10000  loss         0.121802  avg_L1_norm_grad         0.000486  w[0]   -0.024 bias    0.382\n",
      "iter 6540/10000  loss         0.121743  avg_L1_norm_grad         0.000485  w[0]   -0.024 bias    0.382\n",
      "iter 6541/10000  loss         0.121740  avg_L1_norm_grad         0.000485  w[0]   -0.024 bias    0.382\n",
      "iter 6560/10000  loss         0.121680  avg_L1_norm_grad         0.000484  w[0]   -0.024 bias    0.383\n",
      "iter 6561/10000  loss         0.121677  avg_L1_norm_grad         0.000484  w[0]   -0.024 bias    0.383\n",
      "iter 6580/10000  loss         0.121618  avg_L1_norm_grad         0.000483  w[0]   -0.024 bias    0.383\n",
      "iter 6581/10000  loss         0.121615  avg_L1_norm_grad         0.000483  w[0]   -0.024 bias    0.383\n",
      "iter 6600/10000  loss         0.121556  avg_L1_norm_grad         0.000482  w[0]   -0.024 bias    0.384\n",
      "iter 6601/10000  loss         0.121553  avg_L1_norm_grad         0.000482  w[0]   -0.024 bias    0.384\n",
      "iter 6620/10000  loss         0.121495  avg_L1_norm_grad         0.000481  w[0]   -0.024 bias    0.385\n",
      "iter 6621/10000  loss         0.121492  avg_L1_norm_grad         0.000481  w[0]   -0.024 bias    0.385\n",
      "iter 6640/10000  loss         0.121434  avg_L1_norm_grad         0.000479  w[0]   -0.024 bias    0.385\n",
      "iter 6641/10000  loss         0.121431  avg_L1_norm_grad         0.000479  w[0]   -0.024 bias    0.385\n",
      "iter 6660/10000  loss         0.121373  avg_L1_norm_grad         0.000478  w[0]   -0.024 bias    0.386\n",
      "iter 6661/10000  loss         0.121370  avg_L1_norm_grad         0.000478  w[0]   -0.024 bias    0.386\n",
      "iter 6680/10000  loss         0.121312  avg_L1_norm_grad         0.000477  w[0]   -0.024 bias    0.386\n",
      "iter 6681/10000  loss         0.121309  avg_L1_norm_grad         0.000477  w[0]   -0.024 bias    0.386\n",
      "iter 6700/10000  loss         0.121252  avg_L1_norm_grad         0.000476  w[0]   -0.024 bias    0.387\n",
      "iter 6701/10000  loss         0.121249  avg_L1_norm_grad         0.000476  w[0]   -0.024 bias    0.387\n",
      "iter 6720/10000  loss         0.121192  avg_L1_norm_grad         0.000475  w[0]   -0.024 bias    0.387\n",
      "iter 6721/10000  loss         0.121189  avg_L1_norm_grad         0.000475  w[0]   -0.024 bias    0.387\n",
      "iter 6740/10000  loss         0.121132  avg_L1_norm_grad         0.000474  w[0]   -0.024 bias    0.388\n",
      "iter 6741/10000  loss         0.121129  avg_L1_norm_grad         0.000474  w[0]   -0.024 bias    0.388\n",
      "iter 6760/10000  loss         0.121073  avg_L1_norm_grad         0.000473  w[0]   -0.024 bias    0.389\n",
      "iter 6761/10000  loss         0.121070  avg_L1_norm_grad         0.000473  w[0]   -0.024 bias    0.389\n",
      "iter 6780/10000  loss         0.121014  avg_L1_norm_grad         0.000472  w[0]   -0.024 bias    0.389\n",
      "iter 6781/10000  loss         0.121011  avg_L1_norm_grad         0.000472  w[0]   -0.024 bias    0.389\n",
      "iter 6800/10000  loss         0.120955  avg_L1_norm_grad         0.000471  w[0]   -0.024 bias    0.390\n",
      "iter 6801/10000  loss         0.120952  avg_L1_norm_grad         0.000471  w[0]   -0.024 bias    0.390\n",
      "iter 6820/10000  loss         0.120896  avg_L1_norm_grad         0.000469  w[0]   -0.025 bias    0.390\n",
      "iter 6821/10000  loss         0.120893  avg_L1_norm_grad         0.000469  w[0]   -0.025 bias    0.390\n",
      "iter 6840/10000  loss         0.120838  avg_L1_norm_grad         0.000468  w[0]   -0.025 bias    0.391\n",
      "iter 6841/10000  loss         0.120835  avg_L1_norm_grad         0.000468  w[0]   -0.025 bias    0.391\n",
      "iter 6860/10000  loss         0.120780  avg_L1_norm_grad         0.000467  w[0]   -0.025 bias    0.391\n",
      "iter 6861/10000  loss         0.120777  avg_L1_norm_grad         0.000467  w[0]   -0.025 bias    0.391\n",
      "iter 6880/10000  loss         0.120722  avg_L1_norm_grad         0.000466  w[0]   -0.025 bias    0.392\n",
      "iter 6881/10000  loss         0.120719  avg_L1_norm_grad         0.000466  w[0]   -0.025 bias    0.392\n",
      "iter 6900/10000  loss         0.120665  avg_L1_norm_grad         0.000465  w[0]   -0.025 bias    0.393\n",
      "iter 6901/10000  loss         0.120662  avg_L1_norm_grad         0.000465  w[0]   -0.025 bias    0.393\n",
      "iter 6920/10000  loss         0.120608  avg_L1_norm_grad         0.000464  w[0]   -0.025 bias    0.393\n",
      "iter 6921/10000  loss         0.120605  avg_L1_norm_grad         0.000464  w[0]   -0.025 bias    0.393\n",
      "iter 6940/10000  loss         0.120551  avg_L1_norm_grad         0.000463  w[0]   -0.025 bias    0.394\n",
      "iter 6941/10000  loss         0.120548  avg_L1_norm_grad         0.000463  w[0]   -0.025 bias    0.394\n",
      "iter 6960/10000  loss         0.120494  avg_L1_norm_grad         0.000462  w[0]   -0.025 bias    0.394\n",
      "iter 6961/10000  loss         0.120491  avg_L1_norm_grad         0.000462  w[0]   -0.025 bias    0.394\n",
      "iter 6980/10000  loss         0.120438  avg_L1_norm_grad         0.000461  w[0]   -0.025 bias    0.395\n",
      "iter 6981/10000  loss         0.120435  avg_L1_norm_grad         0.000461  w[0]   -0.025 bias    0.395\n",
      "iter 7000/10000  loss         0.120381  avg_L1_norm_grad         0.000460  w[0]   -0.025 bias    0.395\n",
      "iter 7001/10000  loss         0.120379  avg_L1_norm_grad         0.000460  w[0]   -0.025 bias    0.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7020/10000  loss         0.120326  avg_L1_norm_grad         0.000459  w[0]   -0.025 bias    0.396\n",
      "iter 7021/10000  loss         0.120323  avg_L1_norm_grad         0.000459  w[0]   -0.025 bias    0.396\n",
      "iter 7040/10000  loss         0.120270  avg_L1_norm_grad         0.000458  w[0]   -0.025 bias    0.396\n",
      "iter 7041/10000  loss         0.120267  avg_L1_norm_grad         0.000458  w[0]   -0.025 bias    0.396\n",
      "iter 7060/10000  loss         0.120214  avg_L1_norm_grad         0.000457  w[0]   -0.025 bias    0.397\n",
      "iter 7061/10000  loss         0.120212  avg_L1_norm_grad         0.000457  w[0]   -0.025 bias    0.397\n",
      "iter 7080/10000  loss         0.120159  avg_L1_norm_grad         0.000456  w[0]   -0.025 bias    0.398\n",
      "iter 7081/10000  loss         0.120157  avg_L1_norm_grad         0.000456  w[0]   -0.025 bias    0.398\n",
      "iter 7100/10000  loss         0.120104  avg_L1_norm_grad         0.000455  w[0]   -0.026 bias    0.398\n",
      "iter 7101/10000  loss         0.120102  avg_L1_norm_grad         0.000455  w[0]   -0.026 bias    0.398\n",
      "iter 7120/10000  loss         0.120050  avg_L1_norm_grad         0.000454  w[0]   -0.026 bias    0.399\n",
      "iter 7121/10000  loss         0.120047  avg_L1_norm_grad         0.000454  w[0]   -0.026 bias    0.399\n",
      "iter 7140/10000  loss         0.119995  avg_L1_norm_grad         0.000453  w[0]   -0.026 bias    0.399\n",
      "iter 7141/10000  loss         0.119993  avg_L1_norm_grad         0.000453  w[0]   -0.026 bias    0.399\n",
      "iter 7160/10000  loss         0.119941  avg_L1_norm_grad         0.000452  w[0]   -0.026 bias    0.400\n",
      "iter 7161/10000  loss         0.119939  avg_L1_norm_grad         0.000452  w[0]   -0.026 bias    0.400\n",
      "iter 7180/10000  loss         0.119887  avg_L1_norm_grad         0.000451  w[0]   -0.026 bias    0.400\n",
      "iter 7181/10000  loss         0.119885  avg_L1_norm_grad         0.000451  w[0]   -0.026 bias    0.400\n",
      "iter 7200/10000  loss         0.119834  avg_L1_norm_grad         0.000450  w[0]   -0.026 bias    0.401\n",
      "iter 7201/10000  loss         0.119831  avg_L1_norm_grad         0.000450  w[0]   -0.026 bias    0.401\n",
      "iter 7220/10000  loss         0.119780  avg_L1_norm_grad         0.000449  w[0]   -0.026 bias    0.401\n",
      "iter 7221/10000  loss         0.119778  avg_L1_norm_grad         0.000449  w[0]   -0.026 bias    0.401\n",
      "iter 7240/10000  loss         0.119727  avg_L1_norm_grad         0.000448  w[0]   -0.026 bias    0.402\n",
      "iter 7241/10000  loss         0.119725  avg_L1_norm_grad         0.000448  w[0]   -0.026 bias    0.402\n",
      "iter 7260/10000  loss         0.119674  avg_L1_norm_grad         0.000447  w[0]   -0.026 bias    0.403\n",
      "iter 7261/10000  loss         0.119672  avg_L1_norm_grad         0.000447  w[0]   -0.026 bias    0.403\n",
      "iter 7280/10000  loss         0.119622  avg_L1_norm_grad         0.000446  w[0]   -0.026 bias    0.403\n",
      "iter 7281/10000  loss         0.119619  avg_L1_norm_grad         0.000446  w[0]   -0.026 bias    0.403\n",
      "iter 7300/10000  loss         0.119569  avg_L1_norm_grad         0.000445  w[0]   -0.026 bias    0.404\n",
      "iter 7301/10000  loss         0.119567  avg_L1_norm_grad         0.000445  w[0]   -0.026 bias    0.404\n",
      "iter 7320/10000  loss         0.119517  avg_L1_norm_grad         0.000444  w[0]   -0.026 bias    0.404\n",
      "iter 7321/10000  loss         0.119514  avg_L1_norm_grad         0.000444  w[0]   -0.026 bias    0.404\n",
      "iter 7340/10000  loss         0.119465  avg_L1_norm_grad         0.000443  w[0]   -0.026 bias    0.405\n",
      "iter 7341/10000  loss         0.119462  avg_L1_norm_grad         0.000443  w[0]   -0.026 bias    0.405\n",
      "iter 7360/10000  loss         0.119413  avg_L1_norm_grad         0.000442  w[0]   -0.026 bias    0.405\n",
      "iter 7361/10000  loss         0.119411  avg_L1_norm_grad         0.000442  w[0]   -0.026 bias    0.405\n",
      "iter 7380/10000  loss         0.119362  avg_L1_norm_grad         0.000441  w[0]   -0.026 bias    0.406\n",
      "iter 7381/10000  loss         0.119359  avg_L1_norm_grad         0.000441  w[0]   -0.026 bias    0.406\n",
      "iter 7400/10000  loss         0.119310  avg_L1_norm_grad         0.000440  w[0]   -0.027 bias    0.406\n",
      "iter 7401/10000  loss         0.119308  avg_L1_norm_grad         0.000440  w[0]   -0.027 bias    0.406\n",
      "iter 7420/10000  loss         0.119259  avg_L1_norm_grad         0.000439  w[0]   -0.027 bias    0.407\n",
      "iter 7421/10000  loss         0.119257  avg_L1_norm_grad         0.000439  w[0]   -0.027 bias    0.407\n",
      "iter 7440/10000  loss         0.119209  avg_L1_norm_grad         0.000438  w[0]   -0.027 bias    0.407\n",
      "iter 7441/10000  loss         0.119206  avg_L1_norm_grad         0.000438  w[0]   -0.027 bias    0.407\n",
      "iter 7460/10000  loss         0.119158  avg_L1_norm_grad         0.000437  w[0]   -0.027 bias    0.408\n",
      "iter 7461/10000  loss         0.119155  avg_L1_norm_grad         0.000437  w[0]   -0.027 bias    0.408\n",
      "iter 7480/10000  loss         0.119108  avg_L1_norm_grad         0.000436  w[0]   -0.027 bias    0.408\n",
      "iter 7481/10000  loss         0.119105  avg_L1_norm_grad         0.000436  w[0]   -0.027 bias    0.408\n",
      "iter 7500/10000  loss         0.119057  avg_L1_norm_grad         0.000435  w[0]   -0.027 bias    0.409\n",
      "iter 7501/10000  loss         0.119055  avg_L1_norm_grad         0.000435  w[0]   -0.027 bias    0.409\n",
      "iter 7520/10000  loss         0.119007  avg_L1_norm_grad         0.000434  w[0]   -0.027 bias    0.410\n",
      "iter 7521/10000  loss         0.119005  avg_L1_norm_grad         0.000434  w[0]   -0.027 bias    0.410\n",
      "iter 7540/10000  loss         0.118958  avg_L1_norm_grad         0.000433  w[0]   -0.027 bias    0.410\n",
      "iter 7541/10000  loss         0.118955  avg_L1_norm_grad         0.000433  w[0]   -0.027 bias    0.410\n",
      "iter 7560/10000  loss         0.118908  avg_L1_norm_grad         0.000433  w[0]   -0.027 bias    0.411\n",
      "iter 7561/10000  loss         0.118906  avg_L1_norm_grad         0.000432  w[0]   -0.027 bias    0.411\n",
      "iter 7580/10000  loss         0.118859  avg_L1_norm_grad         0.000432  w[0]   -0.027 bias    0.411\n",
      "iter 7581/10000  loss         0.118856  avg_L1_norm_grad         0.000432  w[0]   -0.027 bias    0.411\n",
      "iter 7600/10000  loss         0.118810  avg_L1_norm_grad         0.000431  w[0]   -0.027 bias    0.412\n",
      "iter 7601/10000  loss         0.118807  avg_L1_norm_grad         0.000431  w[0]   -0.027 bias    0.412\n",
      "iter 7620/10000  loss         0.118761  avg_L1_norm_grad         0.000430  w[0]   -0.027 bias    0.412\n",
      "iter 7621/10000  loss         0.118758  avg_L1_norm_grad         0.000430  w[0]   -0.027 bias    0.412\n",
      "iter 7640/10000  loss         0.118712  avg_L1_norm_grad         0.000429  w[0]   -0.027 bias    0.413\n",
      "iter 7641/10000  loss         0.118710  avg_L1_norm_grad         0.000429  w[0]   -0.027 bias    0.413\n",
      "iter 7660/10000  loss         0.118664  avg_L1_norm_grad         0.000428  w[0]   -0.027 bias    0.413\n",
      "iter 7661/10000  loss         0.118661  avg_L1_norm_grad         0.000428  w[0]   -0.027 bias    0.413\n",
      "iter 7680/10000  loss         0.118615  avg_L1_norm_grad         0.000427  w[0]   -0.027 bias    0.414\n",
      "iter 7681/10000  loss         0.118613  avg_L1_norm_grad         0.000427  w[0]   -0.027 bias    0.414\n",
      "iter 7700/10000  loss         0.118567  avg_L1_norm_grad         0.000426  w[0]   -0.028 bias    0.414\n",
      "iter 7701/10000  loss         0.118565  avg_L1_norm_grad         0.000426  w[0]   -0.028 bias    0.414\n",
      "iter 7720/10000  loss         0.118519  avg_L1_norm_grad         0.000425  w[0]   -0.028 bias    0.415\n",
      "iter 7721/10000  loss         0.118517  avg_L1_norm_grad         0.000425  w[0]   -0.028 bias    0.415\n",
      "iter 7740/10000  loss         0.118472  avg_L1_norm_grad         0.000424  w[0]   -0.028 bias    0.415\n",
      "iter 7741/10000  loss         0.118469  avg_L1_norm_grad         0.000424  w[0]   -0.028 bias    0.415\n",
      "iter 7760/10000  loss         0.118424  avg_L1_norm_grad         0.000424  w[0]   -0.028 bias    0.416\n",
      "iter 7761/10000  loss         0.118422  avg_L1_norm_grad         0.000424  w[0]   -0.028 bias    0.416\n",
      "iter 7780/10000  loss         0.118377  avg_L1_norm_grad         0.000423  w[0]   -0.028 bias    0.416\n",
      "iter 7781/10000  loss         0.118375  avg_L1_norm_grad         0.000423  w[0]   -0.028 bias    0.416\n",
      "iter 7800/10000  loss         0.118330  avg_L1_norm_grad         0.000422  w[0]   -0.028 bias    0.417\n",
      "iter 7801/10000  loss         0.118328  avg_L1_norm_grad         0.000422  w[0]   -0.028 bias    0.417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7820/10000  loss         0.118283  avg_L1_norm_grad         0.000421  w[0]   -0.028 bias    0.417\n",
      "iter 7821/10000  loss         0.118281  avg_L1_norm_grad         0.000421  w[0]   -0.028 bias    0.417\n",
      "iter 7840/10000  loss         0.118236  avg_L1_norm_grad         0.000420  w[0]   -0.028 bias    0.418\n",
      "iter 7841/10000  loss         0.118234  avg_L1_norm_grad         0.000420  w[0]   -0.028 bias    0.418\n",
      "iter 7860/10000  loss         0.118190  avg_L1_norm_grad         0.000419  w[0]   -0.028 bias    0.418\n",
      "iter 7861/10000  loss         0.118188  avg_L1_norm_grad         0.000419  w[0]   -0.028 bias    0.418\n",
      "iter 7880/10000  loss         0.118144  avg_L1_norm_grad         0.000418  w[0]   -0.028 bias    0.419\n",
      "iter 7881/10000  loss         0.118141  avg_L1_norm_grad         0.000418  w[0]   -0.028 bias    0.419\n",
      "iter 7900/10000  loss         0.118098  avg_L1_norm_grad         0.000418  w[0]   -0.028 bias    0.420\n",
      "iter 7901/10000  loss         0.118095  avg_L1_norm_grad         0.000418  w[0]   -0.028 bias    0.420\n",
      "iter 7920/10000  loss         0.118052  avg_L1_norm_grad         0.000417  w[0]   -0.028 bias    0.420\n",
      "iter 7921/10000  loss         0.118049  avg_L1_norm_grad         0.000417  w[0]   -0.028 bias    0.420\n",
      "iter 7940/10000  loss         0.118006  avg_L1_norm_grad         0.000416  w[0]   -0.028 bias    0.421\n",
      "iter 7941/10000  loss         0.118004  avg_L1_norm_grad         0.000416  w[0]   -0.028 bias    0.421\n",
      "iter 7960/10000  loss         0.117961  avg_L1_norm_grad         0.000415  w[0]   -0.028 bias    0.421\n",
      "iter 7961/10000  loss         0.117958  avg_L1_norm_grad         0.000415  w[0]   -0.028 bias    0.421\n",
      "iter 7980/10000  loss         0.117915  avg_L1_norm_grad         0.000414  w[0]   -0.028 bias    0.422\n",
      "iter 7981/10000  loss         0.117913  avg_L1_norm_grad         0.000414  w[0]   -0.028 bias    0.422\n",
      "iter 8000/10000  loss         0.117870  avg_L1_norm_grad         0.000413  w[0]   -0.028 bias    0.422\n",
      "iter 8001/10000  loss         0.117868  avg_L1_norm_grad         0.000413  w[0]   -0.028 bias    0.422\n",
      "iter 8020/10000  loss         0.117825  avg_L1_norm_grad         0.000413  w[0]   -0.029 bias    0.423\n",
      "iter 8021/10000  loss         0.117823  avg_L1_norm_grad         0.000412  w[0]   -0.029 bias    0.423\n",
      "iter 8040/10000  loss         0.117780  avg_L1_norm_grad         0.000412  w[0]   -0.029 bias    0.423\n",
      "iter 8041/10000  loss         0.117778  avg_L1_norm_grad         0.000412  w[0]   -0.029 bias    0.423\n",
      "iter 8060/10000  loss         0.117736  avg_L1_norm_grad         0.000411  w[0]   -0.029 bias    0.424\n",
      "iter 8061/10000  loss         0.117734  avg_L1_norm_grad         0.000411  w[0]   -0.029 bias    0.424\n",
      "iter 8080/10000  loss         0.117691  avg_L1_norm_grad         0.000410  w[0]   -0.029 bias    0.424\n",
      "iter 8081/10000  loss         0.117689  avg_L1_norm_grad         0.000410  w[0]   -0.029 bias    0.424\n",
      "iter 8100/10000  loss         0.117647  avg_L1_norm_grad         0.000409  w[0]   -0.029 bias    0.425\n",
      "iter 8101/10000  loss         0.117645  avg_L1_norm_grad         0.000409  w[0]   -0.029 bias    0.425\n",
      "iter 8120/10000  loss         0.117603  avg_L1_norm_grad         0.000408  w[0]   -0.029 bias    0.425\n",
      "iter 8121/10000  loss         0.117601  avg_L1_norm_grad         0.000408  w[0]   -0.029 bias    0.425\n",
      "iter 8140/10000  loss         0.117559  avg_L1_norm_grad         0.000408  w[0]   -0.029 bias    0.426\n",
      "iter 8141/10000  loss         0.117557  avg_L1_norm_grad         0.000408  w[0]   -0.029 bias    0.426\n",
      "iter 8160/10000  loss         0.117515  avg_L1_norm_grad         0.000407  w[0]   -0.029 bias    0.426\n",
      "iter 8161/10000  loss         0.117513  avg_L1_norm_grad         0.000407  w[0]   -0.029 bias    0.426\n",
      "iter 8180/10000  loss         0.117472  avg_L1_norm_grad         0.000406  w[0]   -0.029 bias    0.427\n",
      "iter 8181/10000  loss         0.117470  avg_L1_norm_grad         0.000406  w[0]   -0.029 bias    0.427\n",
      "iter 8200/10000  loss         0.117429  avg_L1_norm_grad         0.000405  w[0]   -0.029 bias    0.427\n",
      "iter 8201/10000  loss         0.117426  avg_L1_norm_grad         0.000405  w[0]   -0.029 bias    0.427\n",
      "iter 8220/10000  loss         0.117385  avg_L1_norm_grad         0.000404  w[0]   -0.029 bias    0.428\n",
      "iter 8221/10000  loss         0.117383  avg_L1_norm_grad         0.000404  w[0]   -0.029 bias    0.428\n",
      "iter 8240/10000  loss         0.117342  avg_L1_norm_grad         0.000404  w[0]   -0.029 bias    0.428\n",
      "iter 8241/10000  loss         0.117340  avg_L1_norm_grad         0.000404  w[0]   -0.029 bias    0.428\n",
      "iter 8260/10000  loss         0.117300  avg_L1_norm_grad         0.000403  w[0]   -0.029 bias    0.429\n",
      "iter 8261/10000  loss         0.117298  avg_L1_norm_grad         0.000403  w[0]   -0.029 bias    0.429\n",
      "iter 8280/10000  loss         0.117257  avg_L1_norm_grad         0.000402  w[0]   -0.029 bias    0.429\n",
      "iter 8281/10000  loss         0.117255  avg_L1_norm_grad         0.000402  w[0]   -0.029 bias    0.429\n",
      "iter 8300/10000  loss         0.117215  avg_L1_norm_grad         0.000401  w[0]   -0.029 bias    0.430\n",
      "iter 8301/10000  loss         0.117212  avg_L1_norm_grad         0.000401  w[0]   -0.029 bias    0.430\n",
      "iter 8320/10000  loss         0.117172  avg_L1_norm_grad         0.000400  w[0]   -0.029 bias    0.430\n",
      "iter 8321/10000  loss         0.117170  avg_L1_norm_grad         0.000400  w[0]   -0.029 bias    0.430\n",
      "iter 8340/10000  loss         0.117130  avg_L1_norm_grad         0.000400  w[0]   -0.030 bias    0.431\n",
      "iter 8341/10000  loss         0.117128  avg_L1_norm_grad         0.000400  w[0]   -0.030 bias    0.431\n",
      "iter 8360/10000  loss         0.117088  avg_L1_norm_grad         0.000399  w[0]   -0.030 bias    0.431\n",
      "iter 8361/10000  loss         0.117086  avg_L1_norm_grad         0.000399  w[0]   -0.030 bias    0.431\n",
      "iter 8380/10000  loss         0.117046  avg_L1_norm_grad         0.000398  w[0]   -0.030 bias    0.432\n",
      "iter 8381/10000  loss         0.117044  avg_L1_norm_grad         0.000398  w[0]   -0.030 bias    0.432\n",
      "iter 8400/10000  loss         0.117005  avg_L1_norm_grad         0.000397  w[0]   -0.030 bias    0.432\n",
      "iter 8401/10000  loss         0.117003  avg_L1_norm_grad         0.000397  w[0]   -0.030 bias    0.432\n",
      "iter 8420/10000  loss         0.116963  avg_L1_norm_grad         0.000397  w[0]   -0.030 bias    0.433\n",
      "iter 8421/10000  loss         0.116961  avg_L1_norm_grad         0.000397  w[0]   -0.030 bias    0.433\n",
      "iter 8440/10000  loss         0.116922  avg_L1_norm_grad         0.000396  w[0]   -0.030 bias    0.433\n",
      "iter 8441/10000  loss         0.116920  avg_L1_norm_grad         0.000396  w[0]   -0.030 bias    0.433\n",
      "iter 8460/10000  loss         0.116881  avg_L1_norm_grad         0.000395  w[0]   -0.030 bias    0.434\n",
      "iter 8461/10000  loss         0.116879  avg_L1_norm_grad         0.000395  w[0]   -0.030 bias    0.434\n",
      "iter 8480/10000  loss         0.116840  avg_L1_norm_grad         0.000394  w[0]   -0.030 bias    0.434\n",
      "iter 8481/10000  loss         0.116838  avg_L1_norm_grad         0.000394  w[0]   -0.030 bias    0.434\n",
      "iter 8500/10000  loss         0.116799  avg_L1_norm_grad         0.000394  w[0]   -0.030 bias    0.435\n",
      "iter 8501/10000  loss         0.116797  avg_L1_norm_grad         0.000394  w[0]   -0.030 bias    0.435\n",
      "iter 8520/10000  loss         0.116758  avg_L1_norm_grad         0.000393  w[0]   -0.030 bias    0.435\n",
      "iter 8521/10000  loss         0.116756  avg_L1_norm_grad         0.000393  w[0]   -0.030 bias    0.435\n",
      "iter 8540/10000  loss         0.116718  avg_L1_norm_grad         0.000392  w[0]   -0.030 bias    0.436\n",
      "iter 8541/10000  loss         0.116716  avg_L1_norm_grad         0.000392  w[0]   -0.030 bias    0.436\n",
      "iter 8560/10000  loss         0.116677  avg_L1_norm_grad         0.000391  w[0]   -0.030 bias    0.436\n",
      "iter 8561/10000  loss         0.116675  avg_L1_norm_grad         0.000391  w[0]   -0.030 bias    0.436\n",
      "iter 8580/10000  loss         0.116637  avg_L1_norm_grad         0.000391  w[0]   -0.030 bias    0.437\n",
      "iter 8581/10000  loss         0.116635  avg_L1_norm_grad         0.000391  w[0]   -0.030 bias    0.437\n",
      "iter 8600/10000  loss         0.116597  avg_L1_norm_grad         0.000390  w[0]   -0.030 bias    0.437\n",
      "iter 8601/10000  loss         0.116595  avg_L1_norm_grad         0.000390  w[0]   -0.030 bias    0.437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8620/10000  loss         0.116557  avg_L1_norm_grad         0.000389  w[0]   -0.030 bias    0.438\n",
      "iter 8621/10000  loss         0.116555  avg_L1_norm_grad         0.000389  w[0]   -0.030 bias    0.438\n",
      "iter 8640/10000  loss         0.116517  avg_L1_norm_grad         0.000388  w[0]   -0.030 bias    0.438\n",
      "iter 8641/10000  loss         0.116515  avg_L1_norm_grad         0.000388  w[0]   -0.030 bias    0.438\n",
      "iter 8660/10000  loss         0.116478  avg_L1_norm_grad         0.000388  w[0]   -0.030 bias    0.439\n",
      "iter 8661/10000  loss         0.116476  avg_L1_norm_grad         0.000388  w[0]   -0.030 bias    0.439\n",
      "iter 8680/10000  loss         0.116438  avg_L1_norm_grad         0.000387  w[0]   -0.031 bias    0.439\n",
      "iter 8681/10000  loss         0.116436  avg_L1_norm_grad         0.000387  w[0]   -0.031 bias    0.439\n",
      "iter 8700/10000  loss         0.116399  avg_L1_norm_grad         0.000386  w[0]   -0.031 bias    0.440\n",
      "iter 8701/10000  loss         0.116397  avg_L1_norm_grad         0.000386  w[0]   -0.031 bias    0.440\n",
      "iter 8720/10000  loss         0.116360  avg_L1_norm_grad         0.000385  w[0]   -0.031 bias    0.440\n",
      "iter 8721/10000  loss         0.116358  avg_L1_norm_grad         0.000385  w[0]   -0.031 bias    0.440\n",
      "iter 8740/10000  loss         0.116321  avg_L1_norm_grad         0.000385  w[0]   -0.031 bias    0.441\n",
      "iter 8741/10000  loss         0.116319  avg_L1_norm_grad         0.000385  w[0]   -0.031 bias    0.441\n",
      "iter 8760/10000  loss         0.116282  avg_L1_norm_grad         0.000384  w[0]   -0.031 bias    0.441\n",
      "iter 8761/10000  loss         0.116280  avg_L1_norm_grad         0.000384  w[0]   -0.031 bias    0.441\n",
      "iter 8780/10000  loss         0.116243  avg_L1_norm_grad         0.000383  w[0]   -0.031 bias    0.441\n",
      "iter 8781/10000  loss         0.116241  avg_L1_norm_grad         0.000383  w[0]   -0.031 bias    0.442\n",
      "iter 8800/10000  loss         0.116205  avg_L1_norm_grad         0.000383  w[0]   -0.031 bias    0.442\n",
      "iter 8801/10000  loss         0.116203  avg_L1_norm_grad         0.000383  w[0]   -0.031 bias    0.442\n",
      "iter 8820/10000  loss         0.116166  avg_L1_norm_grad         0.000382  w[0]   -0.031 bias    0.442\n",
      "iter 8821/10000  loss         0.116165  avg_L1_norm_grad         0.000382  w[0]   -0.031 bias    0.442\n",
      "iter 8840/10000  loss         0.116128  avg_L1_norm_grad         0.000381  w[0]   -0.031 bias    0.443\n",
      "iter 8841/10000  loss         0.116126  avg_L1_norm_grad         0.000381  w[0]   -0.031 bias    0.443\n",
      "iter 8860/10000  loss         0.116090  avg_L1_norm_grad         0.000381  w[0]   -0.031 bias    0.443\n",
      "iter 8861/10000  loss         0.116088  avg_L1_norm_grad         0.000381  w[0]   -0.031 bias    0.443\n",
      "iter 8880/10000  loss         0.116052  avg_L1_norm_grad         0.000380  w[0]   -0.031 bias    0.444\n",
      "iter 8881/10000  loss         0.116050  avg_L1_norm_grad         0.000380  w[0]   -0.031 bias    0.444\n",
      "iter 8900/10000  loss         0.116014  avg_L1_norm_grad         0.000379  w[0]   -0.031 bias    0.444\n",
      "iter 8901/10000  loss         0.116012  avg_L1_norm_grad         0.000379  w[0]   -0.031 bias    0.444\n",
      "iter 8920/10000  loss         0.115977  avg_L1_norm_grad         0.000378  w[0]   -0.031 bias    0.445\n",
      "iter 8921/10000  loss         0.115975  avg_L1_norm_grad         0.000378  w[0]   -0.031 bias    0.445\n",
      "iter 8940/10000  loss         0.115939  avg_L1_norm_grad         0.000378  w[0]   -0.031 bias    0.445\n",
      "iter 8941/10000  loss         0.115937  avg_L1_norm_grad         0.000378  w[0]   -0.031 bias    0.445\n",
      "iter 8960/10000  loss         0.115902  avg_L1_norm_grad         0.000377  w[0]   -0.031 bias    0.446\n",
      "iter 8961/10000  loss         0.115900  avg_L1_norm_grad         0.000377  w[0]   -0.031 bias    0.446\n",
      "iter 8980/10000  loss         0.115864  avg_L1_norm_grad         0.000376  w[0]   -0.031 bias    0.446\n",
      "iter 8981/10000  loss         0.115863  avg_L1_norm_grad         0.000376  w[0]   -0.031 bias    0.446\n",
      "iter 9000/10000  loss         0.115827  avg_L1_norm_grad         0.000376  w[0]   -0.031 bias    0.447\n",
      "iter 9001/10000  loss         0.115825  avg_L1_norm_grad         0.000376  w[0]   -0.031 bias    0.447\n",
      "iter 9020/10000  loss         0.115790  avg_L1_norm_grad         0.000375  w[0]   -0.031 bias    0.447\n",
      "iter 9021/10000  loss         0.115788  avg_L1_norm_grad         0.000375  w[0]   -0.031 bias    0.447\n",
      "iter 9040/10000  loss         0.115753  avg_L1_norm_grad         0.000374  w[0]   -0.032 bias    0.448\n",
      "iter 9041/10000  loss         0.115752  avg_L1_norm_grad         0.000374  w[0]   -0.032 bias    0.448\n",
      "iter 9060/10000  loss         0.115717  avg_L1_norm_grad         0.000374  w[0]   -0.032 bias    0.448\n",
      "iter 9061/10000  loss         0.115715  avg_L1_norm_grad         0.000374  w[0]   -0.032 bias    0.448\n",
      "iter 9080/10000  loss         0.115680  avg_L1_norm_grad         0.000373  w[0]   -0.032 bias    0.449\n",
      "iter 9081/10000  loss         0.115678  avg_L1_norm_grad         0.000373  w[0]   -0.032 bias    0.449\n",
      "iter 9100/10000  loss         0.115644  avg_L1_norm_grad         0.000372  w[0]   -0.032 bias    0.449\n",
      "iter 9101/10000  loss         0.115642  avg_L1_norm_grad         0.000372  w[0]   -0.032 bias    0.449\n",
      "iter 9120/10000  loss         0.115607  avg_L1_norm_grad         0.000372  w[0]   -0.032 bias    0.450\n",
      "iter 9121/10000  loss         0.115606  avg_L1_norm_grad         0.000372  w[0]   -0.032 bias    0.450\n",
      "iter 9140/10000  loss         0.115571  avg_L1_norm_grad         0.000371  w[0]   -0.032 bias    0.450\n",
      "iter 9141/10000  loss         0.115569  avg_L1_norm_grad         0.000371  w[0]   -0.032 bias    0.450\n",
      "iter 9160/10000  loss         0.115535  avg_L1_norm_grad         0.000370  w[0]   -0.032 bias    0.451\n",
      "iter 9161/10000  loss         0.115533  avg_L1_norm_grad         0.000370  w[0]   -0.032 bias    0.451\n",
      "iter 9180/10000  loss         0.115499  avg_L1_norm_grad         0.000370  w[0]   -0.032 bias    0.451\n",
      "iter 9181/10000  loss         0.115497  avg_L1_norm_grad         0.000370  w[0]   -0.032 bias    0.451\n",
      "iter 9200/10000  loss         0.115463  avg_L1_norm_grad         0.000369  w[0]   -0.032 bias    0.451\n",
      "iter 9201/10000  loss         0.115462  avg_L1_norm_grad         0.000369  w[0]   -0.032 bias    0.451\n",
      "iter 9220/10000  loss         0.115428  avg_L1_norm_grad         0.000368  w[0]   -0.032 bias    0.452\n",
      "iter 9221/10000  loss         0.115426  avg_L1_norm_grad         0.000368  w[0]   -0.032 bias    0.452\n",
      "iter 9240/10000  loss         0.115392  avg_L1_norm_grad         0.000368  w[0]   -0.032 bias    0.452\n",
      "iter 9241/10000  loss         0.115390  avg_L1_norm_grad         0.000368  w[0]   -0.032 bias    0.452\n",
      "iter 9260/10000  loss         0.115357  avg_L1_norm_grad         0.000367  w[0]   -0.032 bias    0.453\n",
      "iter 9261/10000  loss         0.115355  avg_L1_norm_grad         0.000367  w[0]   -0.032 bias    0.453\n",
      "iter 9280/10000  loss         0.115322  avg_L1_norm_grad         0.000367  w[0]   -0.032 bias    0.453\n",
      "iter 9281/10000  loss         0.115320  avg_L1_norm_grad         0.000366  w[0]   -0.032 bias    0.453\n",
      "iter 9300/10000  loss         0.115286  avg_L1_norm_grad         0.000366  w[0]   -0.032 bias    0.454\n",
      "iter 9301/10000  loss         0.115285  avg_L1_norm_grad         0.000366  w[0]   -0.032 bias    0.454\n",
      "iter 9320/10000  loss         0.115251  avg_L1_norm_grad         0.000365  w[0]   -0.032 bias    0.454\n",
      "iter 9321/10000  loss         0.115250  avg_L1_norm_grad         0.000365  w[0]   -0.032 bias    0.454\n",
      "iter 9340/10000  loss         0.115216  avg_L1_norm_grad         0.000365  w[0]   -0.032 bias    0.455\n",
      "iter 9341/10000  loss         0.115215  avg_L1_norm_grad         0.000365  w[0]   -0.032 bias    0.455\n",
      "iter 9360/10000  loss         0.115182  avg_L1_norm_grad         0.000364  w[0]   -0.032 bias    0.455\n",
      "iter 9361/10000  loss         0.115180  avg_L1_norm_grad         0.000364  w[0]   -0.032 bias    0.455\n",
      "iter 9380/10000  loss         0.115147  avg_L1_norm_grad         0.000363  w[0]   -0.032 bias    0.456\n",
      "iter 9381/10000  loss         0.115145  avg_L1_norm_grad         0.000363  w[0]   -0.032 bias    0.456\n",
      "iter 9400/10000  loss         0.115113  avg_L1_norm_grad         0.000363  w[0]   -0.033 bias    0.456\n",
      "iter 9401/10000  loss         0.115111  avg_L1_norm_grad         0.000363  w[0]   -0.033 bias    0.456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9420/10000  loss         0.115078  avg_L1_norm_grad         0.000362  w[0]   -0.033 bias    0.457\n",
      "iter 9421/10000  loss         0.115076  avg_L1_norm_grad         0.000362  w[0]   -0.033 bias    0.457\n",
      "iter 9440/10000  loss         0.115044  avg_L1_norm_grad         0.000361  w[0]   -0.033 bias    0.457\n",
      "iter 9441/10000  loss         0.115042  avg_L1_norm_grad         0.000361  w[0]   -0.033 bias    0.457\n",
      "iter 9460/10000  loss         0.115010  avg_L1_norm_grad         0.000361  w[0]   -0.033 bias    0.458\n",
      "iter 9461/10000  loss         0.115008  avg_L1_norm_grad         0.000361  w[0]   -0.033 bias    0.458\n",
      "iter 9480/10000  loss         0.114976  avg_L1_norm_grad         0.000360  w[0]   -0.033 bias    0.458\n",
      "iter 9481/10000  loss         0.114974  avg_L1_norm_grad         0.000360  w[0]   -0.033 bias    0.458\n",
      "iter 9500/10000  loss         0.114942  avg_L1_norm_grad         0.000360  w[0]   -0.033 bias    0.458\n",
      "iter 9501/10000  loss         0.114940  avg_L1_norm_grad         0.000360  w[0]   -0.033 bias    0.458\n",
      "iter 9520/10000  loss         0.114908  avg_L1_norm_grad         0.000359  w[0]   -0.033 bias    0.459\n",
      "iter 9521/10000  loss         0.114906  avg_L1_norm_grad         0.000359  w[0]   -0.033 bias    0.459\n",
      "iter 9540/10000  loss         0.114874  avg_L1_norm_grad         0.000358  w[0]   -0.033 bias    0.459\n",
      "iter 9541/10000  loss         0.114873  avg_L1_norm_grad         0.000358  w[0]   -0.033 bias    0.459\n",
      "iter 9560/10000  loss         0.114841  avg_L1_norm_grad         0.000358  w[0]   -0.033 bias    0.460\n",
      "iter 9561/10000  loss         0.114839  avg_L1_norm_grad         0.000358  w[0]   -0.033 bias    0.460\n",
      "iter 9580/10000  loss         0.114807  avg_L1_norm_grad         0.000357  w[0]   -0.033 bias    0.460\n",
      "iter 9581/10000  loss         0.114806  avg_L1_norm_grad         0.000357  w[0]   -0.033 bias    0.460\n",
      "iter 9600/10000  loss         0.114774  avg_L1_norm_grad         0.000357  w[0]   -0.033 bias    0.461\n",
      "iter 9601/10000  loss         0.114772  avg_L1_norm_grad         0.000357  w[0]   -0.033 bias    0.461\n",
      "iter 9620/10000  loss         0.114741  avg_L1_norm_grad         0.000356  w[0]   -0.033 bias    0.461\n",
      "iter 9621/10000  loss         0.114739  avg_L1_norm_grad         0.000356  w[0]   -0.033 bias    0.461\n",
      "iter 9640/10000  loss         0.114708  avg_L1_norm_grad         0.000355  w[0]   -0.033 bias    0.462\n",
      "iter 9641/10000  loss         0.114706  avg_L1_norm_grad         0.000355  w[0]   -0.033 bias    0.462\n",
      "iter 9660/10000  loss         0.114675  avg_L1_norm_grad         0.000355  w[0]   -0.033 bias    0.462\n",
      "iter 9661/10000  loss         0.114673  avg_L1_norm_grad         0.000355  w[0]   -0.033 bias    0.462\n",
      "iter 9680/10000  loss         0.114642  avg_L1_norm_grad         0.000354  w[0]   -0.033 bias    0.463\n",
      "iter 9681/10000  loss         0.114640  avg_L1_norm_grad         0.000354  w[0]   -0.033 bias    0.463\n",
      "iter 9700/10000  loss         0.114609  avg_L1_norm_grad         0.000354  w[0]   -0.033 bias    0.463\n",
      "iter 9701/10000  loss         0.114608  avg_L1_norm_grad         0.000354  w[0]   -0.033 bias    0.463\n",
      "iter 9720/10000  loss         0.114577  avg_L1_norm_grad         0.000353  w[0]   -0.033 bias    0.463\n",
      "iter 9721/10000  loss         0.114575  avg_L1_norm_grad         0.000353  w[0]   -0.033 bias    0.463\n",
      "iter 9740/10000  loss         0.114544  avg_L1_norm_grad         0.000352  w[0]   -0.033 bias    0.464\n",
      "iter 9741/10000  loss         0.114542  avg_L1_norm_grad         0.000352  w[0]   -0.033 bias    0.464\n",
      "iter 9760/10000  loss         0.114512  avg_L1_norm_grad         0.000352  w[0]   -0.033 bias    0.464\n",
      "iter 9761/10000  loss         0.114510  avg_L1_norm_grad         0.000352  w[0]   -0.033 bias    0.464\n",
      "iter 9780/10000  loss         0.114479  avg_L1_norm_grad         0.000351  w[0]   -0.034 bias    0.465\n",
      "iter 9781/10000  loss         0.114478  avg_L1_norm_grad         0.000351  w[0]   -0.034 bias    0.465\n",
      "iter 9800/10000  loss         0.114447  avg_L1_norm_grad         0.000351  w[0]   -0.034 bias    0.465\n",
      "iter 9801/10000  loss         0.114446  avg_L1_norm_grad         0.000351  w[0]   -0.034 bias    0.465\n",
      "iter 9820/10000  loss         0.114415  avg_L1_norm_grad         0.000350  w[0]   -0.034 bias    0.466\n",
      "iter 9821/10000  loss         0.114413  avg_L1_norm_grad         0.000350  w[0]   -0.034 bias    0.466\n",
      "iter 9840/10000  loss         0.114383  avg_L1_norm_grad         0.000349  w[0]   -0.034 bias    0.466\n",
      "iter 9841/10000  loss         0.114382  avg_L1_norm_grad         0.000349  w[0]   -0.034 bias    0.466\n",
      "iter 9860/10000  loss         0.114351  avg_L1_norm_grad         0.000349  w[0]   -0.034 bias    0.467\n",
      "iter 9861/10000  loss         0.114350  avg_L1_norm_grad         0.000349  w[0]   -0.034 bias    0.467\n",
      "iter 9880/10000  loss         0.114320  avg_L1_norm_grad         0.000348  w[0]   -0.034 bias    0.467\n",
      "iter 9881/10000  loss         0.114318  avg_L1_norm_grad         0.000348  w[0]   -0.034 bias    0.467\n",
      "iter 9900/10000  loss         0.114288  avg_L1_norm_grad         0.000348  w[0]   -0.034 bias    0.467\n",
      "iter 9901/10000  loss         0.114286  avg_L1_norm_grad         0.000348  w[0]   -0.034 bias    0.467\n",
      "iter 9920/10000  loss         0.114256  avg_L1_norm_grad         0.000347  w[0]   -0.034 bias    0.468\n",
      "iter 9921/10000  loss         0.114255  avg_L1_norm_grad         0.000347  w[0]   -0.034 bias    0.468\n",
      "iter 9940/10000  loss         0.114225  avg_L1_norm_grad         0.000347  w[0]   -0.034 bias    0.468\n",
      "iter 9941/10000  loss         0.114223  avg_L1_norm_grad         0.000346  w[0]   -0.034 bias    0.468\n",
      "iter 9960/10000  loss         0.114194  avg_L1_norm_grad         0.000346  w[0]   -0.034 bias    0.469\n",
      "iter 9961/10000  loss         0.114192  avg_L1_norm_grad         0.000346  w[0]   -0.034 bias    0.469\n",
      "iter 9980/10000  loss         0.114162  avg_L1_norm_grad         0.000345  w[0]   -0.034 bias    0.469\n",
      "iter 9981/10000  loss         0.114161  avg_L1_norm_grad         0.000345  w[0]   -0.034 bias    0.469\n",
      "iter 10000/10000  loss         0.114131  avg_L1_norm_grad         0.000345  w[0]   -0.034 bias    0.470\n",
      "Done. Did NOT converge.\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 10000 iters of gradient descent with step_size 0.25\n",
      "iter    0/10000  loss         1.000000  avg_L1_norm_grad         0.024676  w[0]    0.000 bias    0.000\n",
      "iter    1/10000  loss         0.741783  avg_L1_norm_grad         0.036433  w[0]   -0.001 bias    0.001\n",
      "iter    2/10000  loss         1.132563  avg_L1_norm_grad         0.116754  w[0]    0.003 bias    0.047\n",
      "iter    3/10000  loss         3.570826  avg_L1_norm_grad         0.150274  w[0]   -0.009 bias   -0.075\n",
      "iter    4/10000  loss         2.157162  avg_L1_norm_grad         0.154230  w[0]    0.008 bias    0.105\n",
      "iter    5/10000  loss         3.086198  avg_L1_norm_grad         0.148058  w[0]   -0.009 bias   -0.059\n",
      "iter    6/10000  loss         1.398393  avg_L1_norm_grad         0.124772  w[0]    0.008 bias    0.118\n",
      "iter    7/10000  loss         1.542620  avg_L1_norm_grad         0.118735  w[0]   -0.006 bias   -0.014\n",
      "iter    8/10000  loss         0.962242  avg_L1_norm_grad         0.092764  w[0]    0.008 bias    0.128\n",
      "iter    9/10000  loss         0.606342  avg_L1_norm_grad         0.058484  w[0]   -0.003 bias    0.031\n",
      "iter   10/10000  loss         0.298248  avg_L1_norm_grad         0.023488  w[0]    0.004 bias    0.101\n",
      "iter   11/10000  loss         0.234967  avg_L1_norm_grad         0.007335  w[0]    0.001 bias    0.078\n",
      "iter   12/10000  loss         0.224697  avg_L1_norm_grad         0.002861  w[0]    0.002 bias    0.089\n",
      "iter   13/10000  loss         0.220916  avg_L1_norm_grad         0.002686  w[0]    0.002 bias    0.089\n",
      "iter   14/10000  loss         0.217728  avg_L1_norm_grad         0.002528  w[0]    0.002 bias    0.092\n",
      "iter   15/10000  loss         0.214802  avg_L1_norm_grad         0.002451  w[0]    0.002 bias    0.095\n",
      "iter   16/10000  loss         0.212092  avg_L1_norm_grad         0.002373  w[0]    0.002 bias    0.097\n",
      "iter   17/10000  loss         0.209570  avg_L1_norm_grad         0.002303  w[0]    0.002 bias    0.099\n",
      "iter   18/10000  loss         0.207214  avg_L1_norm_grad         0.002238  w[0]    0.002 bias    0.102\n",
      "iter   19/10000  loss         0.205004  avg_L1_norm_grad         0.002178  w[0]    0.002 bias    0.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter   20/10000  loss         0.202924  avg_L1_norm_grad         0.002122  w[0]    0.002 bias    0.106\n",
      "iter   21/10000  loss         0.200961  avg_L1_norm_grad         0.002070  w[0]    0.002 bias    0.109\n",
      "iter   40/10000  loss         0.176080  avg_L1_norm_grad         0.001500  w[0]   -0.001 bias    0.147\n",
      "iter   41/10000  loss         0.175161  avg_L1_norm_grad         0.001481  w[0]   -0.001 bias    0.149\n",
      "iter   60/10000  loss         0.161448  avg_L1_norm_grad         0.001209  w[0]   -0.003 bias    0.181\n",
      "iter   61/10000  loss         0.160876  avg_L1_norm_grad         0.001198  w[0]   -0.003 bias    0.183\n",
      "iter   80/10000  loss         0.151839  avg_L1_norm_grad         0.001028  w[0]   -0.006 bias    0.210\n",
      "iter   81/10000  loss         0.151442  avg_L1_norm_grad         0.001021  w[0]   -0.006 bias    0.212\n",
      "iter  100/10000  loss         0.144941  avg_L1_norm_grad         0.000901  w[0]   -0.008 bias    0.236\n",
      "iter  101/10000  loss         0.144645  avg_L1_norm_grad         0.000896  w[0]   -0.009 bias    0.238\n",
      "iter  120/10000  loss         0.139691  avg_L1_norm_grad         0.000808  w[0]   -0.011 bias    0.260\n",
      "iter  121/10000  loss         0.139460  avg_L1_norm_grad         0.000804  w[0]   -0.011 bias    0.261\n",
      "iter  140/10000  loss         0.135529  avg_L1_norm_grad         0.000733  w[0]   -0.013 bias    0.281\n",
      "iter  141/10000  loss         0.135343  avg_L1_norm_grad         0.000730  w[0]   -0.013 bias    0.282\n",
      "iter  160/10000  loss         0.132130  avg_L1_norm_grad         0.000671  w[0]   -0.016 bias    0.301\n",
      "iter  161/10000  loss         0.131976  avg_L1_norm_grad         0.000669  w[0]   -0.016 bias    0.302\n",
      "iter  180/10000  loss         0.129292  avg_L1_norm_grad         0.000620  w[0]   -0.018 bias    0.319\n",
      "iter  181/10000  loss         0.129162  avg_L1_norm_grad         0.000617  w[0]   -0.018 bias    0.320\n",
      "iter  200/10000  loss         0.126880  avg_L1_norm_grad         0.000576  w[0]   -0.020 bias    0.337\n",
      "iter  201/10000  loss         0.126768  avg_L1_norm_grad         0.000574  w[0]   -0.020 bias    0.338\n",
      "iter  220/10000  loss         0.124800  avg_L1_norm_grad         0.000537  w[0]   -0.022 bias    0.353\n",
      "iter  221/10000  loss         0.124704  avg_L1_norm_grad         0.000536  w[0]   -0.022 bias    0.354\n",
      "iter  240/10000  loss         0.122986  avg_L1_norm_grad         0.000504  w[0]   -0.024 bias    0.369\n",
      "iter  241/10000  loss         0.122902  avg_L1_norm_grad         0.000503  w[0]   -0.024 bias    0.369\n",
      "iter  260/10000  loss         0.121389  avg_L1_norm_grad         0.000475  w[0]   -0.026 bias    0.383\n",
      "iter  261/10000  loss         0.121314  avg_L1_norm_grad         0.000473  w[0]   -0.026 bias    0.384\n",
      "iter  280/10000  loss         0.119970  avg_L1_norm_grad         0.000449  w[0]   -0.027 bias    0.397\n",
      "iter  281/10000  loss         0.119904  avg_L1_norm_grad         0.000448  w[0]   -0.027 bias    0.398\n",
      "iter  300/10000  loss         0.118701  avg_L1_norm_grad         0.000426  w[0]   -0.029 bias    0.411\n",
      "iter  301/10000  loss         0.118641  avg_L1_norm_grad         0.000425  w[0]   -0.029 bias    0.411\n",
      "iter  320/10000  loss         0.117559  avg_L1_norm_grad         0.000405  w[0]   -0.031 bias    0.424\n",
      "iter  321/10000  loss         0.117505  avg_L1_norm_grad         0.000404  w[0]   -0.031 bias    0.424\n",
      "iter  340/10000  loss         0.116525  avg_L1_norm_grad         0.000386  w[0]   -0.032 bias    0.436\n",
      "iter  341/10000  loss         0.116476  avg_L1_norm_grad         0.000386  w[0]   -0.032 bias    0.437\n",
      "iter  360/10000  loss         0.115585  avg_L1_norm_grad         0.000370  w[0]   -0.033 bias    0.448\n",
      "iter  361/10000  loss         0.115540  avg_L1_norm_grad         0.000369  w[0]   -0.034 bias    0.449\n",
      "iter  380/10000  loss         0.114725  avg_L1_norm_grad         0.000354  w[0]   -0.035 bias    0.459\n",
      "iter  381/10000  loss         0.114684  avg_L1_norm_grad         0.000354  w[0]   -0.035 bias    0.460\n",
      "iter  400/10000  loss         0.113937  avg_L1_norm_grad         0.000340  w[0]   -0.036 bias    0.471\n",
      "iter  401/10000  loss         0.113900  avg_L1_norm_grad         0.000339  w[0]   -0.036 bias    0.471\n",
      "iter  420/10000  loss         0.113212  avg_L1_norm_grad         0.000327  w[0]   -0.037 bias    0.481\n",
      "iter  421/10000  loss         0.113177  avg_L1_norm_grad         0.000326  w[0]   -0.037 bias    0.482\n",
      "iter  440/10000  loss         0.112543  avg_L1_norm_grad         0.000315  w[0]   -0.038 bias    0.492\n",
      "iter  441/10000  loss         0.112510  avg_L1_norm_grad         0.000314  w[0]   -0.038 bias    0.492\n",
      "iter  460/10000  loss         0.111922  avg_L1_norm_grad         0.000304  w[0]   -0.039 bias    0.502\n",
      "iter  461/10000  loss         0.111893  avg_L1_norm_grad         0.000303  w[0]   -0.040 bias    0.502\n",
      "iter  480/10000  loss         0.111347  avg_L1_norm_grad         0.000293  w[0]   -0.040 bias    0.512\n",
      "iter  481/10000  loss         0.111319  avg_L1_norm_grad         0.000292  w[0]   -0.041 bias    0.512\n",
      "iter  500/10000  loss         0.110811  avg_L1_norm_grad         0.000283  w[0]   -0.041 bias    0.521\n",
      "iter  501/10000  loss         0.110785  avg_L1_norm_grad         0.000283  w[0]   -0.041 bias    0.522\n",
      "iter  520/10000  loss         0.110311  avg_L1_norm_grad         0.000274  w[0]   -0.042 bias    0.531\n",
      "iter  521/10000  loss         0.110287  avg_L1_norm_grad         0.000273  w[0]   -0.042 bias    0.531\n",
      "iter  540/10000  loss         0.109844  avg_L1_norm_grad         0.000265  w[0]   -0.043 bias    0.540\n",
      "iter  541/10000  loss         0.109822  avg_L1_norm_grad         0.000264  w[0]   -0.043 bias    0.540\n",
      "iter  560/10000  loss         0.109407  avg_L1_norm_grad         0.000257  w[0]   -0.044 bias    0.549\n",
      "iter  561/10000  loss         0.109386  avg_L1_norm_grad         0.000256  w[0]   -0.044 bias    0.549\n",
      "iter  580/10000  loss         0.108996  avg_L1_norm_grad         0.000249  w[0]   -0.045 bias    0.557\n",
      "iter  581/10000  loss         0.108976  avg_L1_norm_grad         0.000248  w[0]   -0.045 bias    0.558\n",
      "iter  600/10000  loss         0.108611  avg_L1_norm_grad         0.000241  w[0]   -0.045 bias    0.566\n",
      "iter  601/10000  loss         0.108592  avg_L1_norm_grad         0.000241  w[0]   -0.045 bias    0.566\n",
      "iter  620/10000  loss         0.108248  avg_L1_norm_grad         0.000234  w[0]   -0.046 bias    0.574\n",
      "iter  621/10000  loss         0.108230  avg_L1_norm_grad         0.000234  w[0]   -0.046 bias    0.574\n",
      "iter  640/10000  loss         0.107906  avg_L1_norm_grad         0.000227  w[0]   -0.047 bias    0.582\n",
      "iter  641/10000  loss         0.107890  avg_L1_norm_grad         0.000227  w[0]   -0.047 bias    0.582\n",
      "iter  660/10000  loss         0.107584  avg_L1_norm_grad         0.000221  w[0]   -0.047 bias    0.590\n",
      "iter  661/10000  loss         0.107568  avg_L1_norm_grad         0.000221  w[0]   -0.047 bias    0.590\n",
      "iter  680/10000  loss         0.107279  avg_L1_norm_grad         0.000215  w[0]   -0.048 bias    0.598\n",
      "iter  681/10000  loss         0.107264  avg_L1_norm_grad         0.000215  w[0]   -0.048 bias    0.598\n",
      "iter  700/10000  loss         0.106991  avg_L1_norm_grad         0.000209  w[0]   -0.048 bias    0.605\n",
      "iter  701/10000  loss         0.106977  avg_L1_norm_grad         0.000209  w[0]   -0.048 bias    0.605\n",
      "iter  720/10000  loss         0.106718  avg_L1_norm_grad         0.000204  w[0]   -0.049 bias    0.613\n",
      "iter  721/10000  loss         0.106705  avg_L1_norm_grad         0.000203  w[0]   -0.049 bias    0.613\n",
      "iter  740/10000  loss         0.106459  avg_L1_norm_grad         0.000198  w[0]   -0.049 bias    0.620\n",
      "iter  741/10000  loss         0.106447  avg_L1_norm_grad         0.000198  w[0]   -0.049 bias    0.620\n",
      "iter  760/10000  loss         0.106214  avg_L1_norm_grad         0.000193  w[0]   -0.050 bias    0.627\n",
      "iter  761/10000  loss         0.106202  avg_L1_norm_grad         0.000193  w[0]   -0.050 bias    0.627\n",
      "iter  780/10000  loss         0.105981  avg_L1_norm_grad         0.000188  w[0]   -0.050 bias    0.634\n",
      "iter  781/10000  loss         0.105970  avg_L1_norm_grad         0.000188  w[0]   -0.050 bias    0.634\n",
      "iter  800/10000  loss         0.105760  avg_L1_norm_grad         0.000184  w[0]   -0.051 bias    0.641\n",
      "iter  801/10000  loss         0.105749  avg_L1_norm_grad         0.000183  w[0]   -0.051 bias    0.641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  820/10000  loss         0.105549  avg_L1_norm_grad         0.000179  w[0]   -0.051 bias    0.647\n",
      "iter  821/10000  loss         0.105539  avg_L1_norm_grad         0.000179  w[0]   -0.051 bias    0.648\n",
      "iter  840/10000  loss         0.105349  avg_L1_norm_grad         0.000175  w[0]   -0.051 bias    0.654\n",
      "iter  841/10000  loss         0.105339  avg_L1_norm_grad         0.000175  w[0]   -0.051 bias    0.654\n",
      "iter  860/10000  loss         0.105158  avg_L1_norm_grad         0.000171  w[0]   -0.052 bias    0.660\n",
      "iter  861/10000  loss         0.105149  avg_L1_norm_grad         0.000170  w[0]   -0.052 bias    0.661\n",
      "iter  880/10000  loss         0.104976  avg_L1_norm_grad         0.000167  w[0]   -0.052 bias    0.667\n",
      "iter  881/10000  loss         0.104967  avg_L1_norm_grad         0.000166  w[0]   -0.052 bias    0.667\n",
      "iter  900/10000  loss         0.104803  avg_L1_norm_grad         0.000163  w[0]   -0.052 bias    0.673\n",
      "iter  901/10000  loss         0.104794  avg_L1_norm_grad         0.000162  w[0]   -0.052 bias    0.673\n",
      "iter  920/10000  loss         0.104637  avg_L1_norm_grad         0.000159  w[0]   -0.052 bias    0.679\n",
      "iter  921/10000  loss         0.104629  avg_L1_norm_grad         0.000159  w[0]   -0.052 bias    0.679\n",
      "iter  940/10000  loss         0.104479  avg_L1_norm_grad         0.000155  w[0]   -0.053 bias    0.685\n",
      "iter  941/10000  loss         0.104471  avg_L1_norm_grad         0.000155  w[0]   -0.053 bias    0.686\n",
      "iter  960/10000  loss         0.104328  avg_L1_norm_grad         0.000152  w[0]   -0.053 bias    0.691\n",
      "iter  961/10000  loss         0.104321  avg_L1_norm_grad         0.000152  w[0]   -0.053 bias    0.691\n",
      "iter  980/10000  loss         0.104184  avg_L1_norm_grad         0.000148  w[0]   -0.053 bias    0.697\n",
      "iter  981/10000  loss         0.104177  avg_L1_norm_grad         0.000148  w[0]   -0.053 bias    0.697\n",
      "iter 1000/10000  loss         0.104046  avg_L1_norm_grad         0.000145  w[0]   -0.053 bias    0.703\n",
      "iter 1001/10000  loss         0.104039  avg_L1_norm_grad         0.000145  w[0]   -0.053 bias    0.703\n",
      "iter 1020/10000  loss         0.103914  avg_L1_norm_grad         0.000142  w[0]   -0.053 bias    0.708\n",
      "iter 1021/10000  loss         0.103907  avg_L1_norm_grad         0.000142  w[0]   -0.053 bias    0.709\n",
      "iter 1040/10000  loss         0.103787  avg_L1_norm_grad         0.000139  w[0]   -0.053 bias    0.714\n",
      "iter 1041/10000  loss         0.103781  avg_L1_norm_grad         0.000139  w[0]   -0.053 bias    0.714\n",
      "iter 1060/10000  loss         0.103666  avg_L1_norm_grad         0.000136  w[0]   -0.053 bias    0.719\n",
      "iter 1061/10000  loss         0.103660  avg_L1_norm_grad         0.000136  w[0]   -0.053 bias    0.720\n",
      "iter 1080/10000  loss         0.103550  avg_L1_norm_grad         0.000133  w[0]   -0.054 bias    0.725\n",
      "iter 1081/10000  loss         0.103544  avg_L1_norm_grad         0.000133  w[0]   -0.054 bias    0.725\n",
      "iter 1100/10000  loss         0.103439  avg_L1_norm_grad         0.000130  w[0]   -0.054 bias    0.730\n",
      "iter 1101/10000  loss         0.103433  avg_L1_norm_grad         0.000130  w[0]   -0.054 bias    0.730\n",
      "iter 1120/10000  loss         0.103332  avg_L1_norm_grad         0.000127  w[0]   -0.054 bias    0.735\n",
      "iter 1121/10000  loss         0.103327  avg_L1_norm_grad         0.000127  w[0]   -0.054 bias    0.736\n",
      "iter 1140/10000  loss         0.103230  avg_L1_norm_grad         0.000125  w[0]   -0.054 bias    0.740\n",
      "iter 1141/10000  loss         0.103225  avg_L1_norm_grad         0.000125  w[0]   -0.054 bias    0.741\n",
      "iter 1160/10000  loss         0.103131  avg_L1_norm_grad         0.000122  w[0]   -0.054 bias    0.746\n",
      "iter 1161/10000  loss         0.103127  avg_L1_norm_grad         0.000122  w[0]   -0.054 bias    0.746\n",
      "iter 1180/10000  loss         0.103037  avg_L1_norm_grad         0.000120  w[0]   -0.054 bias    0.751\n",
      "iter 1181/10000  loss         0.103032  avg_L1_norm_grad         0.000120  w[0]   -0.054 bias    0.751\n",
      "iter 1200/10000  loss         0.102947  avg_L1_norm_grad         0.000117  w[0]   -0.054 bias    0.755\n",
      "iter 1201/10000  loss         0.102942  avg_L1_norm_grad         0.000117  w[0]   -0.054 bias    0.756\n",
      "iter 1220/10000  loss         0.102860  avg_L1_norm_grad         0.000115  w[0]   -0.054 bias    0.760\n",
      "iter 1221/10000  loss         0.102855  avg_L1_norm_grad         0.000115  w[0]   -0.054 bias    0.761\n",
      "iter 1240/10000  loss         0.102776  avg_L1_norm_grad         0.000113  w[0]   -0.054 bias    0.765\n",
      "iter 1241/10000  loss         0.102772  avg_L1_norm_grad         0.000113  w[0]   -0.054 bias    0.765\n",
      "iter 1260/10000  loss         0.102696  avg_L1_norm_grad         0.000110  w[0]   -0.054 bias    0.770\n",
      "iter 1261/10000  loss         0.102692  avg_L1_norm_grad         0.000110  w[0]   -0.054 bias    0.770\n",
      "iter 1280/10000  loss         0.102618  avg_L1_norm_grad         0.000108  w[0]   -0.054 bias    0.774\n",
      "iter 1281/10000  loss         0.102615  avg_L1_norm_grad         0.000108  w[0]   -0.054 bias    0.775\n",
      "iter 1300/10000  loss         0.102544  avg_L1_norm_grad         0.000106  w[0]   -0.054 bias    0.779\n",
      "iter 1301/10000  loss         0.102540  avg_L1_norm_grad         0.000106  w[0]   -0.054 bias    0.779\n",
      "iter 1320/10000  loss         0.102473  avg_L1_norm_grad         0.000104  w[0]   -0.054 bias    0.784\n",
      "iter 1321/10000  loss         0.102469  avg_L1_norm_grad         0.000104  w[0]   -0.054 bias    0.784\n",
      "iter 1340/10000  loss         0.102404  avg_L1_norm_grad         0.000102  w[0]   -0.054 bias    0.788\n",
      "iter 1341/10000  loss         0.102400  avg_L1_norm_grad         0.000102  w[0]   -0.054 bias    0.788\n",
      "iter 1360/10000  loss         0.102337  avg_L1_norm_grad         0.000100  w[0]   -0.054 bias    0.792\n",
      "iter 1361/10000  loss         0.102334  avg_L1_norm_grad         0.000100  w[0]   -0.054 bias    0.793\n",
      "iter 1380/10000  loss         0.102274  avg_L1_norm_grad         0.000098  w[0]   -0.054 bias    0.797\n",
      "iter 1381/10000  loss         0.102271  avg_L1_norm_grad         0.000098  w[0]   -0.054 bias    0.797\n",
      "iter 1400/10000  loss         0.102212  avg_L1_norm_grad         0.000096  w[0]   -0.054 bias    0.801\n",
      "iter 1401/10000  loss         0.102209  avg_L1_norm_grad         0.000096  w[0]   -0.054 bias    0.801\n",
      "iter 1420/10000  loss         0.102153  avg_L1_norm_grad         0.000095  w[0]   -0.054 bias    0.805\n",
      "iter 1421/10000  loss         0.102150  avg_L1_norm_grad         0.000094  w[0]   -0.054 bias    0.805\n",
      "iter 1440/10000  loss         0.102096  avg_L1_norm_grad         0.000093  w[0]   -0.054 bias    0.809\n",
      "iter 1441/10000  loss         0.102093  avg_L1_norm_grad         0.000093  w[0]   -0.054 bias    0.810\n",
      "iter 1460/10000  loss         0.102041  avg_L1_norm_grad         0.000091  w[0]   -0.054 bias    0.813\n",
      "iter 1461/10000  loss         0.102038  avg_L1_norm_grad         0.000091  w[0]   -0.054 bias    0.814\n",
      "iter 1480/10000  loss         0.101988  avg_L1_norm_grad         0.000089  w[0]   -0.054 bias    0.818\n",
      "iter 1481/10000  loss         0.101986  avg_L1_norm_grad         0.000089  w[0]   -0.054 bias    0.818\n",
      "iter 1500/10000  loss         0.101937  avg_L1_norm_grad         0.000088  w[0]   -0.054 bias    0.822\n",
      "iter 1501/10000  loss         0.101935  avg_L1_norm_grad         0.000088  w[0]   -0.054 bias    0.822\n",
      "iter 1520/10000  loss         0.101888  avg_L1_norm_grad         0.000086  w[0]   -0.054 bias    0.825\n",
      "iter 1521/10000  loss         0.101885  avg_L1_norm_grad         0.000086  w[0]   -0.054 bias    0.826\n",
      "iter 1540/10000  loss         0.101840  avg_L1_norm_grad         0.000085  w[0]   -0.054 bias    0.829\n",
      "iter 1541/10000  loss         0.101838  avg_L1_norm_grad         0.000085  w[0]   -0.054 bias    0.830\n",
      "iter 1560/10000  loss         0.101795  avg_L1_norm_grad         0.000083  w[0]   -0.054 bias    0.833\n",
      "iter 1561/10000  loss         0.101792  avg_L1_norm_grad         0.000083  w[0]   -0.054 bias    0.833\n",
      "iter 1580/10000  loss         0.101750  avg_L1_norm_grad         0.000082  w[0]   -0.054 bias    0.837\n",
      "iter 1581/10000  loss         0.101748  avg_L1_norm_grad         0.000081  w[0]   -0.054 bias    0.837\n",
      "iter 1600/10000  loss         0.101708  avg_L1_norm_grad         0.000080  w[0]   -0.054 bias    0.841\n",
      "iter 1601/10000  loss         0.101706  avg_L1_norm_grad         0.000080  w[0]   -0.054 bias    0.841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1620/10000  loss         0.101666  avg_L1_norm_grad         0.000079  w[0]   -0.054 bias    0.844\n",
      "iter 1621/10000  loss         0.101664  avg_L1_norm_grad         0.000079  w[0]   -0.054 bias    0.845\n",
      "iter 1640/10000  loss         0.101627  avg_L1_norm_grad         0.000077  w[0]   -0.054 bias    0.848\n",
      "iter 1641/10000  loss         0.101625  avg_L1_norm_grad         0.000077  w[0]   -0.054 bias    0.848\n",
      "iter 1660/10000  loss         0.101588  avg_L1_norm_grad         0.000076  w[0]   -0.054 bias    0.852\n",
      "iter 1661/10000  loss         0.101586  avg_L1_norm_grad         0.000076  w[0]   -0.054 bias    0.852\n",
      "iter 1680/10000  loss         0.101551  avg_L1_norm_grad         0.000075  w[0]   -0.054 bias    0.855\n",
      "iter 1681/10000  loss         0.101549  avg_L1_norm_grad         0.000075  w[0]   -0.054 bias    0.855\n",
      "iter 1700/10000  loss         0.101515  avg_L1_norm_grad         0.000073  w[0]   -0.053 bias    0.859\n",
      "iter 1701/10000  loss         0.101514  avg_L1_norm_grad         0.000073  w[0]   -0.053 bias    0.859\n",
      "iter 1720/10000  loss         0.101481  avg_L1_norm_grad         0.000072  w[0]   -0.053 bias    0.862\n",
      "iter 1721/10000  loss         0.101479  avg_L1_norm_grad         0.000072  w[0]   -0.053 bias    0.862\n",
      "iter 1740/10000  loss         0.101447  avg_L1_norm_grad         0.000071  w[0]   -0.053 bias    0.866\n",
      "iter 1741/10000  loss         0.101446  avg_L1_norm_grad         0.000071  w[0]   -0.053 bias    0.866\n",
      "iter 1760/10000  loss         0.101415  avg_L1_norm_grad         0.000070  w[0]   -0.053 bias    0.869\n",
      "iter 1761/10000  loss         0.101413  avg_L1_norm_grad         0.000069  w[0]   -0.053 bias    0.869\n",
      "iter 1780/10000  loss         0.101384  avg_L1_norm_grad         0.000068  w[0]   -0.053 bias    0.872\n",
      "iter 1781/10000  loss         0.101382  avg_L1_norm_grad         0.000068  w[0]   -0.053 bias    0.873\n",
      "iter 1800/10000  loss         0.101353  avg_L1_norm_grad         0.000067  w[0]   -0.053 bias    0.876\n",
      "iter 1801/10000  loss         0.101352  avg_L1_norm_grad         0.000067  w[0]   -0.053 bias    0.876\n",
      "iter 1820/10000  loss         0.101324  avg_L1_norm_grad         0.000066  w[0]   -0.053 bias    0.879\n",
      "iter 1821/10000  loss         0.101323  avg_L1_norm_grad         0.000066  w[0]   -0.053 bias    0.879\n",
      "iter 1840/10000  loss         0.101296  avg_L1_norm_grad         0.000065  w[0]   -0.053 bias    0.882\n",
      "iter 1841/10000  loss         0.101295  avg_L1_norm_grad         0.000065  w[0]   -0.053 bias    0.882\n",
      "iter 1860/10000  loss         0.101269  avg_L1_norm_grad         0.000064  w[0]   -0.053 bias    0.885\n",
      "iter 1861/10000  loss         0.101267  avg_L1_norm_grad         0.000064  w[0]   -0.053 bias    0.886\n",
      "iter 1880/10000  loss         0.101242  avg_L1_norm_grad         0.000063  w[0]   -0.053 bias    0.889\n",
      "iter 1881/10000  loss         0.101241  avg_L1_norm_grad         0.000063  w[0]   -0.053 bias    0.889\n",
      "iter 1900/10000  loss         0.101217  avg_L1_norm_grad         0.000062  w[0]   -0.053 bias    0.892\n",
      "iter 1901/10000  loss         0.101215  avg_L1_norm_grad         0.000062  w[0]   -0.053 bias    0.892\n",
      "iter 1920/10000  loss         0.101192  avg_L1_norm_grad         0.000061  w[0]   -0.053 bias    0.895\n",
      "iter 1921/10000  loss         0.101191  avg_L1_norm_grad         0.000061  w[0]   -0.053 bias    0.895\n",
      "iter 1940/10000  loss         0.101168  avg_L1_norm_grad         0.000060  w[0]   -0.053 bias    0.898\n",
      "iter 1941/10000  loss         0.101167  avg_L1_norm_grad         0.000060  w[0]   -0.053 bias    0.898\n",
      "iter 1960/10000  loss         0.101145  avg_L1_norm_grad         0.000059  w[0]   -0.053 bias    0.901\n",
      "iter 1961/10000  loss         0.101144  avg_L1_norm_grad         0.000059  w[0]   -0.053 bias    0.901\n",
      "iter 1980/10000  loss         0.101122  avg_L1_norm_grad         0.000058  w[0]   -0.053 bias    0.904\n",
      "iter 1981/10000  loss         0.101121  avg_L1_norm_grad         0.000058  w[0]   -0.053 bias    0.904\n",
      "iter 2000/10000  loss         0.101101  avg_L1_norm_grad         0.000057  w[0]   -0.052 bias    0.907\n",
      "iter 2001/10000  loss         0.101100  avg_L1_norm_grad         0.000057  w[0]   -0.052 bias    0.907\n",
      "iter 2020/10000  loss         0.101080  avg_L1_norm_grad         0.000056  w[0]   -0.052 bias    0.910\n",
      "iter 2021/10000  loss         0.101079  avg_L1_norm_grad         0.000056  w[0]   -0.052 bias    0.910\n",
      "iter 2040/10000  loss         0.101059  avg_L1_norm_grad         0.000055  w[0]   -0.052 bias    0.912\n",
      "iter 2041/10000  loss         0.101058  avg_L1_norm_grad         0.000055  w[0]   -0.052 bias    0.913\n",
      "iter 2060/10000  loss         0.101040  avg_L1_norm_grad         0.000054  w[0]   -0.052 bias    0.915\n",
      "iter 2061/10000  loss         0.101039  avg_L1_norm_grad         0.000054  w[0]   -0.052 bias    0.915\n",
      "iter 2080/10000  loss         0.101020  avg_L1_norm_grad         0.000053  w[0]   -0.052 bias    0.918\n",
      "iter 2081/10000  loss         0.101019  avg_L1_norm_grad         0.000053  w[0]   -0.052 bias    0.918\n",
      "iter 2100/10000  loss         0.101002  avg_L1_norm_grad         0.000052  w[0]   -0.052 bias    0.921\n",
      "iter 2101/10000  loss         0.101001  avg_L1_norm_grad         0.000052  w[0]   -0.052 bias    0.921\n",
      "iter 2120/10000  loss         0.100984  avg_L1_norm_grad         0.000051  w[0]   -0.052 bias    0.924\n",
      "iter 2121/10000  loss         0.100983  avg_L1_norm_grad         0.000051  w[0]   -0.052 bias    0.924\n",
      "iter 2140/10000  loss         0.100967  avg_L1_norm_grad         0.000051  w[0]   -0.052 bias    0.926\n",
      "iter 2141/10000  loss         0.100966  avg_L1_norm_grad         0.000051  w[0]   -0.052 bias    0.926\n",
      "iter 2160/10000  loss         0.100950  avg_L1_norm_grad         0.000050  w[0]   -0.052 bias    0.929\n",
      "iter 2161/10000  loss         0.100949  avg_L1_norm_grad         0.000050  w[0]   -0.052 bias    0.929\n",
      "iter 2180/10000  loss         0.100934  avg_L1_norm_grad         0.000049  w[0]   -0.052 bias    0.932\n",
      "iter 2181/10000  loss         0.100933  avg_L1_norm_grad         0.000049  w[0]   -0.052 bias    0.932\n",
      "iter 2200/10000  loss         0.100918  avg_L1_norm_grad         0.000048  w[0]   -0.052 bias    0.934\n",
      "iter 2201/10000  loss         0.100917  avg_L1_norm_grad         0.000048  w[0]   -0.052 bias    0.934\n",
      "iter 2220/10000  loss         0.100902  avg_L1_norm_grad         0.000047  w[0]   -0.052 bias    0.937\n",
      "iter 2221/10000  loss         0.100902  avg_L1_norm_grad         0.000047  w[0]   -0.052 bias    0.937\n",
      "iter 2240/10000  loss         0.100888  avg_L1_norm_grad         0.000047  w[0]   -0.052 bias    0.939\n",
      "iter 2241/10000  loss         0.100887  avg_L1_norm_grad         0.000047  w[0]   -0.052 bias    0.939\n",
      "iter 2260/10000  loss         0.100873  avg_L1_norm_grad         0.000046  w[0]   -0.052 bias    0.942\n",
      "iter 2261/10000  loss         0.100873  avg_L1_norm_grad         0.000046  w[0]   -0.052 bias    0.942\n",
      "iter 2280/10000  loss         0.100859  avg_L1_norm_grad         0.000045  w[0]   -0.052 bias    0.944\n",
      "iter 2281/10000  loss         0.100859  avg_L1_norm_grad         0.000045  w[0]   -0.052 bias    0.944\n",
      "iter 2300/10000  loss         0.100846  avg_L1_norm_grad         0.000044  w[0]   -0.051 bias    0.947\n",
      "iter 2301/10000  loss         0.100845  avg_L1_norm_grad         0.000044  w[0]   -0.051 bias    0.947\n",
      "iter 2320/10000  loss         0.100833  avg_L1_norm_grad         0.000044  w[0]   -0.051 bias    0.949\n",
      "iter 2321/10000  loss         0.100832  avg_L1_norm_grad         0.000044  w[0]   -0.051 bias    0.949\n",
      "iter 2340/10000  loss         0.100820  avg_L1_norm_grad         0.000043  w[0]   -0.051 bias    0.952\n",
      "iter 2341/10000  loss         0.100819  avg_L1_norm_grad         0.000043  w[0]   -0.051 bias    0.952\n",
      "iter 2360/10000  loss         0.100808  avg_L1_norm_grad         0.000042  w[0]   -0.051 bias    0.954\n",
      "iter 2361/10000  loss         0.100807  avg_L1_norm_grad         0.000042  w[0]   -0.051 bias    0.954\n",
      "iter 2380/10000  loss         0.100796  avg_L1_norm_grad         0.000042  w[0]   -0.051 bias    0.956\n",
      "iter 2381/10000  loss         0.100795  avg_L1_norm_grad         0.000042  w[0]   -0.051 bias    0.956\n",
      "iter 2400/10000  loss         0.100784  avg_L1_norm_grad         0.000041  w[0]   -0.051 bias    0.959\n",
      "iter 2401/10000  loss         0.100784  avg_L1_norm_grad         0.000041  w[0]   -0.051 bias    0.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2420/10000  loss         0.100773  avg_L1_norm_grad         0.000040  w[0]   -0.051 bias    0.961\n",
      "iter 2421/10000  loss         0.100772  avg_L1_norm_grad         0.000040  w[0]   -0.051 bias    0.961\n",
      "iter 2440/10000  loss         0.100762  avg_L1_norm_grad         0.000040  w[0]   -0.051 bias    0.963\n",
      "iter 2441/10000  loss         0.100761  avg_L1_norm_grad         0.000040  w[0]   -0.051 bias    0.963\n",
      "iter 2460/10000  loss         0.100751  avg_L1_norm_grad         0.000039  w[0]   -0.051 bias    0.966\n",
      "iter 2461/10000  loss         0.100751  avg_L1_norm_grad         0.000039  w[0]   -0.051 bias    0.966\n",
      "iter 2480/10000  loss         0.100741  avg_L1_norm_grad         0.000039  w[0]   -0.051 bias    0.968\n",
      "iter 2481/10000  loss         0.100741  avg_L1_norm_grad         0.000039  w[0]   -0.051 bias    0.968\n",
      "iter 2500/10000  loss         0.100731  avg_L1_norm_grad         0.000038  w[0]   -0.051 bias    0.970\n",
      "iter 2501/10000  loss         0.100731  avg_L1_norm_grad         0.000038  w[0]   -0.051 bias    0.970\n",
      "iter 2520/10000  loss         0.100721  avg_L1_norm_grad         0.000037  w[0]   -0.051 bias    0.972\n",
      "iter 2521/10000  loss         0.100721  avg_L1_norm_grad         0.000037  w[0]   -0.051 bias    0.972\n",
      "iter 2540/10000  loss         0.100712  avg_L1_norm_grad         0.000037  w[0]   -0.051 bias    0.974\n",
      "iter 2541/10000  loss         0.100712  avg_L1_norm_grad         0.000037  w[0]   -0.051 bias    0.974\n",
      "iter 2560/10000  loss         0.100703  avg_L1_norm_grad         0.000036  w[0]   -0.051 bias    0.976\n",
      "iter 2561/10000  loss         0.100702  avg_L1_norm_grad         0.000036  w[0]   -0.051 bias    0.977\n",
      "iter 2580/10000  loss         0.100694  avg_L1_norm_grad         0.000036  w[0]   -0.051 bias    0.979\n",
      "iter 2581/10000  loss         0.100694  avg_L1_norm_grad         0.000036  w[0]   -0.051 bias    0.979\n",
      "iter 2600/10000  loss         0.100685  avg_L1_norm_grad         0.000035  w[0]   -0.051 bias    0.981\n",
      "iter 2601/10000  loss         0.100685  avg_L1_norm_grad         0.000035  w[0]   -0.051 bias    0.981\n",
      "iter 2620/10000  loss         0.100677  avg_L1_norm_grad         0.000035  w[0]   -0.051 bias    0.983\n",
      "iter 2621/10000  loss         0.100677  avg_L1_norm_grad         0.000035  w[0]   -0.051 bias    0.983\n",
      "iter 2640/10000  loss         0.100669  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.985\n",
      "iter 2641/10000  loss         0.100669  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.985\n",
      "iter 2660/10000  loss         0.100661  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.987\n",
      "iter 2661/10000  loss         0.100661  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.987\n",
      "iter 2680/10000  loss         0.100654  avg_L1_norm_grad         0.000033  w[0]   -0.051 bias    0.989\n",
      "iter 2681/10000  loss         0.100653  avg_L1_norm_grad         0.000033  w[0]   -0.051 bias    0.989\n",
      "iter 2700/10000  loss         0.100646  avg_L1_norm_grad         0.000033  w[0]   -0.050 bias    0.991\n",
      "iter 2701/10000  loss         0.100646  avg_L1_norm_grad         0.000033  w[0]   -0.050 bias    0.991\n",
      "iter 2720/10000  loss         0.100639  avg_L1_norm_grad         0.000032  w[0]   -0.050 bias    0.993\n",
      "iter 2721/10000  loss         0.100639  avg_L1_norm_grad         0.000032  w[0]   -0.050 bias    0.993\n",
      "iter 2740/10000  loss         0.100632  avg_L1_norm_grad         0.000032  w[0]   -0.050 bias    0.995\n",
      "iter 2741/10000  loss         0.100632  avg_L1_norm_grad         0.000032  w[0]   -0.050 bias    0.995\n",
      "iter 2760/10000  loss         0.100625  avg_L1_norm_grad         0.000031  w[0]   -0.050 bias    0.997\n",
      "iter 2761/10000  loss         0.100625  avg_L1_norm_grad         0.000031  w[0]   -0.050 bias    0.997\n",
      "iter 2780/10000  loss         0.100619  avg_L1_norm_grad         0.000031  w[0]   -0.050 bias    0.998\n",
      "iter 2781/10000  loss         0.100618  avg_L1_norm_grad         0.000031  w[0]   -0.050 bias    0.999\n",
      "iter 2800/10000  loss         0.100612  avg_L1_norm_grad         0.000030  w[0]   -0.050 bias    1.000\n",
      "iter 2801/10000  loss         0.100612  avg_L1_norm_grad         0.000030  w[0]   -0.050 bias    1.000\n",
      "iter 2820/10000  loss         0.100606  avg_L1_norm_grad         0.000030  w[0]   -0.050 bias    1.002\n",
      "iter 2821/10000  loss         0.100606  avg_L1_norm_grad         0.000030  w[0]   -0.050 bias    1.002\n",
      "iter 2840/10000  loss         0.100600  avg_L1_norm_grad         0.000029  w[0]   -0.050 bias    1.004\n",
      "iter 2841/10000  loss         0.100600  avg_L1_norm_grad         0.000029  w[0]   -0.050 bias    1.004\n",
      "iter 2860/10000  loss         0.100594  avg_L1_norm_grad         0.000029  w[0]   -0.050 bias    1.006\n",
      "iter 2861/10000  loss         0.100594  avg_L1_norm_grad         0.000029  w[0]   -0.050 bias    1.006\n",
      "iter 2880/10000  loss         0.100588  avg_L1_norm_grad         0.000029  w[0]   -0.050 bias    1.008\n",
      "iter 2881/10000  loss         0.100588  avg_L1_norm_grad         0.000029  w[0]   -0.050 bias    1.008\n",
      "iter 2900/10000  loss         0.100583  avg_L1_norm_grad         0.000028  w[0]   -0.050 bias    1.009\n",
      "iter 2901/10000  loss         0.100582  avg_L1_norm_grad         0.000028  w[0]   -0.050 bias    1.009\n",
      "iter 2920/10000  loss         0.100577  avg_L1_norm_grad         0.000028  w[0]   -0.050 bias    1.011\n",
      "iter 2921/10000  loss         0.100577  avg_L1_norm_grad         0.000028  w[0]   -0.050 bias    1.011\n",
      "iter 2940/10000  loss         0.100572  avg_L1_norm_grad         0.000027  w[0]   -0.050 bias    1.013\n",
      "iter 2941/10000  loss         0.100572  avg_L1_norm_grad         0.000027  w[0]   -0.050 bias    1.013\n",
      "iter 2960/10000  loss         0.100567  avg_L1_norm_grad         0.000027  w[0]   -0.050 bias    1.015\n",
      "iter 2961/10000  loss         0.100567  avg_L1_norm_grad         0.000027  w[0]   -0.050 bias    1.015\n",
      "iter 2980/10000  loss         0.100562  avg_L1_norm_grad         0.000027  w[0]   -0.050 bias    1.016\n",
      "iter 2981/10000  loss         0.100562  avg_L1_norm_grad         0.000027  w[0]   -0.050 bias    1.016\n",
      "iter 3000/10000  loss         0.100557  avg_L1_norm_grad         0.000026  w[0]   -0.050 bias    1.018\n",
      "iter 3001/10000  loss         0.100557  avg_L1_norm_grad         0.000026  w[0]   -0.050 bias    1.018\n",
      "iter 3020/10000  loss         0.100552  avg_L1_norm_grad         0.000026  w[0]   -0.050 bias    1.020\n",
      "iter 3021/10000  loss         0.100552  avg_L1_norm_grad         0.000026  w[0]   -0.050 bias    1.020\n",
      "iter 3040/10000  loss         0.100548  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.021\n",
      "iter 3041/10000  loss         0.100548  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.021\n",
      "iter 3060/10000  loss         0.100543  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.023\n",
      "iter 3061/10000  loss         0.100543  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.023\n",
      "iter 3080/10000  loss         0.100539  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.025\n",
      "iter 3081/10000  loss         0.100539  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.025\n",
      "iter 3100/10000  loss         0.100535  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.026\n",
      "iter 3101/10000  loss         0.100535  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.026\n",
      "iter 3120/10000  loss         0.100531  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.028\n",
      "iter 3121/10000  loss         0.100531  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.028\n",
      "iter 3140/10000  loss         0.100527  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.029\n",
      "iter 3141/10000  loss         0.100527  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.029\n",
      "iter 3160/10000  loss         0.100523  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.031\n",
      "iter 3161/10000  loss         0.100523  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.031\n",
      "iter 3180/10000  loss         0.100519  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.032\n",
      "iter 3181/10000  loss         0.100519  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.032\n",
      "iter 3200/10000  loss         0.100516  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.034\n",
      "iter 3201/10000  loss         0.100515  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3220/10000  loss         0.100512  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.035\n",
      "iter 3221/10000  loss         0.100512  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.036\n",
      "iter 3240/10000  loss         0.100509  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.037\n",
      "iter 3241/10000  loss         0.100509  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.037\n",
      "iter 3260/10000  loss         0.100505  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.038\n",
      "iter 3261/10000  loss         0.100505  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.038\n",
      "iter 3280/10000  loss         0.100502  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.040\n",
      "iter 3281/10000  loss         0.100502  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.040\n",
      "iter 3300/10000  loss         0.100499  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.041\n",
      "iter 3301/10000  loss         0.100499  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.041\n",
      "iter 3320/10000  loss         0.100496  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.043\n",
      "iter 3321/10000  loss         0.100496  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.043\n",
      "iter 3340/10000  loss         0.100493  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.044\n",
      "iter 3341/10000  loss         0.100493  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.044\n",
      "iter 3360/10000  loss         0.100490  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.046\n",
      "iter 3361/10000  loss         0.100490  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.046\n",
      "iter 3380/10000  loss         0.100487  avg_L1_norm_grad         0.000020  w[0]   -0.049 bias    1.047\n",
      "iter 3381/10000  loss         0.100487  avg_L1_norm_grad         0.000020  w[0]   -0.049 bias    1.047\n",
      "iter 3400/10000  loss         0.100484  avg_L1_norm_grad         0.000020  w[0]   -0.049 bias    1.048\n",
      "iter 3401/10000  loss         0.100484  avg_L1_norm_grad         0.000020  w[0]   -0.049 bias    1.048\n",
      "iter 3420/10000  loss         0.100482  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.050\n",
      "iter 3421/10000  loss         0.100481  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.050\n",
      "iter 3440/10000  loss         0.100479  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.051\n",
      "iter 3441/10000  loss         0.100479  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.051\n",
      "iter 3460/10000  loss         0.100476  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.052\n",
      "iter 3461/10000  loss         0.100476  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.052\n",
      "iter 3480/10000  loss         0.100474  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.054\n",
      "iter 3481/10000  loss         0.100474  avg_L1_norm_grad         0.000019  w[0]   -0.049 bias    1.054\n",
      "iter 3500/10000  loss         0.100471  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.055\n",
      "iter 3501/10000  loss         0.100471  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.055\n",
      "iter 3520/10000  loss         0.100469  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.056\n",
      "iter 3521/10000  loss         0.100469  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.056\n",
      "iter 3540/10000  loss         0.100467  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.057\n",
      "iter 3541/10000  loss         0.100467  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.058\n",
      "iter 3560/10000  loss         0.100465  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.059\n",
      "iter 3561/10000  loss         0.100464  avg_L1_norm_grad         0.000018  w[0]   -0.049 bias    1.059\n",
      "iter 3580/10000  loss         0.100462  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.060\n",
      "iter 3581/10000  loss         0.100462  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.060\n",
      "iter 3600/10000  loss         0.100460  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.061\n",
      "iter 3601/10000  loss         0.100460  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.061\n",
      "iter 3620/10000  loss         0.100458  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.062\n",
      "iter 3621/10000  loss         0.100458  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.063\n",
      "iter 3640/10000  loss         0.100456  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.064\n",
      "iter 3641/10000  loss         0.100456  avg_L1_norm_grad         0.000017  w[0]   -0.049 bias    1.064\n",
      "iter 3660/10000  loss         0.100454  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.065\n",
      "iter 3661/10000  loss         0.100454  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.065\n",
      "iter 3680/10000  loss         0.100452  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.066\n",
      "iter 3681/10000  loss         0.100452  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.066\n",
      "iter 3700/10000  loss         0.100450  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.067\n",
      "iter 3701/10000  loss         0.100450  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.067\n",
      "iter 3720/10000  loss         0.100449  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.068\n",
      "iter 3721/10000  loss         0.100449  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.068\n",
      "iter 3740/10000  loss         0.100447  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.070\n",
      "iter 3741/10000  loss         0.100447  avg_L1_norm_grad         0.000016  w[0]   -0.049 bias    1.070\n",
      "iter 3760/10000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.071\n",
      "iter 3761/10000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.071\n",
      "iter 3780/10000  loss         0.100444  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.072\n",
      "iter 3781/10000  loss         0.100443  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.072\n",
      "iter 3800/10000  loss         0.100442  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.073\n",
      "iter 3801/10000  loss         0.100442  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.073\n",
      "iter 3820/10000  loss         0.100440  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.074\n",
      "iter 3821/10000  loss         0.100440  avg_L1_norm_grad         0.000015  w[0]   -0.049 bias    1.074\n",
      "iter 3840/10000  loss         0.100439  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.075\n",
      "iter 3841/10000  loss         0.100439  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.075\n",
      "iter 3860/10000  loss         0.100437  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.076\n",
      "iter 3861/10000  loss         0.100437  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.076\n",
      "iter 3880/10000  loss         0.100436  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.077\n",
      "iter 3881/10000  loss         0.100436  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.077\n",
      "iter 3900/10000  loss         0.100434  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.078\n",
      "iter 3901/10000  loss         0.100434  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.078\n",
      "iter 3920/10000  loss         0.100433  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.079\n",
      "iter 3921/10000  loss         0.100433  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.080\n",
      "iter 3940/10000  loss         0.100432  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.080\n",
      "iter 3941/10000  loss         0.100432  avg_L1_norm_grad         0.000014  w[0]   -0.049 bias    1.081\n",
      "iter 3960/10000  loss         0.100430  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.082\n",
      "iter 3961/10000  loss         0.100430  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.082\n",
      "iter 3980/10000  loss         0.100429  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.083\n",
      "iter 3981/10000  loss         0.100429  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.083\n",
      "iter 4000/10000  loss         0.100428  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.084\n",
      "iter 4001/10000  loss         0.100428  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4020/10000  loss         0.100427  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.085\n",
      "iter 4021/10000  loss         0.100427  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.085\n",
      "iter 4040/10000  loss         0.100425  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.086\n",
      "iter 4041/10000  loss         0.100425  avg_L1_norm_grad         0.000013  w[0]   -0.049 bias    1.086\n",
      "iter 4060/10000  loss         0.100424  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.087\n",
      "iter 4061/10000  loss         0.100424  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.087\n",
      "iter 4080/10000  loss         0.100423  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.088\n",
      "iter 4081/10000  loss         0.100423  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.088\n",
      "iter 4100/10000  loss         0.100422  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.088\n",
      "iter 4101/10000  loss         0.100422  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.089\n",
      "iter 4120/10000  loss         0.100421  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.089\n",
      "iter 4121/10000  loss         0.100421  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.089\n",
      "iter 4140/10000  loss         0.100420  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.090\n",
      "iter 4141/10000  loss         0.100420  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.090\n",
      "iter 4160/10000  loss         0.100419  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.091\n",
      "iter 4161/10000  loss         0.100419  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.091\n",
      "iter 4180/10000  loss         0.100418  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.092\n",
      "iter 4181/10000  loss         0.100418  avg_L1_norm_grad         0.000012  w[0]   -0.049 bias    1.092\n",
      "iter 4200/10000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.093\n",
      "iter 4201/10000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.093\n",
      "iter 4220/10000  loss         0.100416  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.094\n",
      "iter 4221/10000  loss         0.100416  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.094\n",
      "iter 4240/10000  loss         0.100415  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.095\n",
      "iter 4241/10000  loss         0.100415  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.095\n",
      "iter 4260/10000  loss         0.100414  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.096\n",
      "iter 4261/10000  loss         0.100414  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.096\n",
      "iter 4280/10000  loss         0.100413  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.097\n",
      "iter 4281/10000  loss         0.100413  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.097\n",
      "iter 4300/10000  loss         0.100412  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.098\n",
      "iter 4301/10000  loss         0.100412  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.098\n",
      "iter 4320/10000  loss         0.100412  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.099\n",
      "iter 4321/10000  loss         0.100412  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.099\n",
      "iter 4340/10000  loss         0.100411  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.099\n",
      "iter 4341/10000  loss         0.100411  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.099\n",
      "iter 4360/10000  loss         0.100410  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.100\n",
      "iter 4361/10000  loss         0.100410  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.100\n",
      "iter 4380/10000  loss         0.100409  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.101\n",
      "iter 4381/10000  loss         0.100409  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.101\n",
      "iter 4400/10000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.102\n",
      "iter 4401/10000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.102\n",
      "iter 4420/10000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.103\n",
      "iter 4421/10000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.103\n",
      "iter 4440/10000  loss         0.100407  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.104\n",
      "iter 4441/10000  loss         0.100407  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.104\n",
      "iter 4460/10000  loss         0.100406  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.104\n",
      "iter 4461/10000  loss         0.100406  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.104\n",
      "iter 4480/10000  loss         0.100406  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.105\n",
      "iter 4481/10000  loss         0.100406  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.105\n",
      "iter 4500/10000  loss         0.100405  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.106\n",
      "iter 4501/10000  loss         0.100405  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.106\n",
      "iter 4520/10000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.107\n",
      "iter 4521/10000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.107\n",
      "iter 4540/10000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.108\n",
      "iter 4541/10000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.108\n",
      "iter 4560/10000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.108\n",
      "iter 4561/10000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.108\n",
      "iter 4580/10000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.109\n",
      "iter 4581/10000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.109\n",
      "iter 4600/10000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.110\n",
      "iter 4601/10000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.110\n",
      "iter 4620/10000  loss         0.100401  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.111\n",
      "iter 4621/10000  loss         0.100401  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.111\n",
      "iter 4640/10000  loss         0.100401  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.111\n",
      "iter 4641/10000  loss         0.100401  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.111\n",
      "iter 4660/10000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.112\n",
      "iter 4661/10000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.112\n",
      "iter 4680/10000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.113\n",
      "iter 4681/10000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.113\n",
      "iter 4700/10000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.113\n",
      "iter 4701/10000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.114\n",
      "iter 4720/10000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.114\n",
      "iter 4721/10000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.114\n",
      "iter 4740/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.115\n",
      "iter 4741/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.115\n",
      "iter 4760/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.116\n",
      "iter 4761/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.116\n",
      "iter 4780/10000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.116\n",
      "iter 4781/10000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.116\n",
      "iter 4800/10000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.117\n",
      "iter 4801/10000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4820/10000  loss         0.100396  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.118\n",
      "iter 4821/10000  loss         0.100396  avg_L1_norm_grad         0.000008  w[0]   -0.049 bias    1.118\n",
      "iter 4840/10000  loss         0.100396  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.118\n",
      "iter 4841/10000  loss         0.100396  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.118\n",
      "iter 4860/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.119\n",
      "iter 4861/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.119\n",
      "iter 4880/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.120\n",
      "iter 4881/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.120\n",
      "iter 4900/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.120\n",
      "iter 4901/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.120\n",
      "iter 4920/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.121\n",
      "iter 4921/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.121\n",
      "iter 4940/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.122\n",
      "iter 4941/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.122\n",
      "iter 4960/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.122\n",
      "iter 4961/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.122\n",
      "iter 4980/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.123\n",
      "iter 4981/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.123\n",
      "iter 5000/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.124\n",
      "iter 5001/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.124\n",
      "iter 5020/10000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.124\n",
      "iter 5021/10000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.124\n",
      "iter 5040/10000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.125\n",
      "iter 5041/10000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.049 bias    1.125\n",
      "iter 5060/10000  loss         0.100392  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.125\n",
      "iter 5061/10000  loss         0.100392  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.125\n",
      "iter 5080/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.126\n",
      "iter 5081/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.126\n",
      "iter 5100/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.127\n",
      "iter 5101/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.127\n",
      "iter 5120/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.127\n",
      "iter 5121/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.127\n",
      "iter 5140/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.128\n",
      "iter 5141/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.128\n",
      "iter 5160/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.128\n",
      "iter 5161/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.128\n",
      "iter 5180/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.129\n",
      "iter 5181/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.129\n",
      "iter 5200/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.129\n",
      "iter 5201/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.129\n",
      "iter 5220/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.130\n",
      "iter 5221/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.130\n",
      "iter 5240/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.131\n",
      "iter 5241/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.049 bias    1.131\n",
      "iter 5260/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.131\n",
      "iter 5261/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.131\n",
      "iter 5280/10000  loss         0.100388  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.132\n",
      "iter 5281/10000  loss         0.100388  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.132\n",
      "iter 5300/10000  loss         0.100388  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.132\n",
      "iter 5301/10000  loss         0.100388  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.132\n",
      "iter 5320/10000  loss         0.100388  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.133\n",
      "iter 5321/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.133\n",
      "iter 5340/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.133\n",
      "iter 5341/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.133\n",
      "iter 5360/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 5361/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 5380/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 5381/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 5400/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 5401/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 5420/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 5421/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 5440/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 5441/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 5460/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 5461/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 5480/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 5481/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 5500/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 5501/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 5520/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 5521/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 5540/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 5541/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 5560/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 5561/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 5580/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 5581/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 5600/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.140\n",
      "iter 5601/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5620/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.140\n",
      "iter 5621/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.140\n",
      "iter 5640/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.141\n",
      "iter 5641/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.141\n",
      "iter 5660/10000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 5661/10000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 5680/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 5681/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 5700/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 5701/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 5720/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 5721/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 5740/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 5741/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 5760/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 5761/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 5780/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 5781/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 5800/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 5801/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 5820/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 5821/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 5840/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 5841/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 5860/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 5861/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 5880/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 5881/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 5900/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 5901/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 5920/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 5921/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 5940/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 5941/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 5960/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.148\n",
      "iter 5961/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.148\n",
      "iter 5980/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.148\n",
      "iter 5981/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.148\n",
      "iter 6000/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.148\n",
      "iter 6001/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.148\n",
      "iter 6020/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.149\n",
      "iter 6021/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.149\n",
      "iter 6040/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.149\n",
      "iter 6041/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.149\n",
      "iter 6060/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 6061/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 6080/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 6081/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 6100/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 6101/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 6120/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 6121/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 6140/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 6141/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 6160/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 6161/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 6180/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 6181/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 6200/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 6201/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 6220/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 6221/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 6240/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 6241/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 6260/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 6261/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 6280/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 6281/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 6300/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 6301/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 6320/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 6321/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 6340/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 6341/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 6360/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 6361/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 6380/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 6381/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 6400/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 6401/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6420/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 6421/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 6440/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 6441/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 6460/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 6461/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 6480/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 6481/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 6500/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 6501/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 6520/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 6521/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 6540/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.158\n",
      "iter 6541/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.158\n",
      "iter 6560/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.158\n",
      "iter 6561/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.158\n",
      "iter 6580/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.158\n",
      "iter 6581/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.158\n",
      "iter 6600/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.159\n",
      "iter 6601/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.159\n",
      "iter 6620/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 6621/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 6640/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 6641/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 6660/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 6661/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 6680/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 6681/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 6700/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 6701/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 6720/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 6721/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 6740/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6741/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6760/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6761/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6780/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6781/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6800/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6801/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 6820/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6821/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6840/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6841/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6860/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6861/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6880/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6881/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 6900/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 6901/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 6920/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 6921/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 6940/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 6941/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 6960/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 6961/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 6980/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 6981/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 7000/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 7001/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 7020/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 7021/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 7040/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 7041/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 7060/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7061/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7080/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7081/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7100/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7101/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7120/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7121/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 7140/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 7141/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 7160/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 7161/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 7180/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 7181/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 7200/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 7201/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7220/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7221/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7240/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7241/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7260/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7261/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7280/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7281/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7300/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7301/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 7320/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7321/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7340/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7341/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7360/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7361/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7380/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7381/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 7400/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7401/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7420/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7421/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7440/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7441/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7460/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7461/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.169\n",
      "iter 7480/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 7481/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "Done. Converged after 7495 iterations.\n",
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 10000 iters of gradient descent with step_size 0.78\n",
      "iter    0/10000  loss         1.000000  avg_L1_norm_grad         0.024676  w[0]    0.000 bias    0.000\n",
      "iter    1/10000  loss         0.650411  avg_L1_norm_grad         0.070058  w[0]   -0.002 bias    0.003\n",
      "iter    2/10000  loss         9.024562  avg_L1_norm_grad         0.168237  w[0]    0.023 bias    0.266\n",
      "iter    3/10000  loss        13.488173  avg_L1_norm_grad         0.152032  w[0]   -0.035 bias   -0.294\n",
      "iter    4/10000  loss         4.278193  avg_L1_norm_grad         0.160722  w[0]    0.020 bias    0.272\n",
      "iter    5/10000  loss        12.630333  avg_L1_norm_grad         0.152042  w[0]   -0.036 bias   -0.261\n",
      "iter    6/10000  loss         1.937613  avg_L1_norm_grad         0.090340  w[0]    0.019 bias    0.305\n",
      "iter    7/10000  loss         2.355437  avg_L1_norm_grad         0.097101  w[0]   -0.012 bias    0.011\n",
      "iter    8/10000  loss         2.275068  avg_L1_norm_grad         0.094728  w[0]    0.023 bias    0.373\n",
      "iter    9/10000  loss         1.780820  avg_L1_norm_grad         0.073715  w[0]   -0.009 bias    0.065\n",
      "iter   10/10000  loss         0.810761  avg_L1_norm_grad         0.036132  w[0]    0.017 bias    0.340\n",
      "iter   11/10000  loss         0.381617  avg_L1_norm_grad         0.008814  w[0]    0.004 bias    0.224\n",
      "iter   12/10000  loss         0.333203  avg_L1_norm_grad         0.002560  w[0]    0.007 bias    0.259\n",
      "iter   13/10000  loss         0.323370  avg_L1_norm_grad         0.002477  w[0]    0.006 bias    0.264\n",
      "iter   14/10000  loss         0.314205  avg_L1_norm_grad         0.002402  w[0]    0.006 bias    0.268\n",
      "iter   15/10000  loss         0.305640  avg_L1_norm_grad         0.002331  w[0]    0.005 bias    0.272\n",
      "iter   16/10000  loss         0.297616  avg_L1_norm_grad         0.002264  w[0]    0.004 bias    0.276\n",
      "iter   17/10000  loss         0.290078  avg_L1_norm_grad         0.002202  w[0]    0.003 bias    0.281\n",
      "iter   18/10000  loss         0.282979  avg_L1_norm_grad         0.002143  w[0]    0.002 bias    0.285\n",
      "iter   19/10000  loss         0.276276  avg_L1_norm_grad         0.002089  w[0]    0.001 bias    0.289\n",
      "iter   20/10000  loss         0.269932  avg_L1_norm_grad         0.002038  w[0]    0.001 bias    0.293\n",
      "iter   21/10000  loss         0.263914  avg_L1_norm_grad         0.001990  w[0]   -0.000 bias    0.296\n",
      "iter   40/10000  loss         0.187785  avg_L1_norm_grad         0.001390  w[0]   -0.017 bias    0.358\n",
      "iter   41/10000  loss         0.185123  avg_L1_norm_grad         0.001368  w[0]   -0.018 bias    0.361\n",
      "iter   60/10000  loss         0.149412  avg_L1_norm_grad         0.001010  w[0]   -0.032 bias    0.405\n",
      "iter   61/10000  loss         0.148135  avg_L1_norm_grad         0.000994  w[0]   -0.032 bias    0.407\n",
      "iter   80/10000  loss         0.130817  avg_L1_norm_grad         0.000740  w[0]   -0.042 bias    0.443\n",
      "iter   81/10000  loss         0.130181  avg_L1_norm_grad         0.000730  w[0]   -0.042 bias    0.445\n",
      "iter  100/10000  loss         0.121195  avg_L1_norm_grad         0.000564  w[0]   -0.049 bias    0.476\n",
      "iter  101/10000  loss         0.120846  avg_L1_norm_grad         0.000557  w[0]   -0.050 bias    0.477\n",
      "iter  120/10000  loss         0.115667  avg_L1_norm_grad         0.000448  w[0]   -0.054 bias    0.504\n",
      "iter  121/10000  loss         0.115456  avg_L1_norm_grad         0.000443  w[0]   -0.055 bias    0.505\n",
      "iter  140/10000  loss         0.112214  avg_L1_norm_grad         0.000366  w[0]   -0.058 bias    0.529\n",
      "iter  141/10000  loss         0.112077  avg_L1_norm_grad         0.000362  w[0]   -0.058 bias    0.530\n",
      "iter  160/10000  loss         0.109923  avg_L1_norm_grad         0.000306  w[0]   -0.061 bias    0.552\n",
      "iter  161/10000  loss         0.109829  avg_L1_norm_grad         0.000304  w[0]   -0.061 bias    0.553\n",
      "iter  180/10000  loss         0.108318  avg_L1_norm_grad         0.000262  w[0]   -0.062 bias    0.573\n",
      "iter  181/10000  loss         0.108250  avg_L1_norm_grad         0.000261  w[0]   -0.062 bias    0.575\n",
      "iter  200/10000  loss         0.107135  avg_L1_norm_grad         0.000229  w[0]   -0.064 bias    0.594\n",
      "iter  201/10000  loss         0.107084  avg_L1_norm_grad         0.000228  w[0]   -0.064 bias    0.595\n",
      "iter  220/10000  loss         0.106223  avg_L1_norm_grad         0.000204  w[0]   -0.064 bias    0.613\n",
      "iter  221/10000  loss         0.106183  avg_L1_norm_grad         0.000202  w[0]   -0.064 bias    0.614\n",
      "iter  240/10000  loss         0.105492  avg_L1_norm_grad         0.000183  w[0]   -0.065 bias    0.632\n",
      "iter  241/10000  loss         0.105459  avg_L1_norm_grad         0.000182  w[0]   -0.065 bias    0.633\n",
      "iter  260/10000  loss         0.104889  avg_L1_norm_grad         0.000167  w[0]   -0.065 bias    0.650\n",
      "iter  261/10000  loss         0.104862  avg_L1_norm_grad         0.000166  w[0]   -0.065 bias    0.651\n",
      "iter  280/10000  loss         0.104382  avg_L1_norm_grad         0.000153  w[0]   -0.065 bias    0.667\n",
      "iter  281/10000  loss         0.104358  avg_L1_norm_grad         0.000153  w[0]   -0.065 bias    0.668\n",
      "iter  300/10000  loss         0.103947  avg_L1_norm_grad         0.000142  w[0]   -0.064 bias    0.684\n",
      "iter  301/10000  loss         0.103927  avg_L1_norm_grad         0.000142  w[0]   -0.064 bias    0.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  320/10000  loss         0.103571  avg_L1_norm_grad         0.000133  w[0]   -0.064 bias    0.700\n",
      "iter  321/10000  loss         0.103553  avg_L1_norm_grad         0.000132  w[0]   -0.064 bias    0.701\n",
      "iter  340/10000  loss         0.103242  avg_L1_norm_grad         0.000124  w[0]   -0.063 bias    0.716\n",
      "iter  341/10000  loss         0.103227  avg_L1_norm_grad         0.000124  w[0]   -0.063 bias    0.717\n",
      "iter  360/10000  loss         0.102953  avg_L1_norm_grad         0.000116  w[0]   -0.063 bias    0.731\n",
      "iter  361/10000  loss         0.102940  avg_L1_norm_grad         0.000116  w[0]   -0.063 bias    0.732\n",
      "iter  380/10000  loss         0.102698  avg_L1_norm_grad         0.000109  w[0]   -0.062 bias    0.746\n",
      "iter  381/10000  loss         0.102687  avg_L1_norm_grad         0.000109  w[0]   -0.062 bias    0.746\n",
      "iter  400/10000  loss         0.102472  avg_L1_norm_grad         0.000103  w[0]   -0.062 bias    0.760\n",
      "iter  401/10000  loss         0.102462  avg_L1_norm_grad         0.000103  w[0]   -0.062 bias    0.760\n",
      "iter  420/10000  loss         0.102271  avg_L1_norm_grad         0.000097  w[0]   -0.061 bias    0.773\n",
      "iter  421/10000  loss         0.102262  avg_L1_norm_grad         0.000097  w[0]   -0.061 bias    0.774\n",
      "iter  440/10000  loss         0.102092  avg_L1_norm_grad         0.000092  w[0]   -0.061 bias    0.787\n",
      "iter  441/10000  loss         0.102084  avg_L1_norm_grad         0.000091  w[0]   -0.060 bias    0.787\n",
      "iter  460/10000  loss         0.101932  avg_L1_norm_grad         0.000087  w[0]   -0.060 bias    0.799\n",
      "iter  461/10000  loss         0.101924  avg_L1_norm_grad         0.000086  w[0]   -0.060 bias    0.800\n",
      "iter  480/10000  loss         0.101788  avg_L1_norm_grad         0.000082  w[0]   -0.059 bias    0.812\n",
      "iter  481/10000  loss         0.101781  avg_L1_norm_grad         0.000082  w[0]   -0.059 bias    0.812\n",
      "iter  500/10000  loss         0.101659  avg_L1_norm_grad         0.000077  w[0]   -0.059 bias    0.824\n",
      "iter  501/10000  loss         0.101653  avg_L1_norm_grad         0.000077  w[0]   -0.059 bias    0.824\n",
      "iter  520/10000  loss         0.101543  avg_L1_norm_grad         0.000073  w[0]   -0.058 bias    0.835\n",
      "iter  521/10000  loss         0.101537  avg_L1_norm_grad         0.000073  w[0]   -0.058 bias    0.836\n",
      "iter  540/10000  loss         0.101438  avg_L1_norm_grad         0.000070  w[0]   -0.058 bias    0.846\n",
      "iter  541/10000  loss         0.101434  avg_L1_norm_grad         0.000069  w[0]   -0.058 bias    0.847\n",
      "iter  560/10000  loss         0.101344  avg_L1_norm_grad         0.000066  w[0]   -0.057 bias    0.857\n",
      "iter  561/10000  loss         0.101340  avg_L1_norm_grad         0.000066  w[0]   -0.057 bias    0.857\n",
      "iter  580/10000  loss         0.101259  avg_L1_norm_grad         0.000063  w[0]   -0.057 bias    0.867\n",
      "iter  581/10000  loss         0.101255  avg_L1_norm_grad         0.000062  w[0]   -0.057 bias    0.868\n",
      "iter  600/10000  loss         0.101182  avg_L1_norm_grad         0.000059  w[0]   -0.056 bias    0.877\n",
      "iter  601/10000  loss         0.101179  avg_L1_norm_grad         0.000059  w[0]   -0.056 bias    0.878\n",
      "iter  620/10000  loss         0.101113  avg_L1_norm_grad         0.000056  w[0]   -0.056 bias    0.887\n",
      "iter  621/10000  loss         0.101109  avg_L1_norm_grad         0.000056  w[0]   -0.056 bias    0.887\n",
      "iter  640/10000  loss         0.101049  avg_L1_norm_grad         0.000054  w[0]   -0.055 bias    0.896\n",
      "iter  641/10000  loss         0.101046  avg_L1_norm_grad         0.000054  w[0]   -0.055 bias    0.897\n",
      "iter  660/10000  loss         0.100992  avg_L1_norm_grad         0.000051  w[0]   -0.055 bias    0.905\n",
      "iter  661/10000  loss         0.100989  avg_L1_norm_grad         0.000051  w[0]   -0.055 bias    0.906\n",
      "iter  680/10000  loss         0.100940  avg_L1_norm_grad         0.000049  w[0]   -0.054 bias    0.914\n",
      "iter  681/10000  loss         0.100938  avg_L1_norm_grad         0.000048  w[0]   -0.054 bias    0.914\n",
      "iter  700/10000  loss         0.100893  avg_L1_norm_grad         0.000046  w[0]   -0.054 bias    0.922\n",
      "iter  701/10000  loss         0.100891  avg_L1_norm_grad         0.000046  w[0]   -0.054 bias    0.923\n",
      "iter  720/10000  loss         0.100850  avg_L1_norm_grad         0.000044  w[0]   -0.054 bias    0.930\n",
      "iter  721/10000  loss         0.100848  avg_L1_norm_grad         0.000044  w[0]   -0.054 bias    0.931\n",
      "iter  740/10000  loss         0.100811  avg_L1_norm_grad         0.000042  w[0]   -0.053 bias    0.938\n",
      "iter  741/10000  loss         0.100809  avg_L1_norm_grad         0.000042  w[0]   -0.053 bias    0.939\n",
      "iter  760/10000  loss         0.100775  avg_L1_norm_grad         0.000040  w[0]   -0.053 bias    0.946\n",
      "iter  761/10000  loss         0.100773  avg_L1_norm_grad         0.000040  w[0]   -0.053 bias    0.946\n",
      "iter  780/10000  loss         0.100742  avg_L1_norm_grad         0.000038  w[0]   -0.053 bias    0.953\n",
      "iter  781/10000  loss         0.100741  avg_L1_norm_grad         0.000038  w[0]   -0.053 bias    0.954\n",
      "iter  800/10000  loss         0.100713  avg_L1_norm_grad         0.000036  w[0]   -0.052 bias    0.960\n",
      "iter  801/10000  loss         0.100711  avg_L1_norm_grad         0.000036  w[0]   -0.052 bias    0.961\n",
      "iter  820/10000  loss         0.100686  avg_L1_norm_grad         0.000035  w[0]   -0.052 bias    0.967\n",
      "iter  821/10000  loss         0.100684  avg_L1_norm_grad         0.000035  w[0]   -0.052 bias    0.968\n",
      "iter  840/10000  loss         0.100661  avg_L1_norm_grad         0.000033  w[0]   -0.052 bias    0.974\n",
      "iter  841/10000  loss         0.100660  avg_L1_norm_grad         0.000033  w[0]   -0.052 bias    0.974\n",
      "iter  860/10000  loss         0.100638  avg_L1_norm_grad         0.000032  w[0]   -0.052 bias    0.980\n",
      "iter  861/10000  loss         0.100637  avg_L1_norm_grad         0.000032  w[0]   -0.052 bias    0.981\n",
      "iter  880/10000  loss         0.100618  avg_L1_norm_grad         0.000030  w[0]   -0.052 bias    0.987\n",
      "iter  881/10000  loss         0.100617  avg_L1_norm_grad         0.000030  w[0]   -0.052 bias    0.987\n",
      "iter  900/10000  loss         0.100599  avg_L1_norm_grad         0.000029  w[0]   -0.051 bias    0.993\n",
      "iter  901/10000  loss         0.100598  avg_L1_norm_grad         0.000029  w[0]   -0.051 bias    0.993\n",
      "iter  920/10000  loss         0.100581  avg_L1_norm_grad         0.000028  w[0]   -0.051 bias    0.999\n",
      "iter  921/10000  loss         0.100580  avg_L1_norm_grad         0.000028  w[0]   -0.051 bias    0.999\n",
      "iter  940/10000  loss         0.100565  avg_L1_norm_grad         0.000026  w[0]   -0.051 bias    1.004\n",
      "iter  941/10000  loss         0.100564  avg_L1_norm_grad         0.000026  w[0]   -0.051 bias    1.004\n",
      "iter  960/10000  loss         0.100551  avg_L1_norm_grad         0.000025  w[0]   -0.051 bias    1.010\n",
      "iter  961/10000  loss         0.100550  avg_L1_norm_grad         0.000025  w[0]   -0.051 bias    1.010\n",
      "iter  980/10000  loss         0.100537  avg_L1_norm_grad         0.000024  w[0]   -0.051 bias    1.015\n",
      "iter  981/10000  loss         0.100537  avg_L1_norm_grad         0.000024  w[0]   -0.051 bias    1.015\n",
      "iter 1000/10000  loss         0.100525  avg_L1_norm_grad         0.000023  w[0]   -0.051 bias    1.020\n",
      "iter 1001/10000  loss         0.100524  avg_L1_norm_grad         0.000023  w[0]   -0.051 bias    1.020\n",
      "iter 1020/10000  loss         0.100514  avg_L1_norm_grad         0.000022  w[0]   -0.051 bias    1.025\n",
      "iter 1021/10000  loss         0.100513  avg_L1_norm_grad         0.000022  w[0]   -0.051 bias    1.025\n",
      "iter 1040/10000  loss         0.100503  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.030\n",
      "iter 1041/10000  loss         0.100503  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.030\n",
      "iter 1060/10000  loss         0.100494  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.035\n",
      "iter 1061/10000  loss         0.100493  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.035\n",
      "iter 1080/10000  loss         0.100485  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.039\n",
      "iter 1081/10000  loss         0.100484  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.040\n",
      "iter 1100/10000  loss         0.100477  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.044\n",
      "iter 1101/10000  loss         0.100476  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1120/10000  loss         0.100469  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.048\n",
      "iter 1121/10000  loss         0.100469  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.048\n",
      "iter 1140/10000  loss         0.100462  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.052\n",
      "iter 1141/10000  loss         0.100462  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.052\n",
      "iter 1160/10000  loss         0.100456  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.056\n",
      "iter 1161/10000  loss         0.100456  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.056\n",
      "iter 1180/10000  loss         0.100450  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.060\n",
      "iter 1181/10000  loss         0.100450  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.060\n",
      "iter 1200/10000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.064\n",
      "iter 1201/10000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.064\n",
      "iter 1220/10000  loss         0.100440  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.067\n",
      "iter 1221/10000  loss         0.100440  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.068\n",
      "iter 1240/10000  loss         0.100435  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.071\n",
      "iter 1241/10000  loss         0.100435  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.071\n",
      "iter 1260/10000  loss         0.100431  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.074\n",
      "iter 1261/10000  loss         0.100431  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.075\n",
      "iter 1280/10000  loss         0.100427  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.078\n",
      "iter 1281/10000  loss         0.100427  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.078\n",
      "iter 1300/10000  loss         0.100424  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.081\n",
      "iter 1301/10000  loss         0.100423  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.081\n",
      "iter 1320/10000  loss         0.100420  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.084\n",
      "iter 1321/10000  loss         0.100420  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.084\n",
      "iter 1340/10000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.087\n",
      "iter 1341/10000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.087\n",
      "iter 1360/10000  loss         0.100414  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.090\n",
      "iter 1361/10000  loss         0.100414  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.090\n",
      "iter 1380/10000  loss         0.100412  avg_L1_norm_grad         0.000010  w[0]   -0.050 bias    1.093\n",
      "iter 1381/10000  loss         0.100412  avg_L1_norm_grad         0.000010  w[0]   -0.050 bias    1.093\n",
      "iter 1400/10000  loss         0.100409  avg_L1_norm_grad         0.000010  w[0]   -0.050 bias    1.096\n",
      "iter 1401/10000  loss         0.100409  avg_L1_norm_grad         0.000010  w[0]   -0.050 bias    1.096\n",
      "iter 1420/10000  loss         0.100407  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.099\n",
      "iter 1421/10000  loss         0.100407  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.099\n",
      "iter 1440/10000  loss         0.100405  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.101\n",
      "iter 1441/10000  loss         0.100405  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.101\n",
      "iter 1460/10000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.104\n",
      "iter 1461/10000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.104\n",
      "iter 1480/10000  loss         0.100401  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.106\n",
      "iter 1481/10000  loss         0.100401  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.106\n",
      "iter 1500/10000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.109\n",
      "iter 1501/10000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.109\n",
      "iter 1520/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.111\n",
      "iter 1521/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.111\n",
      "iter 1540/10000  loss         0.100397  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.113\n",
      "iter 1541/10000  loss         0.100396  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.113\n",
      "iter 1560/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.115\n",
      "iter 1561/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.116\n",
      "iter 1580/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.118\n",
      "iter 1581/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.118\n",
      "iter 1600/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.120\n",
      "iter 1601/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.120\n",
      "iter 1620/10000  loss         0.100392  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.122\n",
      "iter 1621/10000  loss         0.100392  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.122\n",
      "iter 1640/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.124\n",
      "iter 1641/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.124\n",
      "iter 1660/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.125\n",
      "iter 1661/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.126\n",
      "iter 1680/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.127\n",
      "iter 1681/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.127\n",
      "iter 1700/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.129\n",
      "iter 1701/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.129\n",
      "iter 1720/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.131\n",
      "iter 1721/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.131\n",
      "iter 1740/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.133\n",
      "iter 1741/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.133\n",
      "iter 1760/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 1761/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 1780/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 1781/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 1800/10000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.137\n",
      "iter 1801/10000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.137\n",
      "iter 1820/10000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.139\n",
      "iter 1821/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.139\n",
      "iter 1840/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 1841/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 1860/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 1861/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 1880/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 1881/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 1900/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 1901/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1920/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 1921/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 1940/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.147\n",
      "iter 1941/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.147\n",
      "iter 1960/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.148\n",
      "iter 1961/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.148\n",
      "iter 1980/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.149\n",
      "iter 1981/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 2000/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 2001/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 2020/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 2021/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 2040/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 2041/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 2060/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 2061/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 2080/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 2081/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 2100/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 2101/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 2120/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.157\n",
      "iter 2121/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.157\n",
      "iter 2140/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 2141/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 2160/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 2161/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 2180/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 2181/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 2200/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 2201/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 2220/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 2221/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 2240/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 2241/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 2260/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 2261/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 2280/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 2281/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 2300/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 2301/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 2320/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 2321/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 2340/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 2341/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 2360/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 2361/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 2380/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 2381/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 2400/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.168\n",
      "iter 2401/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.168\n",
      "iter 2420/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 2421/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 2440/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 2441/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 2460/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 2461/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 2480/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 2481/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 2500/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.171\n",
      "iter 2501/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.171\n",
      "iter 2520/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.172\n",
      "iter 2521/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.172\n",
      "iter 2540/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.172\n",
      "iter 2541/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.172\n",
      "iter 2560/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 2561/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 2580/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 2581/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 2600/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 2601/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 2620/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 2621/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 2640/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 2641/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 2660/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 2661/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 2680/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 2681/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 2700/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 2701/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2720/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 2721/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 2740/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 2741/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 2760/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 2761/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 2780/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 2781/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 2800/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 2801/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 2820/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 2821/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 2840/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 2841/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 2860/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 2861/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 2880/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 2881/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 2900/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 2901/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 2920/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 2921/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 2940/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 2941/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 2960/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 2961/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 2980/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 2981/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 3000/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 3001/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 3020/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.182\n",
      "iter 3021/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.182\n",
      "iter 3040/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.182\n",
      "iter 3041/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.182\n",
      "iter 3060/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.182\n",
      "iter 3061/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.182\n",
      "iter 3080/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.182\n",
      "iter 3081/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.182\n",
      "iter 3100/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3101/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3120/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3121/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3140/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3141/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3160/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3161/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.183\n",
      "iter 3180/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3181/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3200/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3201/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3220/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3221/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3240/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3241/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.184\n",
      "iter 3260/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.185\n",
      "iter 3261/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.185\n",
      "iter 3280/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.185\n",
      "iter 3281/10000  loss         0.100377  avg_L1_norm_grad         0.000000  w[0]   -0.051 bias    1.185\n",
      "Done. Converged after 3299 iterations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m_lr = LogisticRegressionGradientDescent(\n",
    "        alpha=10, step_size = 0.25, init_w_recipe='zeros')\n",
    "m_lr.fit(tsetx_AF, tsety_A)\n",
    "l_lr = LogisticRegressionGradientDescent(\n",
    "        alpha=10, step_size = 0.78, init_w_recipe='zeros')\n",
    "l_lr.fit(tsetx_AF, tsety_A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x26d81c797b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(s_lr.trace_steps, np.array(s_lr.trace_loss)[:,0], label =\"small step size\")\n",
    "plt.plot(m_lr.trace_steps, np.array(m_lr.trace_loss)[:,0], label=\"medium step size\")\n",
    "plt.plot(l_lr.trace_steps, np.array(l_lr.trace_loss)[:,0], label = \"large step size\")\n",
    "plt.set_xlim([0, 30])\n",
    "plt.title(\"Iterations vs Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x26d83027780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(s_lr.trace_steps, s_lr.trace_L1_norm_of_grad, label = \"small step size\")\n",
    "plt.plot(m_lr.trace_steps, m_lr.trace_L1_norm_of_grad, label = \"medium step size\")\n",
    "plt.plot(l_lr.trace_steps, l_lr.trace_L1_norm_of_grad, label = \"large step size\")\n",
    "plt.title(\"Iterations vs L1 Norm of Grad\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Iterations vs Weight and Bias')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(s_lr.trace_steps, np.array(s_lr.trace_w)[:,154], label=\"small weight 154\")\n",
    "plt.plot(s_lr.trace_steps, np.array(s_lr.trace_w)[:,-1], label = \"small bias\")\n",
    "plt.plot(m_lr.trace_steps, np.array(m_lr.trace_w)[:,154], label=\"medium weight 154\")\n",
    "plt.plot(m_lr.trace_steps, np.array(m_lr.trace_w)[:,-1], label = \"medium bias\")\n",
    "plt.plot(l_lr.trace_steps, np.array(l_lr.trace_w)[:,154], label=\"large weight 154\")\n",
    "plt.plot(l_lr.trace_steps, np.array(l_lr.trace_w)[:,-1], label = \"large bias\")\n",
    "plt.legend()\n",
    "plt.title(\"Iterations vs Weight and Bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include a thorough caption for the figure describing what trends you observe (what should happen to the loss across all 3 lines? what actually happens?). Be sure to specify which specific step sizes are shown, any relevant information about other step sizes you tried, and which step size you recommend to use going forward. Did you find any step sizes that diverged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the Iteration and Loss plot, the loss decreases as there are more iterations. The medium and large step size lines end early because they converge in less than 10,000 steps. The small step size, which doesn't converge approaches the same value that the other two step sizes do. For small step size, I used step_size = 0.01. For medium step_size, I used 0.25, and for large step size, I used 0.78. I found that any number past 0.8 for step_size diverged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing w_G with 785 features using recipe: zeros\n",
      "Running up to 10000 iters of gradient descent with step_size 0.5\n",
      "iter    0/10000  loss         1.000000  avg_L1_norm_grad         0.024676  w[0]    0.000 bias    0.000\n",
      "iter    1/10000  loss         0.652834  avg_L1_norm_grad         0.058458  w[0]   -0.001 bias    0.002\n",
      "iter    2/10000  loss         4.480393  avg_L1_norm_grad         0.167414  w[0]    0.012 bias    0.145\n",
      "iter    3/10000  loss         9.642480  avg_L1_norm_grad         0.151994  w[0]   -0.025 bias   -0.212\n",
      "iter    4/10000  loss         1.741418  avg_L1_norm_grad         0.129421  w[0]    0.010 bias    0.151\n",
      "iter    5/10000  loss         6.221672  avg_L1_norm_grad         0.151786  w[0]   -0.018 bias   -0.121\n",
      "iter    6/10000  loss         2.955412  avg_L1_norm_grad         0.146658  w[0]    0.016 bias    0.241\n",
      "iter    7/10000  loss         4.446966  avg_L1_norm_grad         0.147311  w[0]   -0.016 bias   -0.069\n",
      "iter    8/10000  loss         2.279642  avg_L1_norm_grad         0.115011  w[0]    0.018 bias    0.282\n",
      "iter    9/10000  loss         1.348115  avg_L1_norm_grad         0.071511  w[0]   -0.008 bias    0.041\n",
      "iter   10/10000  loss         0.421200  avg_L1_norm_grad         0.021189  w[0]    0.009 bias    0.212\n",
      "iter   11/10000  loss         0.285284  avg_L1_norm_grad         0.003384  w[0]    0.004 bias    0.169\n",
      "iter   12/10000  loss         0.276035  avg_L1_norm_grad         0.002788  w[0]    0.004 bias    0.177\n",
      "iter   13/10000  loss         0.268467  avg_L1_norm_grad         0.002686  w[0]    0.004 bias    0.181\n",
      "iter   14/10000  loss         0.261457  avg_L1_norm_grad         0.002595  w[0]    0.003 bias    0.184\n",
      "iter   15/10000  loss         0.254948  avg_L1_norm_grad         0.002512  w[0]    0.003 bias    0.187\n",
      "iter   16/10000  loss         0.248890  avg_L1_norm_grad         0.002435  w[0]    0.003 bias    0.191\n",
      "iter   17/10000  loss         0.243236  avg_L1_norm_grad         0.002362  w[0]    0.002 bias    0.194\n",
      "iter   18/10000  loss         0.237947  avg_L1_norm_grad         0.002294  w[0]    0.002 bias    0.197\n",
      "iter   19/10000  loss         0.232988  avg_L1_norm_grad         0.002230  w[0]    0.001 bias    0.200\n",
      "iter   20/10000  loss         0.228328  avg_L1_norm_grad         0.002171  w[0]    0.001 bias    0.203\n",
      "iter   21/10000  loss         0.223939  avg_L1_norm_grad         0.002115  w[0]    0.000 bias    0.206\n",
      "iter   40/10000  loss         0.171064  avg_L1_norm_grad         0.001441  w[0]   -0.009 bias    0.256\n",
      "iter   41/10000  loss         0.169277  avg_L1_norm_grad         0.001417  w[0]   -0.009 bias    0.259\n",
      "iter   60/10000  loss         0.145200  avg_L1_norm_grad         0.001048  w[0]   -0.018 bias    0.298\n",
      "iter   61/10000  loss         0.144328  avg_L1_norm_grad         0.001033  w[0]   -0.018 bias    0.300\n",
      "iter   80/10000  loss         0.132288  avg_L1_norm_grad         0.000789  w[0]   -0.025 bias    0.334\n",
      "iter   81/10000  loss         0.131836  avg_L1_norm_grad         0.000778  w[0]   -0.025 bias    0.335\n",
      "iter  100/10000  loss         0.125286  avg_L1_norm_grad         0.000618  w[0]   -0.030 bias    0.364\n",
      "iter  101/10000  loss         0.125023  avg_L1_norm_grad         0.000611  w[0]   -0.030 bias    0.366\n",
      "iter  120/10000  loss         0.120979  avg_L1_norm_grad         0.000510  w[0]   -0.035 bias    0.392\n",
      "iter  121/10000  loss         0.120806  avg_L1_norm_grad         0.000506  w[0]   -0.035 bias    0.393\n",
      "iter  140/10000  loss         0.118004  avg_L1_norm_grad         0.000438  w[0]   -0.038 bias    0.417\n",
      "iter  141/10000  loss         0.117878  avg_L1_norm_grad         0.000435  w[0]   -0.039 bias    0.418\n",
      "iter  160/10000  loss         0.115776  avg_L1_norm_grad         0.000387  w[0]   -0.042 bias    0.440\n",
      "iter  161/10000  loss         0.115678  avg_L1_norm_grad         0.000385  w[0]   -0.042 bias    0.441\n",
      "iter  180/10000  loss         0.114017  avg_L1_norm_grad         0.000348  w[0]   -0.044 bias    0.462\n",
      "iter  181/10000  loss         0.113939  avg_L1_norm_grad         0.000346  w[0]   -0.044 bias    0.463\n",
      "iter  200/10000  loss         0.112579  avg_L1_norm_grad         0.000318  w[0]   -0.047 bias    0.482\n",
      "iter  201/10000  loss         0.112513  avg_L1_norm_grad         0.000317  w[0]   -0.047 bias    0.483\n",
      "iter  220/10000  loss         0.111371  avg_L1_norm_grad         0.000294  w[0]   -0.049 bias    0.501\n",
      "iter  221/10000  loss         0.111316  avg_L1_norm_grad         0.000293  w[0]   -0.049 bias    0.502\n",
      "iter  240/10000  loss         0.110339  avg_L1_norm_grad         0.000273  w[0]   -0.050 bias    0.519\n",
      "iter  241/10000  loss         0.110291  avg_L1_norm_grad         0.000272  w[0]   -0.050 bias    0.520\n",
      "iter  260/10000  loss         0.109443  avg_L1_norm_grad         0.000255  w[0]   -0.052 bias    0.536\n",
      "iter  261/10000  loss         0.109401  avg_L1_norm_grad         0.000255  w[0]   -0.052 bias    0.537\n",
      "iter  280/10000  loss         0.108657  avg_L1_norm_grad         0.000240  w[0]   -0.053 bias    0.553\n",
      "iter  281/10000  loss         0.108620  avg_L1_norm_grad         0.000239  w[0]   -0.053 bias    0.554\n",
      "iter  300/10000  loss         0.107961  avg_L1_norm_grad         0.000226  w[0]   -0.054 bias    0.569\n",
      "iter  301/10000  loss         0.107928  avg_L1_norm_grad         0.000225  w[0]   -0.054 bias    0.569\n",
      "iter  320/10000  loss         0.107341  avg_L1_norm_grad         0.000214  w[0]   -0.055 bias    0.584\n",
      "iter  321/10000  loss         0.107312  avg_L1_norm_grad         0.000213  w[0]   -0.055 bias    0.584\n",
      "iter  340/10000  loss         0.106786  avg_L1_norm_grad         0.000203  w[0]   -0.056 bias    0.598\n",
      "iter  341/10000  loss         0.106759  avg_L1_norm_grad         0.000202  w[0]   -0.056 bias    0.599\n",
      "iter  360/10000  loss         0.106286  avg_L1_norm_grad         0.000192  w[0]   -0.056 bias    0.612\n",
      "iter  361/10000  loss         0.106262  avg_L1_norm_grad         0.000192  w[0]   -0.056 bias    0.613\n",
      "iter  380/10000  loss         0.105834  avg_L1_norm_grad         0.000183  w[0]   -0.057 bias    0.626\n",
      "iter  381/10000  loss         0.105812  avg_L1_norm_grad         0.000183  w[0]   -0.057 bias    0.627\n",
      "iter  400/10000  loss         0.105424  avg_L1_norm_grad         0.000174  w[0]   -0.057 bias    0.639\n",
      "iter  401/10000  loss         0.105405  avg_L1_norm_grad         0.000174  w[0]   -0.057 bias    0.640\n",
      "iter  420/10000  loss         0.105052  avg_L1_norm_grad         0.000166  w[0]   -0.058 bias    0.652\n",
      "iter  421/10000  loss         0.105034  avg_L1_norm_grad         0.000166  w[0]   -0.058 bias    0.653\n",
      "iter  440/10000  loss         0.104712  avg_L1_norm_grad         0.000159  w[0]   -0.058 bias    0.664\n",
      "iter  441/10000  loss         0.104696  avg_L1_norm_grad         0.000159  w[0]   -0.058 bias    0.665\n",
      "iter  460/10000  loss         0.104402  avg_L1_norm_grad         0.000152  w[0]   -0.058 bias    0.676\n",
      "iter  461/10000  loss         0.104388  avg_L1_norm_grad         0.000152  w[0]   -0.058 bias    0.677\n",
      "iter  480/10000  loss         0.104118  avg_L1_norm_grad         0.000145  w[0]   -0.058 bias    0.688\n",
      "iter  481/10000  loss         0.104105  avg_L1_norm_grad         0.000145  w[0]   -0.058 bias    0.688\n",
      "iter  500/10000  loss         0.103858  avg_L1_norm_grad         0.000139  w[0]   -0.058 bias    0.699\n",
      "iter  501/10000  loss         0.103845  avg_L1_norm_grad         0.000139  w[0]   -0.058 bias    0.700\n",
      "iter  520/10000  loss         0.103619  avg_L1_norm_grad         0.000133  w[0]   -0.058 bias    0.710\n",
      "iter  521/10000  loss         0.103607  avg_L1_norm_grad         0.000133  w[0]   -0.058 bias    0.711\n",
      "iter  540/10000  loss         0.103398  avg_L1_norm_grad         0.000128  w[0]   -0.058 bias    0.721\n",
      "iter  541/10000  loss         0.103388  avg_L1_norm_grad         0.000128  w[0]   -0.058 bias    0.721\n",
      "iter  560/10000  loss         0.103195  avg_L1_norm_grad         0.000123  w[0]   -0.058 bias    0.731\n",
      "iter  561/10000  loss         0.103186  avg_L1_norm_grad         0.000123  w[0]   -0.058 bias    0.731\n",
      "iter  580/10000  loss         0.103008  avg_L1_norm_grad         0.000118  w[0]   -0.058 bias    0.741\n",
      "iter  581/10000  loss         0.102999  avg_L1_norm_grad         0.000118  w[0]   -0.058 bias    0.741\n",
      "iter  600/10000  loss         0.102835  avg_L1_norm_grad         0.000113  w[0]   -0.058 bias    0.751\n",
      "iter  601/10000  loss         0.102827  avg_L1_norm_grad         0.000113  w[0]   -0.058 bias    0.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  620/10000  loss         0.102675  avg_L1_norm_grad         0.000109  w[0]   -0.058 bias    0.760\n",
      "iter  621/10000  loss         0.102667  avg_L1_norm_grad         0.000109  w[0]   -0.058 bias    0.761\n",
      "iter  640/10000  loss         0.102527  avg_L1_norm_grad         0.000105  w[0]   -0.058 bias    0.770\n",
      "iter  641/10000  loss         0.102520  avg_L1_norm_grad         0.000105  w[0]   -0.058 bias    0.770\n",
      "iter  660/10000  loss         0.102389  avg_L1_norm_grad         0.000101  w[0]   -0.058 bias    0.779\n",
      "iter  661/10000  loss         0.102383  avg_L1_norm_grad         0.000101  w[0]   -0.058 bias    0.779\n",
      "iter  680/10000  loss         0.102262  avg_L1_norm_grad         0.000097  w[0]   -0.057 bias    0.787\n",
      "iter  681/10000  loss         0.102256  avg_L1_norm_grad         0.000097  w[0]   -0.057 bias    0.788\n",
      "iter  700/10000  loss         0.102143  avg_L1_norm_grad         0.000094  w[0]   -0.057 bias    0.796\n",
      "iter  701/10000  loss         0.102138  avg_L1_norm_grad         0.000094  w[0]   -0.057 bias    0.796\n",
      "iter  720/10000  loss         0.102033  avg_L1_norm_grad         0.000090  w[0]   -0.057 bias    0.804\n",
      "iter  721/10000  loss         0.102028  avg_L1_norm_grad         0.000090  w[0]   -0.057 bias    0.805\n",
      "iter  740/10000  loss         0.101931  avg_L1_norm_grad         0.000087  w[0]   -0.057 bias    0.812\n",
      "iter  741/10000  loss         0.101926  avg_L1_norm_grad         0.000087  w[0]   -0.057 bias    0.813\n",
      "iter  760/10000  loss         0.101836  avg_L1_norm_grad         0.000084  w[0]   -0.057 bias    0.820\n",
      "iter  761/10000  loss         0.101831  avg_L1_norm_grad         0.000084  w[0]   -0.057 bias    0.821\n",
      "iter  780/10000  loss         0.101747  avg_L1_norm_grad         0.000081  w[0]   -0.056 bias    0.828\n",
      "iter  781/10000  loss         0.101742  avg_L1_norm_grad         0.000081  w[0]   -0.056 bias    0.828\n",
      "iter  800/10000  loss         0.101664  avg_L1_norm_grad         0.000078  w[0]   -0.056 bias    0.836\n",
      "iter  801/10000  loss         0.101660  avg_L1_norm_grad         0.000078  w[0]   -0.056 bias    0.836\n",
      "iter  820/10000  loss         0.101587  avg_L1_norm_grad         0.000075  w[0]   -0.056 bias    0.843\n",
      "iter  821/10000  loss         0.101583  avg_L1_norm_grad         0.000075  w[0]   -0.056 bias    0.843\n",
      "iter  840/10000  loss         0.101514  avg_L1_norm_grad         0.000073  w[0]   -0.056 bias    0.850\n",
      "iter  841/10000  loss         0.101511  avg_L1_norm_grad         0.000073  w[0]   -0.056 bias    0.850\n",
      "iter  860/10000  loss         0.101447  avg_L1_norm_grad         0.000070  w[0]   -0.056 bias    0.857\n",
      "iter  861/10000  loss         0.101444  avg_L1_norm_grad         0.000070  w[0]   -0.056 bias    0.857\n",
      "iter  880/10000  loss         0.101384  avg_L1_norm_grad         0.000068  w[0]   -0.055 bias    0.864\n",
      "iter  881/10000  loss         0.101381  avg_L1_norm_grad         0.000068  w[0]   -0.055 bias    0.864\n",
      "iter  900/10000  loss         0.101325  avg_L1_norm_grad         0.000066  w[0]   -0.055 bias    0.871\n",
      "iter  901/10000  loss         0.101322  avg_L1_norm_grad         0.000066  w[0]   -0.055 bias    0.871\n",
      "iter  920/10000  loss         0.101270  avg_L1_norm_grad         0.000063  w[0]   -0.055 bias    0.877\n",
      "iter  921/10000  loss         0.101267  avg_L1_norm_grad         0.000063  w[0]   -0.055 bias    0.877\n",
      "iter  940/10000  loss         0.101218  avg_L1_norm_grad         0.000061  w[0]   -0.055 bias    0.884\n",
      "iter  941/10000  loss         0.101216  avg_L1_norm_grad         0.000061  w[0]   -0.055 bias    0.884\n",
      "iter  960/10000  loss         0.101170  avg_L1_norm_grad         0.000059  w[0]   -0.054 bias    0.890\n",
      "iter  961/10000  loss         0.101168  avg_L1_norm_grad         0.000059  w[0]   -0.054 bias    0.890\n",
      "iter  980/10000  loss         0.101125  avg_L1_norm_grad         0.000057  w[0]   -0.054 bias    0.896\n",
      "iter  981/10000  loss         0.101122  avg_L1_norm_grad         0.000057  w[0]   -0.054 bias    0.896\n",
      "iter 1000/10000  loss         0.101082  avg_L1_norm_grad         0.000056  w[0]   -0.054 bias    0.902\n",
      "iter 1001/10000  loss         0.101080  avg_L1_norm_grad         0.000055  w[0]   -0.054 bias    0.902\n",
      "iter 1020/10000  loss         0.101042  avg_L1_norm_grad         0.000054  w[0]   -0.054 bias    0.908\n",
      "iter 1021/10000  loss         0.101040  avg_L1_norm_grad         0.000054  w[0]   -0.054 bias    0.908\n",
      "iter 1040/10000  loss         0.101005  avg_L1_norm_grad         0.000052  w[0]   -0.054 bias    0.913\n",
      "iter 1041/10000  loss         0.101003  avg_L1_norm_grad         0.000052  w[0]   -0.054 bias    0.914\n",
      "iter 1060/10000  loss         0.100970  avg_L1_norm_grad         0.000050  w[0]   -0.053 bias    0.919\n",
      "iter 1061/10000  loss         0.100968  avg_L1_norm_grad         0.000050  w[0]   -0.053 bias    0.919\n",
      "iter 1080/10000  loss         0.100936  avg_L1_norm_grad         0.000049  w[0]   -0.053 bias    0.924\n",
      "iter 1081/10000  loss         0.100935  avg_L1_norm_grad         0.000049  w[0]   -0.053 bias    0.925\n",
      "iter 1100/10000  loss         0.100905  avg_L1_norm_grad         0.000047  w[0]   -0.053 bias    0.930\n",
      "iter 1101/10000  loss         0.100904  avg_L1_norm_grad         0.000047  w[0]   -0.053 bias    0.930\n",
      "iter 1120/10000  loss         0.100876  avg_L1_norm_grad         0.000046  w[0]   -0.053 bias    0.935\n",
      "iter 1121/10000  loss         0.100875  avg_L1_norm_grad         0.000046  w[0]   -0.053 bias    0.935\n",
      "iter 1140/10000  loss         0.100849  avg_L1_norm_grad         0.000044  w[0]   -0.053 bias    0.940\n",
      "iter 1141/10000  loss         0.100848  avg_L1_norm_grad         0.000044  w[0]   -0.053 bias    0.940\n",
      "iter 1160/10000  loss         0.100823  avg_L1_norm_grad         0.000043  w[0]   -0.053 bias    0.945\n",
      "iter 1161/10000  loss         0.100822  avg_L1_norm_grad         0.000043  w[0]   -0.053 bias    0.945\n",
      "iter 1180/10000  loss         0.100799  avg_L1_norm_grad         0.000042  w[0]   -0.052 bias    0.950\n",
      "iter 1181/10000  loss         0.100798  avg_L1_norm_grad         0.000042  w[0]   -0.052 bias    0.950\n",
      "iter 1200/10000  loss         0.100776  avg_L1_norm_grad         0.000040  w[0]   -0.052 bias    0.954\n",
      "iter 1201/10000  loss         0.100775  avg_L1_norm_grad         0.000040  w[0]   -0.052 bias    0.955\n",
      "iter 1220/10000  loss         0.100754  avg_L1_norm_grad         0.000039  w[0]   -0.052 bias    0.959\n",
      "iter 1221/10000  loss         0.100753  avg_L1_norm_grad         0.000039  w[0]   -0.052 bias    0.959\n",
      "iter 1240/10000  loss         0.100734  avg_L1_norm_grad         0.000038  w[0]   -0.052 bias    0.964\n",
      "iter 1241/10000  loss         0.100733  avg_L1_norm_grad         0.000038  w[0]   -0.052 bias    0.964\n",
      "iter 1260/10000  loss         0.100715  avg_L1_norm_grad         0.000037  w[0]   -0.052 bias    0.968\n",
      "iter 1261/10000  loss         0.100714  avg_L1_norm_grad         0.000037  w[0]   -0.052 bias    0.968\n",
      "iter 1280/10000  loss         0.100697  avg_L1_norm_grad         0.000036  w[0]   -0.052 bias    0.972\n",
      "iter 1281/10000  loss         0.100696  avg_L1_norm_grad         0.000036  w[0]   -0.052 bias    0.973\n",
      "iter 1300/10000  loss         0.100680  avg_L1_norm_grad         0.000035  w[0]   -0.052 bias    0.977\n",
      "iter 1301/10000  loss         0.100679  avg_L1_norm_grad         0.000035  w[0]   -0.052 bias    0.977\n",
      "iter 1320/10000  loss         0.100664  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.981\n",
      "iter 1321/10000  loss         0.100663  avg_L1_norm_grad         0.000034  w[0]   -0.051 bias    0.981\n",
      "iter 1340/10000  loss         0.100649  avg_L1_norm_grad         0.000033  w[0]   -0.051 bias    0.985\n",
      "iter 1341/10000  loss         0.100648  avg_L1_norm_grad         0.000033  w[0]   -0.051 bias    0.985\n",
      "iter 1360/10000  loss         0.100635  avg_L1_norm_grad         0.000032  w[0]   -0.051 bias    0.989\n",
      "iter 1361/10000  loss         0.100634  avg_L1_norm_grad         0.000032  w[0]   -0.051 bias    0.989\n",
      "iter 1380/10000  loss         0.100621  avg_L1_norm_grad         0.000031  w[0]   -0.051 bias    0.993\n",
      "iter 1381/10000  loss         0.100621  avg_L1_norm_grad         0.000031  w[0]   -0.051 bias    0.993\n",
      "iter 1400/10000  loss         0.100608  avg_L1_norm_grad         0.000030  w[0]   -0.051 bias    0.997\n",
      "iter 1401/10000  loss         0.100608  avg_L1_norm_grad         0.000030  w[0]   -0.051 bias    0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1420/10000  loss         0.100596  avg_L1_norm_grad         0.000029  w[0]   -0.051 bias    1.000\n",
      "iter 1421/10000  loss         0.100596  avg_L1_norm_grad         0.000029  w[0]   -0.051 bias    1.000\n",
      "iter 1440/10000  loss         0.100585  avg_L1_norm_grad         0.000028  w[0]   -0.051 bias    1.004\n",
      "iter 1441/10000  loss         0.100585  avg_L1_norm_grad         0.000028  w[0]   -0.051 bias    1.004\n",
      "iter 1460/10000  loss         0.100574  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.008\n",
      "iter 1461/10000  loss         0.100574  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.008\n",
      "iter 1480/10000  loss         0.100564  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.011\n",
      "iter 1481/10000  loss         0.100564  avg_L1_norm_grad         0.000027  w[0]   -0.051 bias    1.011\n",
      "iter 1500/10000  loss         0.100555  avg_L1_norm_grad         0.000026  w[0]   -0.051 bias    1.014\n",
      "iter 1501/10000  loss         0.100554  avg_L1_norm_grad         0.000026  w[0]   -0.051 bias    1.015\n",
      "iter 1520/10000  loss         0.100546  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.018\n",
      "iter 1521/10000  loss         0.100545  avg_L1_norm_grad         0.000025  w[0]   -0.050 bias    1.018\n",
      "iter 1540/10000  loss         0.100537  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.021\n",
      "iter 1541/10000  loss         0.100537  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.021\n",
      "iter 1560/10000  loss         0.100529  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.024\n",
      "iter 1561/10000  loss         0.100529  avg_L1_norm_grad         0.000024  w[0]   -0.050 bias    1.025\n",
      "iter 1580/10000  loss         0.100521  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.028\n",
      "iter 1581/10000  loss         0.100521  avg_L1_norm_grad         0.000023  w[0]   -0.050 bias    1.028\n",
      "iter 1600/10000  loss         0.100514  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.031\n",
      "iter 1601/10000  loss         0.100514  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.031\n",
      "iter 1620/10000  loss         0.100507  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.034\n",
      "iter 1621/10000  loss         0.100507  avg_L1_norm_grad         0.000022  w[0]   -0.050 bias    1.034\n",
      "iter 1640/10000  loss         0.100501  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.037\n",
      "iter 1641/10000  loss         0.100500  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.037\n",
      "iter 1660/10000  loss         0.100495  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.040\n",
      "iter 1661/10000  loss         0.100494  avg_L1_norm_grad         0.000021  w[0]   -0.050 bias    1.040\n",
      "iter 1680/10000  loss         0.100489  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.042\n",
      "iter 1681/10000  loss         0.100488  avg_L1_norm_grad         0.000020  w[0]   -0.050 bias    1.043\n",
      "iter 1700/10000  loss         0.100483  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.045\n",
      "iter 1701/10000  loss         0.100483  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.045\n",
      "iter 1720/10000  loss         0.100478  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.048\n",
      "iter 1721/10000  loss         0.100478  avg_L1_norm_grad         0.000019  w[0]   -0.050 bias    1.048\n",
      "iter 1740/10000  loss         0.100473  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.051\n",
      "iter 1741/10000  loss         0.100473  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.051\n",
      "iter 1760/10000  loss         0.100468  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.053\n",
      "iter 1761/10000  loss         0.100468  avg_L1_norm_grad         0.000018  w[0]   -0.050 bias    1.053\n",
      "iter 1780/10000  loss         0.100464  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.056\n",
      "iter 1781/10000  loss         0.100464  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.056\n",
      "iter 1800/10000  loss         0.100460  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.058\n",
      "iter 1801/10000  loss         0.100459  avg_L1_norm_grad         0.000017  w[0]   -0.050 bias    1.059\n",
      "iter 1820/10000  loss         0.100456  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.061\n",
      "iter 1821/10000  loss         0.100455  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.061\n",
      "iter 1840/10000  loss         0.100452  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.063\n",
      "iter 1841/10000  loss         0.100452  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.063\n",
      "iter 1860/10000  loss         0.100448  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.066\n",
      "iter 1861/10000  loss         0.100448  avg_L1_norm_grad         0.000016  w[0]   -0.050 bias    1.066\n",
      "iter 1880/10000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.068\n",
      "iter 1881/10000  loss         0.100445  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.068\n",
      "iter 1900/10000  loss         0.100441  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.070\n",
      "iter 1901/10000  loss         0.100441  avg_L1_norm_grad         0.000015  w[0]   -0.050 bias    1.071\n",
      "iter 1920/10000  loss         0.100438  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.073\n",
      "iter 1921/10000  loss         0.100438  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.073\n",
      "iter 1940/10000  loss         0.100435  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.075\n",
      "iter 1941/10000  loss         0.100435  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.075\n",
      "iter 1960/10000  loss         0.100433  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.077\n",
      "iter 1961/10000  loss         0.100433  avg_L1_norm_grad         0.000014  w[0]   -0.050 bias    1.077\n",
      "iter 1980/10000  loss         0.100430  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.079\n",
      "iter 1981/10000  loss         0.100430  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.079\n",
      "iter 2000/10000  loss         0.100428  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.081\n",
      "iter 2001/10000  loss         0.100427  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.081\n",
      "iter 2020/10000  loss         0.100425  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.083\n",
      "iter 2021/10000  loss         0.100425  avg_L1_norm_grad         0.000013  w[0]   -0.050 bias    1.083\n",
      "iter 2040/10000  loss         0.100423  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.085\n",
      "iter 2041/10000  loss         0.100423  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.085\n",
      "iter 2060/10000  loss         0.100421  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.087\n",
      "iter 2061/10000  loss         0.100421  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.087\n",
      "iter 2080/10000  loss         0.100419  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.089\n",
      "iter 2081/10000  loss         0.100419  avg_L1_norm_grad         0.000012  w[0]   -0.050 bias    1.089\n",
      "iter 2100/10000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.091\n",
      "iter 2101/10000  loss         0.100417  avg_L1_norm_grad         0.000011  w[0]   -0.050 bias    1.091\n",
      "iter 2120/10000  loss         0.100415  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.093\n",
      "iter 2121/10000  loss         0.100415  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.093\n",
      "iter 2140/10000  loss         0.100413  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.095\n",
      "iter 2141/10000  loss         0.100413  avg_L1_norm_grad         0.000011  w[0]   -0.049 bias    1.095\n",
      "iter 2160/10000  loss         0.100411  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.096\n",
      "iter 2161/10000  loss         0.100411  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.097\n",
      "iter 2180/10000  loss         0.100410  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.098\n",
      "iter 2181/10000  loss         0.100410  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.098\n",
      "iter 2200/10000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.100\n",
      "iter 2201/10000  loss         0.100408  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2220/10000  loss         0.100407  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.102\n",
      "iter 2221/10000  loss         0.100407  avg_L1_norm_grad         0.000010  w[0]   -0.049 bias    1.102\n",
      "iter 2240/10000  loss         0.100406  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.103\n",
      "iter 2241/10000  loss         0.100405  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.103\n",
      "iter 2260/10000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.105\n",
      "iter 2261/10000  loss         0.100404  avg_L1_norm_grad         0.000009  w[0]   -0.049 bias    1.105\n",
      "iter 2280/10000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.106\n",
      "iter 2281/10000  loss         0.100403  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.106\n",
      "iter 2300/10000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.108\n",
      "iter 2301/10000  loss         0.100402  avg_L1_norm_grad         0.000009  w[0]   -0.050 bias    1.108\n",
      "iter 2320/10000  loss         0.100401  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.109\n",
      "iter 2321/10000  loss         0.100401  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.110\n",
      "iter 2340/10000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.111\n",
      "iter 2341/10000  loss         0.100400  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.111\n",
      "iter 2360/10000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.112\n",
      "iter 2361/10000  loss         0.100399  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.113\n",
      "iter 2380/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.114\n",
      "iter 2381/10000  loss         0.100398  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.114\n",
      "iter 2400/10000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.115\n",
      "iter 2401/10000  loss         0.100397  avg_L1_norm_grad         0.000008  w[0]   -0.050 bias    1.115\n",
      "iter 2420/10000  loss         0.100396  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.117\n",
      "iter 2421/10000  loss         0.100396  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.117\n",
      "iter 2440/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.118\n",
      "iter 2441/10000  loss         0.100395  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.118\n",
      "iter 2460/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.119\n",
      "iter 2461/10000  loss         0.100394  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.119\n",
      "iter 2480/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.121\n",
      "iter 2481/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.121\n",
      "iter 2500/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.122\n",
      "iter 2501/10000  loss         0.100393  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.122\n",
      "iter 2520/10000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.123\n",
      "iter 2521/10000  loss         0.100392  avg_L1_norm_grad         0.000007  w[0]   -0.050 bias    1.123\n",
      "iter 2540/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.124\n",
      "iter 2541/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.125\n",
      "iter 2560/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.126\n",
      "iter 2561/10000  loss         0.100391  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.126\n",
      "iter 2580/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.127\n",
      "iter 2581/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.127\n",
      "iter 2600/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.128\n",
      "iter 2601/10000  loss         0.100390  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.128\n",
      "iter 2620/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.129\n",
      "iter 2621/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.129\n",
      "iter 2640/10000  loss         0.100389  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.130\n",
      "iter 2641/10000  loss         0.100388  avg_L1_norm_grad         0.000006  w[0]   -0.050 bias    1.130\n",
      "iter 2660/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.131\n",
      "iter 2661/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.131\n",
      "iter 2680/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.132\n",
      "iter 2681/10000  loss         0.100388  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.133\n",
      "iter 2700/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 2701/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.134\n",
      "iter 2720/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 2721/10000  loss         0.100387  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.135\n",
      "iter 2740/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 2741/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.136\n",
      "iter 2760/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 2761/10000  loss         0.100386  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.137\n",
      "iter 2780/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 2781/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.138\n",
      "iter 2800/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 2801/10000  loss         0.100385  avg_L1_norm_grad         0.000005  w[0]   -0.050 bias    1.139\n",
      "iter 2820/10000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 2821/10000  loss         0.100385  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 2840/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.140\n",
      "iter 2841/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 2860/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 2861/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.141\n",
      "iter 2880/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 2881/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.142\n",
      "iter 2900/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 2901/10000  loss         0.100384  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.143\n",
      "iter 2920/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 2921/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.144\n",
      "iter 2940/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 2941/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.145\n",
      "iter 2960/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 2961/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.146\n",
      "iter 2980/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 2981/10000  loss         0.100383  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 3000/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n",
      "iter 3001/10000  loss         0.100382  avg_L1_norm_grad         0.000004  w[0]   -0.050 bias    1.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3020/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.148\n",
      "iter 3021/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.148\n",
      "iter 3040/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.149\n",
      "iter 3041/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.149\n",
      "iter 3060/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3061/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3080/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3081/10000  loss         0.100382  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.150\n",
      "iter 3100/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 3101/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.151\n",
      "iter 3120/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 3121/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.152\n",
      "iter 3140/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3141/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3160/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3161/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.153\n",
      "iter 3180/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 3181/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.154\n",
      "iter 3200/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3201/10000  loss         0.100381  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3220/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3221/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.155\n",
      "iter 3240/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 3241/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.156\n",
      "iter 3260/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3261/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3280/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3281/10000  loss         0.100380  avg_L1_norm_grad         0.000003  w[0]   -0.050 bias    1.157\n",
      "iter 3300/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3301/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3320/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3321/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.158\n",
      "iter 3340/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 3341/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.159\n",
      "iter 3360/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3361/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3380/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3381/10000  loss         0.100380  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.160\n",
      "iter 3400/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3401/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3420/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3421/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.161\n",
      "iter 3440/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3441/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3460/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3461/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.162\n",
      "iter 3480/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3481/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3500/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3501/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.163\n",
      "iter 3520/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3521/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3540/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3541/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.164\n",
      "iter 3560/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3561/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3580/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3581/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.165\n",
      "iter 3600/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3601/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3620/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3621/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.166\n",
      "iter 3640/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3641/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3660/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3661/10000  loss         0.100379  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.167\n",
      "iter 3680/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3681/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3700/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3701/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3720/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3721/10000  loss         0.100378  avg_L1_norm_grad         0.000002  w[0]   -0.050 bias    1.168\n",
      "iter 3740/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3741/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3760/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3761/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.169\n",
      "iter 3780/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3781/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3800/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3801/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3820/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3821/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.170\n",
      "iter 3840/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3841/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3860/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3861/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3880/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3881/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.171\n",
      "iter 3900/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3901/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3920/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3921/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.172\n",
      "iter 3940/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.173\n",
      "iter 3941/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.050 bias    1.173\n",
      "iter 3960/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 3961/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 3980/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 3981/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 4000/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.173\n",
      "iter 4001/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4020/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4021/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4040/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4041/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4060/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4061/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.174\n",
      "iter 4080/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4081/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4100/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4101/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4120/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4121/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.175\n",
      "iter 4140/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4141/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4160/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4161/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4180/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4181/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4200/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4201/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.176\n",
      "iter 4220/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4221/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4240/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4241/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4260/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4261/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4280/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.177\n",
      "iter 4281/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4300/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4301/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4320/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4321/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4340/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4341/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4360/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4361/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.178\n",
      "iter 4380/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4381/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4400/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4401/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4420/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4421/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4440/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4441/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.179\n",
      "iter 4460/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4461/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4480/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4481/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4500/10000  loss         0.100378  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4501/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4520/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4521/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4540/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4541/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.180\n",
      "iter 4560/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "iter 4561/10000  loss         0.100377  avg_L1_norm_grad         0.000001  w[0]   -0.051 bias    1.181\n",
      "Done. Converged after 4577 iterations.\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegressionGradientDescent(\n",
    "        alpha=10, step_size = 0.5, init_w_recipe='zeros')\n",
    "lr.fit(tsetx_AF, tsety_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
